{"meta":{"title":"Jermmy's Lazy Blog","subtitle":null,"description":"In me the tiger sniffs the rose.","author":"Jermmy","url":"https://jermmy.github.io"},"posts":[{"title":"神经网络量化入门--Add和Concat","slug":"2020-12-7-network-quantization-6","date":"2020-12-07T14:42:03.000Z","updated":"2020-12-14T14:58:46.397Z","comments":true,"path":"2020/12/07/2020-12-7-network-quantization-6/","link":"","permalink":"https://jermmy.github.io/2020/12/07/2020-12-7-network-quantization-6/","excerpt":"好久没更新了，一方面是因为工作繁忙，另一方面主要是懒。\n之前写过几篇关于神经网络量化的文章，主要是对 Google 量化论文以及白皮书的解读，但有一些细节的问题当时没有提及。这篇文章想补充其中一个问题：关于 ElementwiseAdd (简称 EltwiseAdd) 和 Concat 的量化。","text":"好久没更新了，一方面是因为工作繁忙，另一方面主要是懒。 之前写过几篇关于神经网络量化的文章，主要是对 Google 量化论文以及白皮书的解读，但有一些细节的问题当时没有提及。这篇文章想补充其中一个问题：关于 ElementwiseAdd (简称 EltwiseAdd) 和 Concat 的量化。 EltwiseAdd量化 EltwiseAdd 的量化主要是在论文的附录里面提及的。过程不是太复杂，如果了解量化的基本原理的话，完全可以自己推导出来。 回忆一下量化的基本公式： \\[ r=S(q-Z) \\tag{1} \\] (看不懂的可以再参考一下我之前的文章) 这里面 \\(r\\) 是实数域中的数值 (一般是 float)，\\(q\\) 则是量化后的整型数值 (常用的是 int8)。 EltwiseAdd 就是对两个 tensor 的数值逐个相加。假设两个 tensor 中的数值分别是 \\(r_1\\)、\\(r_2\\)，相加得到的和用 \\(r_3\\) 表示，那全精度下的 EltwiseAdd 可以表示为： \\[ r_3 = r_1 + r_2 \\tag{2} \\] 用量化的公式代入进去后可以得到： \\[ S_3(q_3-Z_3)=S_1(q_1-Z_1)+S_2(q_2-Z_2) \\tag{3} \\] 稍作整理可以得到： \\[ q_3=\\frac{S_1}{S_3}(q_1-Z_1+\\frac{S_2}{S_1}(q_2-Z_2))+Z_3 \\tag{4} \\] 注意，这里有两个 scale 运算需要转换为定点小数加一个 bitshift 的运算 (具体做法见之前的文章)。除了需要对输出按照 \\(\\frac{S_1}{S_3}\\) 放缩外，其中一个输入也需要按照 \\(\\frac{S_2}{S_1}\\) 进行放缩，这一步就是论文中提到的 rescale。 这一部分的代码我就不准备在 pytorch 中实现了，毕竟这个模块的量化最主要的就是统计输入跟输出的 minmax，因此训练代码几乎没什么内容，主要的工作都是在推理引擎实现的。因此这篇文章我会摘取 tflite 中部分实现简单说明一下。 下面是 tf1.5 中我摘取的部分关于 EltwiseAdd 的量化实现，对应的链接是https://github.com/tensorflow/tensorflow/blob/r1.15/tensorflow/lite/kernels/internal/reference/add.h#L53： 1234567891011121314151617181920212223242526inline void AddElementwise(int size, const ArithmeticParams&amp; params, const uint8* input1_data, const uint8* input2_data, uint8* output_data) &#123; // ......此处省略若干无关代码 for (int i = 0; i &lt; size; ++i) &#123; const int32 input1_val = params.input1_offset + input1_data[i]; const int32 input2_val = params.input2_offset + input2_data[i]; const int32 shifted_input1_val = input1_val * (1 &lt;&lt; params.left_shift); const int32 shifted_input2_val = input2_val * (1 &lt;&lt; params.left_shift); const int32 scaled_input1_val = MultiplyByQuantizedMultiplierSmallerThanOneExp( shifted_input1_val, params.input1_multiplier, params.input1_shift); const int32 scaled_input2_val = MultiplyByQuantizedMultiplierSmallerThanOneExp( shifted_input2_val, params.input2_multiplier, params.input2_shift); const int32 raw_sum = scaled_input1_val + scaled_input2_val; const int32 raw_output = MultiplyByQuantizedMultiplierSmallerThanOneExp( raw_sum, params.output_multiplier, params.output_shift) + params.output_offset; const int32 clamped_output = std::min(params.quantized_activation_max, std::max(params.quantized_activation_min, raw_output)); output_data[i] = static_cast&lt;uint8&gt;(clamped_output); &#125;&#125; 这里面有个函数 MultiplyByQuantizedMultiplierSmallerThanOneExp，它的主要作用是调用 gemmlowp 中的函数将乘以 scale 的浮点运算转换为乘以一个定点小数加 bitshift 的操作，由于涉及比较多底层操作，不在本文讨论之内。 整段代码的逻辑和上文分析的基本类似，首先是对输入加 offset 操作，对应公式中的 \\(q_i-Z_i\\)，然后分别对两个输入乘以 scale，那按照上文的描述，一般来说只有一个输入需要进行 rescale 操作，另一个输入的 scale 其实是 1。在对两个输入相加后得到输出 (代码中的 raw_sum)，会按照同样的方式对输出进行 scale 放缩并加上 offset，最后再 clamp 到 uint8 的数值范围内。 Concat量化 Concat 可以采用和 EltwiseAdd 类似的操作，对其中一个输入进行 rescale 后再 concat，最后再对输出进行 rescale，参考如下推导： \\[ r_3=concat[r_1, r_2] \\tag{5} \\] 代入量化公式： \\[ S_3(q_3-Z_3)=concat[S_1(q_1-Z_1),S_2(q_2-Z_2)] \\tag{6} \\] 整理后得到： \\[ \\frac{S_3}{S_1}(q_3-Z_3)=concat[(q_1-Z_1),\\frac{S_2}{S_1}(q_2-Z_2)] \\tag{7} \\] 不过 rescale 本身是存在精度损失的，而 Concat 严格来说是一个无损的操作 (concat 其实就是内存拷贝而已)，因此论文建议统一输入输出的 scale 来避免 rescale： 不过我始终想不通要如何在没有 rescale 的情况下统一输入输出的 scale。论文中也没有提及相关的实现，很多细节只能到 tflite 的源码中查找。 可以明确的一点是，output 的 minmax 可以通过取两个输入的最小 min 和最大 max 来确定。那无非存在两种情况：1. 其中一个输入的 minmax 覆盖了整个范围，即输出的 minmax 完全由某一个输入确定；2. minmax 分别来自两个输入，即一个输入的 min 和 另一个输入的 max 确定输出的 minmax。 为了了解 Google 到底怎么处理 concat 量化，我稍微翻了下 tf1.5 中对于量化 concat 的实现。 下面是在源码中找到的部分代码注释： 123456789// There are two inputs for concat, \"input0\" and \"input1\". \"input0\" has [0, 5]// as min/max and \"input1\" has [0, 10] as min/max. The output \"output\" for// concat has [0, 10] as min/max.// After applyging QuantizeModel(), \"input0\" will have a requant op added, along// with a tensor \"input0_reqaunt\" that has [0, 10] as min/max. So the topology// becomes:// input0 -&gt; requant -&gt; input0_requant \\// concat - output// input1 / 具体位置在：https://github.com/tensorflow/tensorflow/blob/r1.15/tensorflow/lite/tools/optimize/quantize_model_test.cc#L303 这段注释说的是上面的情况 1，即其中一个输入的 minmax 覆盖了整个范围。这种情况下，tflite 的做法是将 range 较小的输入进行 requant，即根据大 range 的 minmax，来重新量化这个输入。 那具体怎么 requant 呢？这里需要在另一段代码中找细节： 123456789101112131415161718192021222324252627282930inline void ConcatenationWithScaling(const ConcatenationParams&amp; params, const RuntimeShape* const* input_shapes, const uint8* const* input_data, const RuntimeShape&amp; output_shape, uint8* output_data) &#123; .... const float inverse_output_scale = 1.f / output_scale; uint8* output_ptr = output_data; for (int k = 0; k &lt; outer_size; k++) &#123; for (int i = 0; i &lt; inputs_count; ++i) &#123; const int copy_size = input_shapes[i]-&gt;Dims(axis) * base_inner_size; const uint8* input_ptr = input_data[i] + k * copy_size; if (input_zeropoint[i] == output_zeropoint &amp;&amp; input_scale[i] == output_scale) &#123; memcpy(output_ptr, input_ptr, copy_size); &#125; else &#123; const float scale = input_scale[i] * inverse_output_scale; const float bias = -input_zeropoint[i] * scale; for (int j = 0; j &lt; copy_size; ++j) &#123; const int32_t value = static_cast&lt;int32_t&gt;(std::round(input_ptr[j] * scale + bias)) + output_zeropoint; output_ptr[j] = static_cast&lt;uint8_t&gt;(std::max(std::min(255, value), 0)); &#125; &#125; output_ptr += copy_size; &#125; &#125;&#125; 这里只贴了其中比较关键的实现，链接：https://github.com/tensorflow/tensorflow/blob/r1.15/tensorflow/lite/kernels/internal/reference/reference_ops.h#L1164。 具体做法是这样的：如果输入的 scale、zeropoint 和输出不一样，那么就对该输入按照输出的 scale 和 zeropoint 重新 requant，表示成公式的话是这样子的： \\[ \\begin{align} q_3&amp;=(q_1\\frac{S_1}{S_3}-Z_1\\frac{S_1}{S_3})+Z_3 \\notag \\\\ &amp;=\\frac{S_1}{S_3}(q_1-Z_1)+Z_3 \\tag{8} \\end{align} \\] 对比上面公式 (7)，我发现这他喵不就是对输入 \\(q_1\\) 进行 rescale 吗？而且，上面这段代码不会区分 \\(q_1\\)、\\(q_2\\)，只要发现输入的 scale 和 zeropoint 和输出对不上，就会对任何一个输入进行 requant。 换言之，量化 concat 可以用公式表示为： \\[ q_3=concat[\\frac{S_1}{S_3}(q_1-Z_1)+Z_3,\\frac{S_2}{S_3}(q_2-Z_2)+Z_3] \\tag{9} \\] 总结 这篇文章是对网络量化中 EltwiseAdd 和 Concat 两个操作的补充，由于有 rescale 以及 requant 的存在，这两个运算相比 float 而言，计算量反而更大，而且可能导致精度上的损失。因此在量化网络的时候，需要关注这两个函数的输入 range 不要相差太大，以避免精度损失过大。 参考 tensorflow量化部分整理ing Quantizing Networks","raw":null,"content":null,"categories":[{"name":"深度学习","slug":"深度学习","permalink":"https://jermmy.github.io/categories/深度学习/"}],"tags":[{"name":"深度学习","slug":"深度学习","permalink":"https://jermmy.github.io/tags/深度学习/"}]},{"title":"女朋友让我做一个表情包生成器，然后。。。","slug":"2020-10-8-emotion-generation","date":"2020-10-08T08:55:42.000Z","updated":"2020-10-09T15:05:39.960Z","comments":true,"path":"2020/10/08/2020-10-8-emotion-generation/","link":"","permalink":"https://jermmy.github.io/2020/10/08/2020-10-8-emotion-generation/","excerpt":"一个月前，女朋友让我做一个生成表情包的工具，趁着国庆的尾巴搞了一下，一番操作后，终于让懂王喜笑颜开了。\n\n\n","text":"一个月前，女朋友让我做一个生成表情包的工具，趁着国庆的尾巴搞了一下，一番操作后，终于让懂王喜笑颜开了。 说起表情包生成，我最先想到的便是 2018 年的一篇论文 GANimation。这篇论文的新颖之处在于把表情和脸部肌肉的变化联系在一起。要知道，自从有了 GAN 后，主流研究都是把表情变化当作模态迁移「domain transfer」来研究的，比如比较出名的 StarGAN： 所谓模态迁移，就是说我们把人物图片分成不同的数据分布，微笑属于一种数据分布，悲伤属于另一种，然后通过 GAN 网络把一种数据分布转换为另一种，这样就达到表情变换的作用。 不过，我一直认为表情是很难分类的，不同表情之间可以有交集的部分，同一种表情也有强弱之分。而模态迁移把表情固定成了事先设定的几类，这种思路不仅限制了表情的类别，同时也不容易搞清楚模型本身在做什么事情。 而 GANimation 则是把表情变化和脸部肌肉的运动结合在一起，通过控制不同肌肉的变化来实现表情变化。这种方法理论上可以控制生成任意表情，同时，由于我们可以通过控制肌肉变化的力度来控制算法模型，因此也更能搞清楚模型背后的机理。 好了，前面说了这么多废话，那 GANimation 到底是如何实现的呢？要了解这一点，首先需要知道人类是如何控制表情变化的。这个问题并不复杂，早在 1970 年的时候，就有人研究了各式各样的表情，并把它们和特定的肌肉联系在一起，他们把这种肌肉的变化称为 Action Unit「AU」： 比如，当你惊恐的时候，你的眉毛就会由中间翘起；当你严肃生气的时候，你的眉毛就会从两边翘起。 把这些 Action Unit 组合起来，就构成了一种种表情。如此一来，就可以通过控制 Action Unit，来达到控制表情的目的。再结合 GAN 的强大之处，就可以灵活地控制每一种表情的变化： 当然啦，这个模型本身并没有脱离模态迁移的套路，不过，相比前面的 StarGAN，它可以更连续地控制每一种表情的变化力度，相当于把表情变化这个任务分解得更加精细，因此我个人觉得实用性更好一些。 是骡子是马，还是要拉出来溜溜。这篇论文的作者没有放出他们训练好的模型，不过好在 github 上有一些开源实现，而且效果还不错。我找了完成度最高的代码，并尝试了一下作者提供的模型，感觉效果还不错。比如拿懂王做的实验： 还有安倍酱的效果： 甚至这位： 当然，也有一些不协调的例子： 有些图很明显把胡子也生成出来了，这可能是因为模型是在欧美人的数据集上训练的，而欧美人的胡子普遍较多，导致模型受到数据集的影响产生自己的偏好。 另外，懂王的表情看起来也比安倍好一点，除了懂王本人更符合欧美人的特征外，也说明了，有些人天生适合做表情包。 不过，这个模型的效果也只能试试 demo，离真正上线使用还隔着好几个 StyleGAN 的距离。毕竟这是两年前的论文，模型结构也存在欠缺之处。不过这个思路基本是可行的，如果能用一些更先进的技术稍加改进，应该会有更不错的效果。 参考 GANimation: Anatomically-aware Facial Animation from a Single Image","raw":null,"content":null,"categories":[{"name":"深度学习","slug":"深度学习","permalink":"https://jermmy.github.io/categories/深度学习/"}],"tags":[{"name":"深度学习","slug":"深度学习","permalink":"https://jermmy.github.io/tags/深度学习/"}]},{"title":"神经网络量化入门--Folding BN ReLU代码实现","slug":"2020-8-9-network-quantization-5","date":"2020-08-09T13:10:27.000Z","updated":"2020-10-08T09:50:25.532Z","comments":true,"path":"2020/08/09/2020-8-9-network-quantization-5/","link":"","permalink":"https://jermmy.github.io/2020/08/09/2020-8-9-network-quantization-5/","excerpt":"上一篇文章介绍了如何把 BatchNorm 和 ReLU 合并到 Conv 中，这篇文章会介绍具体的代码实现。本文相关代码都可以在 github 上找到。\n\n\n","text":"上一篇文章介绍了如何把 BatchNorm 和 ReLU 合并到 Conv 中，这篇文章会介绍具体的代码实现。本文相关代码都可以在 github 上找到。 Folding BN 回顾一下前文把 BN 合并到 Conv 中的公式： \\[ \\begin{align} y_{bn}&amp;=\\frac{\\gamma}{\\sqrt{\\sigma_y^2+\\epsilon}}(\\sum_{i}^N w_i x_i + b-\\mu_y)+\\beta \\notag \\\\ &amp;=\\gamma&#39;(\\sum_{i}^Nw_ix_i+b-\\mu_y)+\\beta \\notag \\\\ &amp;=\\sum_{i}^N \\gamma&#39;w_ix_i+\\gamma&#39;(b-\\mu_y)+\\beta \\tag{1} \\end{align} \\] 其中，\\(x\\) 是卷积层的输入，\\(w\\)、\\(b\\) 分别是 Conv 的参数 weight 和 bias，\\(\\gamma\\)、\\(\\beta\\) 是 BN 层的参数。 对于 BN 的合并，首先，我们需要熟悉 pytorch 中的 BatchNorm2d 模块。 pytorch 中的 BatchNorm2d 针对 feature map 的每一个 channel 都会计算一个均值和方差，所以公式 (1) 需要对 weight 和 bias 进行 channel wise 的计算。另外，BatchNorm2d 中有一个布尔变量 affine，当该变量为 true 的时候，(1) 式中的 \\(\\gamma\\) 和 \\(\\beta\\) 就是可学习的， BatchNorm2d 会中有两个变量：weight 和 bias，来分别存放这两个参数。而当 affine 为 false 的时候，就直接默认 \\(\\gamma=1\\)，\\(\\beta=0\\)，相当于 BN 中没有可学习的参数。默认情况下，我们都设置 affine=True。 我们沿用之前的代码，先定义一个 QConvBNReLU 模块： 12345678class QConvBNReLU(QModule): def __init__(self, conv_module, bn_module, qi=True, qo=True, num_bits=8): super(QConvBNReLU, self).__init__(qi=qi, qo=qo, num_bits=num_bits) self.num_bits = num_bits self.conv_module = conv_module self.bn_module = bn_module self.qw = QParam(num_bits=num_bits) 这个模块会把全精度网络中的 Conv2d 和 BN 接收进来，并重新封装成量化的模块。 接着，定义合并 BN 后的 forward 流程： 1234567891011121314151617181920212223242526272829303132333435363738394041424344def forward(self, x): if hasattr(self, 'qi'): self.qi.update(x) x = FakeQuantize.apply(x, self.qi) if self.training: # 开启BN层训练 y = F.conv2d(x, self.conv_module.weight, self.conv_module.bias, stride=self.conv_module.stride, padding=self.conv_module.padding, dilation=self.conv_module.dilation, groups=self.conv_module.groups) y = y.permute(1, 0, 2, 3) # NCHW -&gt; CNHW y = y.contiguous().view(self.conv_module.out_channels, -1) # CNHW -&gt; (C,NHW)，这一步是为了方便channel wise计算均值和方差 mean = y.mean(1) var = y.var(1) self.bn_module.running_mean = \\ self.bn_module.momentum * self.bn_module.running_mean + \\ (1 - self.bn_module.momentum) * mean self.bn_module.running_var = \\ self.bn_module.momentum * self.bn_module.running_var + \\ (1 - self.bn_module.momentum) * var else: # BN层不更新 mean = self.bn_module.running_mean var = self.bn_module.running_var std = torch.sqrt(var + self.bn_module.eps) weight, bias = self.fold_bn(mean, std) self.qw.update(weight.data) x = F.conv2d(x, FakeQuantize.apply(weight, self.qw), bias, stride=self.conv_module.stride, padding=self.conv_module.padding, dilation=self.conv_module.dilation, groups=self.conv_module.groups) x = F.relu(x) if hasattr(self, 'qo'): self.qo.update(x) x = FakeQuantize.apply(x, self.qo) return x 这个过程就是对 Google 论文的那张图的诠释，跟一般的卷积量化的区别就是需要先获得 BN 层的参数，再把它们 folding 到 Conv 中，最后跑正常的卷积量化流程。不过，根据论文的表述，我们还需要在 forward 的过程更新 BN 的均值、方差，这部分对应上面代码 if self.training 分支下的部分。 然后，根据公式 (1)，我们可以计算出 fold BN 后，卷积层的 weight 和 bias： 1234567891011121314151617def fold_bn(self, mean, std): if self.bn_module.affine: gamma_ = self.bn_module.weight / std # 这一步计算gamma' weight = self.conv_module.weight * gamma_.view(self.conv_module.out_channels, 1, 1, 1) if self.conv_module.bias is not None: bias = gamma_ * self.conv_module.bias - gamma_ * mean + self.bn_module.bias else: bias = self.bn_module.bias - gamma_ * mean else: # affine为False的情况，gamma=1, beta=0 gamma_ = 1 / std weight = self.conv_module.weight * gamma_ if self.conv_module.bias is not None: bias = gamma_ * self.conv_module.bias - gamma_ * mean else: bias = -gamma_ * mean return weight, bias 上面的代码直接参照公式 (1) 就可以看懂，其中 gamma_ 就是公式中的 \\(\\gamma&#39;\\)。由于前面提到，pytorch 的 BatchNorm2d 中，\\(\\gamma\\) 和 \\(\\beta\\) 可能是可学习的变量，也可能默认为 1 和 0，因此根据 affine 是否为 True 分了两种情况，原理上基本类似，这里就不再赘述。 合并ReLU 前面说了，ReLU 的合并可以通过在 ReLU 之后统计 minmax，再计算 scale 和 zeropoint 的方式来实现，因此这部分代码非常简单，就是在 forward 的时候，在做完 relu 后再统计 minmax 即可，对应代码片段为： 123456789101112131415161718192021222324def forward(self, x): if hasattr(self, 'qi'): self.qi.update(x) x = FakeQuantize.apply(x, self.qi) ... weight, bias = self.fold_bn(mean, std) self.qw.update(weight.data) x = F.conv2d(x, FakeQuantize.apply(weight, self.qw), bias, stride=self.conv_module.stride, padding=self.conv_module.padding, dilation=self.conv_module.dilation, groups=self.conv_module.groups) x = F.relu(x) # &lt;-- calculate minmax after relu if hasattr(self, 'qo'): self.qo.update(x) x = FakeQuantize.apply(x, self.qo) return x 将 BN 和 ReLU 合并到 Conv 中，QConvBNReLU 模块本身就是一个普通的卷积了，因此量化推理的过程和之前文章的 QConv2d 一样，这里不再赘述。 实验 这里照例给出一些实验结果。 本文的实验还是在 mnist 上进行，我重新定义了一个包含 BN 的新网络： 12345678910111213141516171819202122class NetBN(nn.Module): def __init__(self, num_channels=1): super(NetBN, self).__init__() self.conv1 = nn.Conv2d(num_channels, 40, 3, 1) self.bn1 = nn.BatchNorm2d(40) self.conv2 = nn.Conv2d(40, 40, 3, 1) self.bn2 = nn.BatchNorm2d(40) self.fc = nn.Linear(5 * 5 * 40, 10) def forward(self, x): x = self.conv1(x) x = self.bn1(x) x = F.relu(x) x = F.max_pool2d(x, 2, 2) x = self.conv2(x) x = self.bn2(x) x = F.relu(x) x = F.max_pool2d(x, 2, 2) x = x.view(-1, 5 * 5 * 40) x = self.fc(x) return x 量化该网络的代码如下： 123456def quantize(self, num_bits=8): self.qconv1 = QConvBNReLU(self.conv1, self.bn1, qi=True, qo=True, num_bits=num_bits) self.qmaxpool2d_1 = QMaxPooling2d(kernel_size=2, stride=2, padding=0) self.qconv2 = QConvBNReLU(self.conv2, self.bn2, qi=False, qo=True, num_bits=num_bits) self.qmaxpool2d_2 = QMaxPooling2d(kernel_size=2, stride=2, padding=0) self.qfc = QLinear(self.fc, qi=False, qo=True, num_bits=num_bits) 整体的代码风格基本和之前一样，不熟悉的读者建议先阅读我之前的量化文章。 先训练一个全精度网络「相关代码在 train.py 里面」，可以得到全精度模型的准确率是 99%。 然后，我又跑了一遍后训练量化以及量化感知训练，在不同量化 bit 下的精度如下表所示「由于学习率对量化感知训练的影响非常大，这里顺便附上每个 bit 对应的学习率」： bit 1 2 3 4 5 6 7 8 后训练量化 10% 11% 10% 35% 82% 85% 85% 87% 量化感知训练 10% 19% 59% 91% 92% 94% 94% 95% lr 0.00001 0.0001 0.02 0.02 0.02 0.02 0.02 0.04 对比之前文章的结果，加入 BN 后，后训练量化在精度上的下降更加明显，而量化感知训练依然能带来较大的精度提升。但在低 bit 情况下，由于信息损失严重，网络的优化会变的非常困难。 总结 这篇文章给出了 Folding BN 和 ReLU 的代码实现，主要是想帮助初学者加深对公式细节的理解。至此，这系列教程基本告一段落，希望能帮助小白们快速入门这一领域。后面会不定期介绍一些我觉得有趣的 AI 技术，感兴趣的读者欢迎吃瓜吐槽。","raw":null,"content":null,"categories":[{"name":"深度学习","slug":"深度学习","permalink":"https://jermmy.github.io/categories/深度学习/"}],"tags":[{"name":"深度学习","slug":"深度学习","permalink":"https://jermmy.github.io/tags/深度学习/"}]},{"title":"神经网络量化入门--Folding BN ReLU","slug":"2020-7-19-network-quantization-4","date":"2020-07-19T06:38:47.000Z","updated":"2020-12-13T13:00:29.974Z","comments":true,"path":"2020/07/19/2020-7-19-network-quantization-4/","link":"","permalink":"https://jermmy.github.io/2020/07/19/2020-7-19-network-quantization-4/","excerpt":"上一篇文章介绍了量化训练的基本流程，本文介绍量化中如何把 BatchNorm 和 ReLU 合并到 Conv 中。\n\n\n","text":"上一篇文章介绍了量化训练的基本流程，本文介绍量化中如何把 BatchNorm 和 ReLU 合并到 Conv 中。 Folding BatchNorm BatchNorm 是 Google 提出的一种加速神经网络训练的技术，在很多网络中基本是标配。 回忆一下，BatchNorm 其实就是在每一层输出的时候做了一遍归一化操作： 其中 \\(x_i\\) 是网络中间某一层的激活值，\\(\\mu_{\\beta}\\)、\\(\\sigma_{\\beta}\\) 分别是其均值和方差，\\(y_i\\) 则是过了 BN 后的输出。 一般卷积层与BN合并 Folding BatchNorm 不是量化才有的操作，在一般的网络中，为了加速网络推理，我们也可以把 BN 合并到 Conv 中。 合并的过程是这样的，假设有一个已经训练好的 Conv 和 BN： 假设 Conv 的 weight 和 bias 分别是 \\(w\\) 和 \\(b\\)。那么卷积层的输出为： \\[ y=\\sum_{i}^N w_i x_i + b \\tag{1} \\] 图中 BN 层的均值和标准差可以表示为 \\(\\mu_{y}\\)、\\(\\sigma_{y}\\)，那么根据论文的表述，BN 层的输出为： \\[ \\begin{align} y_{bn}&amp;=\\gamma \\hat{y}+\\beta \\notag \\\\ &amp;=\\gamma \\frac{y-\\mu_y}{\\sqrt{\\sigma_y^2+\\epsilon}}+\\beta \\tag{2} \\end{align} \\] 然后我们把 (1) 代入 (2) 中可以得到： \\[ y_{bn}=\\frac{\\gamma}{\\sqrt{\\sigma_y^2+\\epsilon}}(\\sum_{i}^N w_i x_i + b-\\mu_y)+\\beta \\tag{3} \\] 我们用 \\(\\gamma&#39;\\) 来表示 \\(\\frac{\\gamma}{\\sqrt{\\sigma_y^2+\\epsilon}}\\)，那么 (3) 可以简化为： \\[ \\begin{align} y_{bn}&amp;=\\gamma&#39;(\\sum_{i}^Nw_ix_i+b-\\mu_y)+\\beta \\notag \\\\ &amp;=\\sum_{i}^N \\gamma&#39;w_ix_i+\\gamma&#39;(b-\\mu_y)+\\beta \\tag{4} \\end{align} \\] 发现没有，(4) 式形式上跟 (1) 式一模一样，因此它本质上也是一个 Conv 运算，我们只需要用 \\(w_i&#39;=\\gamma&#39;w_i\\) 和 \\(b&#39;=\\gamma&#39;(b-\\mu_y)+\\beta\\) 来作为原来卷积的 weight 和 bias，就相当于把 BN 的操作合并到了 Conv 里面。实际 inference 的时候，由于 BN 层的参数已经固定了，因此可以把 BN 层 folding 到 Conv 里面，省去 BN 层的计算开销。 量化 BatchNorm Folding 量化网络时可以用同样的方法把 BN 合并到 Conv 中。 如果量化时不想更新 BN 的参数 (比如后训练量化)，那我们就先把 BN 合并到 Conv 中，直接量化新的 Conv 即可。 如果量化时需要更新 BN 的参数 (比如量化感知训练)，那也很好处理。Google 把这个流程的心法写在一张图上了： 由于实际 inference 的时候，BN 是 folding 到 Conv 中的，因此在量化训练的时候也需要模拟这个操作，得到新的 weight 和 bias，并用新的 Conv 估计量化误差来回传梯度。 Conv与ReLU合并 在量化中，Conv + ReLU 这样的结构一般也是合并成一个 Conv 进行运算的，而这一点在全精度模型中则办不到。 在之前的文章中说过，ReLU 前后应该使用同一个 scale 和 zeropoint。这是因为 ReLU 本身没有做任何的数学运算，只是一个截断函数，如果使用不同的 scale 和 zeropoint，会导致无法量化回 float 域。 看下图这个例子。假设 ReLU 前的数值范围是 \\(r_{in} \\in [-1, 1]\\)，那么经过 ReLU 后的数值范围是 \\(r_{out} \\in [0,1]\\)。假设量化到 uint8 类型，即 [0, 255]，那么 ReLU 前后的 scale 分别为 \\(S_{in}=\\frac{2}{255}\\)、\\(S_{out}=\\frac{1}{255}\\)，zp 分别为 \\(Z_{in}=128\\)、\\(Z_{out}=0\\)。 再假设 ReLU 前的浮点数是 \\(r_{in}=0.5\\)，那么经过 ReLU 后的值依然是 0.5。换算成整型的话，ReLU 前的整数是 \\(q_{in}=192\\)，由于 \\(Z_{in}=128\\)，因此过完 ReLU 后的数值依然是 192。但是，\\(S_{out}\\) 和 \\(Z_{out}\\) 已经发生了变化，因此反量化后的 \\(r_{out}\\) 不再是 0.5，而这不是我们想要的。所以，如果想要保证量化的 ReLU 和浮点型的 ReLU 之间的一致性，就必须保证 \\(S_{in}\\)、\\(S_{out}\\) 以及 \\(Z_{in}\\)、\\(Z_{out}\\) 是一致的。 但是保证前后的 scale 和 zp 一致，没规定一定得用 \\(S_{in}\\) 和 \\(Z_{in}\\)，我们一样可以用 ReLU 之后的 scale 和 zp。不过，使用哪一个 scale 和 zp，意义完全不一样。如果使用 ReLU 之后的 scale 和 zp，那我们就可以用量化本身的截断功能来实现 ReLU 的作用。 想要理解这一点，需要回顾一下量化的基本公式： \\[ q=round(\\frac{r}{S}+Z) \\tag{5} \\] 注意，这里的 round 除了把 float 型四舍五入转成 int 型外，还需要保证 \\(q\\) 的数值在特定范围内「例如 0～255」，相当于要做一遍 clip 操作。因此，这个公式更准确的写法应该是「假设量化到 uint8 数值」： \\[ q=round(clip(\\frac{r}{S}+Z, 0, 255)) \\tag{6} \\] 记住，ReLU 本身就是在做 clip。所以，我们才能用量化的截断功能来模拟 ReLU 的功能。 再举个例子。 假设有一个上图所示的 Conv+ReLU 的结构，其中，Conv 后的数值范围是 \\(r_{in} \\in [-1,1]\\)。在前面的文章中，我们都是用 ReLU 前的数值来统计 minmax 并计算 scale 和 zp，并把该 scale 和 zp 沿用到 ReLU 之后。这部分的计算可以参照图中上半部分。 但现在，我们想在 ReLU 之后统计 minmax，并用 ReLU 后的 scale 和 zp 作为 ReLU 前的 scale 和 zp「即 Conv 后面的 scale 和 zp」，结果会怎样呢？ 看图中下半部分，假设 Conv 后的数值是 \\(r_{in}=-0.5\\)，此时，由于 Conv 之后的 scale 和 zp 变成了 \\(\\frac{1}{255}\\) 和 \\(0\\)，因此，量化的整型数值为： \\[ \\begin{align} q&amp;=round(\\frac{-0.5}{\\frac{1}{255}}+0) \\notag \\\\ &amp;=round(-128) \\notag \\\\ &amp;=0 \\tag{7} \\end{align} \\] 注意，上面的量化过程中，我们执行了截断操作，把 \\(q\\) 从 -128 截断成 0，而这一步本来应该是在 ReLU 里面计算的！然后，我们如果根据 \\(S_{out}\\) 和 \\(Z_{out}\\) 反量化回去，就会得到 \\(r_{out}=0\\)，而它正是原先 ReLU 计算后得到的数值。 因此，通过在 Conv 后直接使用 ReLU 后的 scale 和 zp，我们实现了将 ReLU 合并到 Conv 里面的过程。 那对于 ReLU 外的其他激活函数，是否可以同样合并到 Conv 里面呢？这取决于其他函数是否也只是在做 clip 操作，例如 ReLU6 也有同样的性质。但对于其他绝大部分函数来说，由于它们本身包含其他数学运算，因此就不具备类似性质。 总结 这篇文章主要介绍了如何把 BatchNorm 和 ReLU 合并成一个 Conv，从而加速量化推理。按照计划，应该和之前的文章一样，给出代码实现。但我在测试代码的时候发现有一些 bug 需要解决，正好也控制一下篇幅，下篇文章会给出相关的代码实现。","raw":null,"content":null,"categories":[{"name":"深度学习","slug":"深度学习","permalink":"https://jermmy.github.io/categories/深度学习/"}],"tags":[{"name":"深度学习","slug":"深度学习","permalink":"https://jermmy.github.io/tags/深度学习/"}]},{"title":"神经网络量化入门--量化感知训练","slug":"2020-7-11-network-quantization-3","date":"2020-07-11T03:13:37.000Z","updated":"2020-07-28T15:08:50.000Z","comments":true,"path":"2020/07/11/2020-7-11-network-quantization-3/","link":"","permalink":"https://jermmy.github.io/2020/07/11/2020-7-11-network-quantization-3/","excerpt":"上一篇文章介绍了后训练量化的基本流程，并用 pytorch 演示了最简单的后训练量化算法。\n后训练量化虽然操作简单，并且大部分推理框架都提供了这类离线量化算法 (如 tensorrt、ncnn，SNPE 等)，但有时候这种方法并不能保证足够的精度，因此本文介绍另一种比后训练量化更有效地量化方法——量化感知训练。\n量化感知训练，顾名思义，就是在量化的过程中，对网络进行训练，从而让网络参数能更好地适应量化带来的信息损失。这种方式更加灵活，因此准确性普遍比后训练量化要高。当然，它的一大缺点是操作起来不方便，这一点后面会详谈。\n同样地，这篇文章会讲解最简单的量化训练算法流程，并沿用之前文章的代码框架，用 pytorch 从零构建量化训练算法的流程。","text":"上一篇文章介绍了后训练量化的基本流程，并用 pytorch 演示了最简单的后训练量化算法。 后训练量化虽然操作简单，并且大部分推理框架都提供了这类离线量化算法 (如 tensorrt、ncnn，SNPE 等)，但有时候这种方法并不能保证足够的精度，因此本文介绍另一种比后训练量化更有效地量化方法——量化感知训练。 量化感知训练，顾名思义，就是在量化的过程中，对网络进行训练，从而让网络参数能更好地适应量化带来的信息损失。这种方式更加灵活，因此准确性普遍比后训练量化要高。当然，它的一大缺点是操作起来不方便，这一点后面会详谈。 同样地，这篇文章会讲解最简单的量化训练算法流程，并沿用之前文章的代码框架，用 pytorch 从零构建量化训练算法的流程。 量化训练的困难 要理解量化训练的困难之处，需要了解量化训练相比普通的全精度训练有什么区别。为了看清这一点，我们回顾一下上一篇文章中卷积量化的代码： 1234567891011121314151617class QConv2d(QModule): def forward(self, x): if hasattr(self, 'qi'): self.qi.update(x) self.qw.update(self.conv_module.weight.data) self.conv_module.weight.data = self.qw.quantize_tensor(self.conv_module.weight.data) self.conv_module.weight.data = self.qw.dequantize_tensor(self.conv_module.weight.data) x = self.conv_module(x) if hasattr(self, 'qo'): self.qo.update(x) return x 这里面区别于全精度模型的地方在于，我们在卷积运算前先对 weight 做了一遍量化，然后又再反量化成 float。这一步在后训练量化中其实可有可无，但量化感知训练中却是需要的「之前为了代码上的一致，我提前把这一步加上去了」 那这一步有什么特别吗？可以回顾一下量化的具体操作： 123456789101112def quantize_tensor(x, scale, zero_point, num_bits=8, signed=False): if signed: qmin = - 2. ** (num_bits - 1) qmax = 2. ** (num_bits - 1) - 1 else: qmin = 0. qmax = 2.**num_bits - 1. q_x = zero_point + x / scale q_x.clamp_(qmin, qmax).round_() return q_x.float() 这里面有个 round 函数，而这个函数是没法训练的。它的函数图像如下： 这个函数几乎每一处的梯度都是 0，如果网络中存在该函数，会导致反向传播的梯度也变成 0。 可以看个例子： 1234567891011121314151617181920212223conv = nn.Conv2d(3, 1, 3, 1)def quantize(weight): w = weight.round() return wclass QuantConv(nn.Module): def __init__(self, conv_module): super(QuantConv, self).__init__() self.conv_module = conv_module def forward(self, x): return F.conv2d(x, quantize(self.conv_module.weight), self.conv_module.bias, 3, 1)x = torch.randn((1, 3, 4, 4))quantconv = QuantConv(conv)a = quantconv(x).sum().backward()print(quantconv.conv_module.weight.grad) 这个例子里面，我将权重 weight 做了一遍 round 操作后，再进行卷积运算，但返回的梯度全是 0： 1234567891011tensor([[[[0., 0., 0.], [0., 0., 0.], [0., 0., 0.]], [[0., 0., 0.], [0., 0., 0.], [0., 0., 0.]], [[0., 0., 0.], [0., 0., 0.], [0., 0., 0.]]]]) 换言之，这个函数是没法学习的，从而导致量化训练进行不下去。 Straight Through Estimator 那要怎么解决这个问题呢？ 一个很容易想到的方法是，直接跳过伪量化的过程，避开 round。直接把卷积层的梯度回传到伪量化之前的 weight 上。这样一来，由于卷积中用的 weight 是经过伪量化操作的，因此可以模拟量化误差，把这些误差的梯度回传到原来的 weight，又可以更新权重，使其适应量化产生的误差，量化训练就可以正常进行下去了。 这个方法就叫做 Straight Through Estimator(STE)。 pytorch实现 本文的相关代码都可以在 https://github.com/Jermmy/pytorch-quantization-demo 上找到。 伪量化节点实现 上面讲完量化训练最基本的思路，下面我们继续沿用前文的代码框架，加入量化训练的部分。 首先，我们需要修改伪量化的写法，之前的代码是直接对 weight 的数值做了伪量化： 12self.conv_module.weight.data = self.qw.quantize_tensor(self.conv_module.weight.data)self.conv_module.weight.data = self.qw.dequantize_tensor(self.conv_module.weight.data) 这在后训练量化里面没有问题，但在 pytorch 中，这种写法是没法回传梯度的，因此量化训练里面，需要重新修改伪量化节点的写法。 另外，STE 需要我们重新定义反向传播的梯度。因此，需要借助 pytorch 中的 Function 接口来重新定义伪量化的过程： 12345678910111213from torch.autograd import Functionclass FakeQuantize(Function): @staticmethod def forward(ctx, x, qparam): x = qparam.quantize_tensor(x) x = qparam.dequantize_tensor(x) return x @staticmethod def backward(ctx, grad_output): return grad_output, None 这里面的 forward 函数，和之前的写法是类似的，就是把数值量化之后再反量化回去。但在 backward 中，我们直接返回了后一层传过来的梯度 grad_output，相当于直接跳过了伪量化这一层的梯度计算，让梯度直接流到前一层 (Straight Through)。 pytorch 定义 backward 函数的返回变量需要与 forward 的输入参数对应，分别表示对应输入的梯度。由于 qparam 只是统计 min、max，不需要梯度，因此返回给它的梯度是 None。 量化卷积代码 量化卷积层的代码除了 forward 中需要修改伪量化节点外，其余的和之前的文章基本一致： 1234567891011121314151617181920class QConv2d(QModule): def forward(self, x): if hasattr(self, 'qi'): self.qi.update(x) x = FakeQuantize.apply(x, self.qi) self.qw.update(self.conv_module.weight.data) x = F.conv2d(x, FakeQuantize.apply(self.conv_module.weight, self.qw), self.conv_module.bias, stride=self.conv_module.stride, padding=self.conv_module.padding, dilation=self.conv_module.dilation, groups=self.conv_module.groups) if hasattr(self, 'qo'): self.qo.update(x) x = FakeQuantize.apply(x, self.qo) return x 由于我们需要先对 weight 做一些伪量化的操作，根据 pytorch 中的规则，在做卷积运算的时候，不能像之前一样用 x = self.conv_module(x) 的写法，而要用 F.conv2d 来调用。另外，之前的代码中输入输出没有加伪量化节点，这在后训练量化中没有问题，但在量化训练中最好加上，方便网络更好地感知量化带来的损失。 由于上一篇文章中做量化推理的时候，我发现精度损失不算太重，3 个 bit 的情况下，准确率依然能达到 96%。为了更好地体会量化训练带来的收益，我们把量化推理的代码再细致一点，加大量化损失： 12345678910class QConv2d(QModule): def quantize_inference(self, x): x = x - self.qi.zero_point x = self.conv_module(x) x = self.M * x x.round_() # 多加一个round操作 x = x + self.qo.zero_point x.clamp_(0., 2.**self.num_bits-1.).round_() return x 相比之前的代码，其实就是多加了个 round，让量化推理更接近真实的推理过程。 量化训练的收益 这里仍然沿用之前文章里的小网络，在 mnist 上测试分类准确率。由于量化推理有修改，为了方便对比，我重新跑了一遍后训练量化的准确率： bit 1 2 3 4 5 6 7 8 accuracy 10% 47% 83% 96% 98% 98% 98% 98% 接下来，测试一下量化训练的效果，下面是 bit=3 时输出的 log： 123456789101112131415161718Test set: Full Model Accuracy: 98%Quantization bit: 3Quantize Aware Training Epoch: 1 [3200/60000] Loss: 0.087867Quantize Aware Training Epoch: 1 [6400/60000] Loss: 0.219696Quantize Aware Training Epoch: 1 [9600/60000] Loss: 0.283124Quantize Aware Training Epoch: 1 [12800/60000] Loss: 0.172751Quantize Aware Training Epoch: 1 [16000/60000] Loss: 0.315173Quantize Aware Training Epoch: 1 [19200/60000] Loss: 0.302261Quantize Aware Training Epoch: 1 [22400/60000] Loss: 0.218039Quantize Aware Training Epoch: 1 [25600/60000] Loss: 0.301568Quantize Aware Training Epoch: 1 [28800/60000] Loss: 0.252994Quantize Aware Training Epoch: 1 [32000/60000] Loss: 0.138346Quantize Aware Training Epoch: 1 [35200/60000] Loss: 0.203350...Test set: Quant Model Accuracy: 90% 总的实验结果如下： bit 1 2 3 4 5 6 7 8 accuracy 10% 63% 90% 97% 98% 98% 98% 98% 用曲线把它们 plot 在一起： 灰色线是量化训练，橙色线是后训练量化，可以看到，在 bit = 2、3 的时候，量化训练能带来很明显的提升。 在 bit = 1 的时候，我发现量化训练回传的梯度为 0，训练基本失败了。这是因为 bit = 1 的时候，整个网络已经退化成一个二值网络了，而低比特量化训练本身不是一件容易的事情，虽然我们前面用 STE 解决了梯度的问题，但由于低比特会使得网络的信息损失巨大，因此通常的训练方式很难起到作用。 另外，量化训练本身存在很多 trick，在这个实验中我发现，学习率对结果的影响非常显著，尤其是低比特量化的时候，学习率太高容易导致梯度变为 0，导致量化训练完全不起作用「一度以为代码出错」。 量化训练部署 前面说过，量化训练虽然收益明显，但实际应用起来却比后训练量化麻烦得多。 目前大部分主流推理框架在处理后训练量化时，只需要用户把模型和数据扔进去，就可以得到量化模型，然后直接部署。但很少有框架支持量化训练。目前量化训练缺少统一的规范，各家推理引擎的量化算法虽然本质一样，但很多细节处很难做到一致。而目前大家做模型训练的前端框架是不统一的「当然主流还是 tf 和 pytorch」，如果各家的推理引擎需要支持不同前端的量化训练，就需要针对不同的前端框架，按照后端部署的实现规则「比如哪些层的量化需要合并、weight 是否采用对称量化等」，从头再搭一套量化训练框架，这个工作量想想就吓人。 总结 这篇文章主要介绍了量化训练的基本方法，并用 pytorch 构建了一个简单的量化训练实例。下一篇文章会介绍这系列教程的最后一篇文章——关于 fold BatchNorm 相关的知识。 参考 Torch.round() gradient pytorch实现简单的straight-through estimator(STE)","raw":null,"content":null,"categories":[{"name":"深度学习","slug":"深度学习","permalink":"https://jermmy.github.io/categories/深度学习/"}],"tags":[{"name":"深度学习","slug":"深度学习","permalink":"https://jermmy.github.io/tags/深度学习/"}]},{"title":"神经网络量化入门--后训练量化","slug":"2020-7-4-network-quantization-2","date":"2020-07-04T03:27:15.000Z","updated":"2020-07-11T04:00:31.000Z","comments":true,"path":"2020/07/04/2020-7-4-network-quantization-2/","link":"","permalink":"https://jermmy.github.io/2020/07/04/2020-7-4-network-quantization-2/","excerpt":"上一篇文章介绍了矩阵量化的基本原理，并推广到卷积网络中。这一章开始，我会逐步深入到卷积网络的量化细节中，并用 pytorch 从零搭建一个量化模型，帮助读者实际感受量化的具体流程。\n本章中，我们来具体学习最简单的量化方法——后训练量化「post training quantization」\n由于本人接触量化不久，如表述有错，欢迎指正。","text":"上一篇文章介绍了矩阵量化的基本原理，并推广到卷积网络中。这一章开始，我会逐步深入到卷积网络的量化细节中，并用 pytorch 从零搭建一个量化模型，帮助读者实际感受量化的具体流程。 本章中，我们来具体学习最简单的量化方法——后训练量化「post training quantization」 由于本人接触量化不久，如表述有错，欢迎指正。 卷积层量化 卷积网络最核心的要素是卷积，前文虽然有提及卷积运算的量化，但省略了很多细节，本文继续深入卷积层的量化。 这里我们继续沿用之前的公式，用 \\(S\\)、\\(Z\\) 表示 scale 和 zero point，\\(r\\) 表示浮点实数，\\(q\\) 表示定点整数。 假设卷积的权重 weight 为 \\(w\\)，bias 为 \\(b\\)，输入为 \\(x\\)，输出的激活值为 \\(a\\)。由于卷积本质上就是矩阵运算，因此可以表示成: \\[ a=\\sum_{i}^N w_i x_i+b \\tag{1} \\] 由此得到量化的公式: \\[ S_a (q_a-Z_a)=\\sum_{i}^N S_w(q_w-Z_w)S_x(q_x-Z_x)+S_b(q_b-Z_b) \\tag{2} \\] \\[ q_a=\\frac{S_w S_x}{S_a}\\sum_{i}^N (q_w-Z_w)(q_x-Z_x)+\\frac{S_b}{S_a}(q_b-Z_b)+Z_a \\tag{3} \\] 这里面非整数的部分就只有 \\(\\frac{S_w S_x}{S_a}\\)、\\(\\frac{S_b}{S_a}\\)，因此接下来就是把这部分也变成定点运算。 对于 bias，由于 \\(\\sum_{i}^N (q_w-Z_w)(q_x-Z_x)\\) 的结果通常会用 int32 的整数存储，因此 bias 通常也量化到 int32。这里我们可以直接用 \\(S_w S_x\\) 来代替 \\(S_b\\)，由于 \\(S_w\\)、\\(S_x\\) 都是对应 8 个 bit 的缩放比例，因此 \\(S_w S_x\\) 最多就放缩到 16 个 bit，用 32bit 来存放 bias 绰绰有余，而 \\(Z_b\\) 则直接记为 0。 因此，公式 (3) 再次调整为: \\[ \\begin{align} q_a&amp;=\\frac{S_w S_x}{S_a}(\\sum_{i}^N(q_w-Z_w)(q_x-Z_x)+q_b)+Z_a \\notag \\\\ &amp;=M(\\sum_{i}^N q_wq_x-\\sum_i^N q_wZ_x-\\sum_i^N q_xZ_w+\\sum_i^NZ_wZ_x+q_b)+Z_a \\tag{4} \\end{align} \\] 其中，\\(M=\\frac{S_w S_x}{S_a}\\)。 根据上一篇文章的介绍，\\(M\\) 可以通过一个定点小数加上 bit shift 来实现，因此公式 (4) 完全可以通过定点运算进行计算。由于 \\(Z_w\\)、\\(q_w\\)、\\(Z_x\\)、\\(q_b\\) 都是可以事先计算的，因此 \\(\\sum_i^N q_wZ_x\\)、\\(\\sum_i^NZ_wZ_x+q_b\\) 也可以事先计算好，实际 inference 的时候，只需要计算 \\(\\sum_{i}^N q_wq_x\\) 和 \\(\\sum_i^N q_xZ_w\\) 即可。 卷积网络量化流程 了解完整个卷积层的量化，现在我们再来完整过一遍卷积网络的量化流程。 我们继续沿用前文的小网络： 其中，\\(x\\)、\\(y\\) 表示输入和输出，\\(a_1\\)、\\(a_2\\) 是网络中间的 feature map，\\(q_x\\) 表示 \\(x\\) 量化后的定点数，\\(q_{a1}\\) 等同理。 在后训练量化中，我们需要一些样本来统计 \\(x\\)、\\(a_1\\)、\\(a_2\\) 以及 \\(y\\) 的数值范围「即 min, max」，再根据量化的位数以及量化方法来计算 scale 和 zero point。 本文中，我们先采用最简单的量化方式，即统计 min、max 后，按照线性量化公式: \\[ S = \\frac{r_{max}-r_{min}}{q_{max}-q_{min}} \\tag{5} \\] \\[ Z = round(q_{max} - \\frac{r_{max}}{S}) \\tag{6} \\] 来计算 scale 和 zero point。 需要注意的是，除了第一个 conv 需要统计输入 \\(x\\) 的 min、max 外，其他层都只需要统计中间输出 feature 的 min、max 即可。另外，对于 relu、maxpooling 这类激活函数来说，它们会沿用上一层输出的 min、max，不需要额外统计，即上图中 \\(a_1\\)、\\(a_2\\) 会共享相同的 min、max 「为何这些激活函数可以共享 min max，以及哪些激活函数有这种性质，之后有时间可以细说」。 因此，在最简单的后训练量化算法中，我们会先按照正常的 forward 流程跑一些数据，在这个过程中，统计输入输出以及中间 feature map 的 min、max。等统计得差不多了，我们就可以根据 min、max 来计算 scale 和 zero point，然后根据公式 (4) 对一些数据项提前计算。 之后，在 inference 的时候，我们会先把输入 \\(x\\) 量化成定点整数 \\(q_x\\)，然后按照公式 (4) 计算卷积的输出 \\(q_{a1}\\)，这个结果依然是整型的，然后继续计算 relu 的输出 \\(q_{a2}\\)。对于 fc 层来说，它本质上也是矩阵运算，因此也可以用公式 (4) 计算，然后得到 \\(q_y\\)。最后，根据 fc 层已经计算出来的 scale 和 zero point，推算回浮点实数 \\(y\\)。除了输入输出的量化和反量化操作，其他流程完全可以用定点运算来完成。 pytorch实现 有了上面的铺垫，现在开始用 pytorch 从零搭建量化模型。 下文的代码都可以在github上找到。 基础量化函数 首先，我们需要把量化的基本公式，也就是公式 (5)(6) 先实现： 12345678910111213141516171819202122232425262728293031def calcScaleZeroPoint(min_val, max_val, num_bits=8): qmin = 0. qmax = 2. ** num_bits - 1. scale = float((max_val - min_val) / (qmax - qmin)) # S=(rmax-rmin)/(qmax-qmin) zero_point = qmax - max_val / scale # Z=round(qmax-rmax/scale) if zero_point &lt; qmin: zero_point = qmin elif zero_point &gt; qmax: zero_point = qmax zero_point = int(zero_point) return scale, zero_pointdef quantize_tensor(x, scale, zero_point, num_bits=8, signed=False): if signed: qmin = - 2. ** (num_bits - 1) qmax = 2. ** (num_bits - 1) - 1 else: qmin = 0. qmax = 2.**num_bits - 1. q_x = zero_point + x / scale q_x.clamp_(qmin, qmax).round_() # q=round(r/S+Z) return q_x.float() # 由于pytorch不支持int类型的运算，因此我们还是用float来表示整数 def dequantize_tensor(q_x, scale, zero_point): return scale * (q_x - zero_point) # r=S(q-Z) 前面提到，在后训练量化过程中，需要先统计样本以及中间层的 min、max，同时也频繁涉及到一些量化、反量化操作，因此我们可以把这些功能都封装成一个 QParam 类： 1234567891011121314151617181920212223class QParam: def __init__(self, num_bits=8): self.num_bits = num_bits self.scale = None self.zero_point = None self.min = None self.max = None def update(self, tensor): if self.max is None or self.max &lt; tensor.max(): self.max = tensor.max() if self.min is None or self.min &gt; tensor.min(): self.min = tensor.min() self.scale, self.zero_point = calcScaleZeroPoint(self.min, self.max, self.num_bits) def quantize_tensor(self, tensor): return quantize_tensor(tensor, self.scale, self.zero_point, num_bits=self.num_bits) def dequantize_tensor(self, q_x): return dequantize_tensor(q_x, self.scale, self.zero_point) 上面的 update 函数就是用来统计 min、max 的。 量化网络模块 下面要来实现一些最基本网络模块的量化形式，包括 conv、relu、maxpooling 以及 fc 层。 首先我们定义一个量化基类，这样可以减少一些重复代码，也能让代码结构更加清晰： 1234567891011121314class QModule(nn.Module): def __init__(self, qi=True, qo=True, num_bits=8): super(QModule, self).__init__() if qi: self.qi = QParam(num_bits=num_bits) if qo: self.qo = QParam(num_bits=num_bits) def freeze(self): pass def quantize_inference(self, x): raise NotImplementedError('quantize_inference should be implemented.') 这个基类规定了每个量化模块都需要提供的方法。 首先是 __init__ 函数，除了指定量化的位数外，还需指定是否提供量化输入 (qi) 及输出参数 (qo)。在前面也提到，不是每一个网络模块都需要统计输入的 min、max，大部分中间层都是用上一层的 qo 来作为自己的 qi 的，另外有些中间层的激活函数也是直接用上一层的 qi 来作为自己的 qi 和 qo。 其次是 freeze 函数，这个函数会在统计完 min、max 后发挥作用。正如上文所说的，公式 (4) 中有很多项是可以提前计算好的，freeze 就是把这些项提前固定下来，同时也将网络的权重由浮点实数转化为定点整数。 最后是 quantize_inference，这个函数主要是量化 inference 的时候会使用。实际 inference 的时候和正常的 forward 会有一些差异，可以根据之后的代码体会一下。 下面重点看量化卷积层的实现： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152class QConv2d(QModule): def __init__(self, conv_module, qi=True, qo=True, num_bits=8): super(QConv2d, self).__init__(qi=qi, qo=qo, num_bits=num_bits) self.num_bits = num_bits self.conv_module = conv_module self.qw = QParam(num_bits=num_bits) def freeze(self, qi=None, qo=None): if hasattr(self, 'qi') and qi is not None: raise ValueError('qi has been provided in init function.') if not hasattr(self, 'qi') and qi is None: raise ValueError('qi is not existed, should be provided.') if hasattr(self, 'qo') and qo is not None: raise ValueError('qo has been provided in init function.') if not hasattr(self, 'qo') and qo is None: raise ValueError('qo is not existed, should be provided.') if qi is not None: self.qi = qi if qo is not None: self.qo = qo self.M = self.qw.scale * self.qi.scale / self.qo.scale self.conv_module.weight.data = self.qw.quantize_tensor(self.conv_module.weight.data) self.conv_module.weight.data = self.conv_module.weight.data - self.qw.zero_point self.conv_module.bias.data = quantize_tensor(self.conv_module.bias.data, scale=self.qi.scale * self.qw.scale, zero_point=0, signed=True) def forward(self, x): if hasattr(self, 'qi'): self.qi.update(x) self.qw.update(self.conv_module.weight.data) self.conv_module.weight.data = self.qw.quantize_tensor(self.conv_module.weight.data) self.conv_module.weight.data = self.qw.dequantize_tensor(self.conv_module.weight.data) x = self.conv_module(x) if hasattr(self, 'qo'): self.qo.update(x) return x def quantize_inference(self, x): x = x - self.qi.zero_point x = self.fc_module(x) x = self.M * x + self.qo.zero_point return x 这个类基本涵盖了最精华的部分。 首先是 __init__ 函数，可以看到我传入了一个 conv_module 模块，这个模块对应全精度的卷积层，另外的 qw 参数则是用来统计 weight 的 min、max 以及对 weight 进行量化用的。 其次是 freeze 函数，这个函数主要就是计算公式 (4) 中的 \\(M\\)、\\(q_w\\) 以及 \\(q_b\\)。由于完全实现公式 (4) 的加速效果需要更底层代码的支持，因此在 pytorch 中我用了更简单的实现方式，即优化前的公式 (4): \\[ q_a=M(\\sum_{i}^N(q_w-Z_w)(q_x-Z_x)+q_b)+Z_a \\tag{7} \\] 这里的 \\(M\\) 本来也需要通过移位来实现定点化加速，但 pytorch 中 bit shift 操作不好实现，因此我们还是用原始的乘法操作来代替。 注意到 freeze 函数可能会传入 qi 或者 qo​，这也是之前提到的，有些中间的模块不会有自己的 qi，而是复用之前层的 qo 作为自己的 qi。 接着是 forward 函数，这个函数和正常的 forward 一样，也是在 float 上进行的，只不过需要统计输入输出以及 weight 的 min、max 而已。有读者可能会疑惑为什么需要对 weight 量化到 int8 然后又反量化回 float，这里其实就是所谓的伪量化节点，因为我们在实际量化 inference 的时候会把 weight 量化到 int8，这个过程本身是有精度损失的 (来自四舍五入的 round 带来的截断误差)，所以在统计 min、max 的时候，需要把这个过程带来的误差也模拟进去。 最后是 quantize_inference 函数，这个函数在实际 inference 的时候会被调用，对应的就是上面的公式 (7)。注意，这个函数里面的卷积操作是在 int 上进行的，这是量化推理加速的关键「当然，由于 pytorch 的限制，我们仍然是在 float 上计算，只不过数值都是整数。这也可以看出量化推理是跟底层实现紧密结合的技术」。 理解 QConv2d 后，其他模块基本上异曲同工，这里不再赘述。 完整的量化网络 我们定义一个简单的卷积网络： 12345678910111213141516class Net(nn.Module): def __init__(self, num_channels=1): super(Net, self).__init__() self.conv1 = nn.Conv2d(num_channels, 40, 3, 1) self.conv2 = nn.Conv2d(40, 40, 3, 1, groups=20) # 这里用分组网络，可以增大量化带来的误差 self.fc = nn.Linear(5*5*40, 10) def forward(self, x): x = F.relu(self.conv1(x)) x = F.max_pool2d(x, 2, 2) x = F.relu(self.conv2(x)) x = F.max_pool2d(x, 2, 2) x = x.view(-1, 5*5*40) x = self.fc(x) return x 接下来就是把这个网络的每个模块进行量化，我们单独定义一个 quantize 函数来逐个量化每个模块： 12345678910class Net(nn.Module): def quantize(self, num_bits=8): self.qconv1 = QConv2d(self.conv1, qi=True, qo=True, num_bits=num_bits) self.qrelu1 = QReLU() self.qmaxpool2d_1 = QMaxPooling2d(kernel_size=2, stride=2, padding=0) self.qconv2 = QConv2d(self.conv2, qi=False, qo=True, num_bits=num_bits) self.qrelu2 = QReLU() self.qmaxpool2d_2 = QMaxPooling2d(kernel_size=2, stride=2, padding=0) self.qfc = QLinear(self.fc, qi=False, qo=True, num_bits=num_bits) 注意，这里只有第一层的 conv 需要 qi，后面的模块基本是复用前面层的 qo 作为当前层的 qi。 接着定义一个 quantize_forward 函数来统计 min、max，同时模拟量化误差： 123456789101112class Net(nn.Module): def quantize_forward(self, x): x = self.qconv1(x) x = self.qrelu1(x) x = self.qmaxpool2d_1(x) x = self.qconv2(x) x = self.qrelu2(x) x = self.qmaxpool2d_2(x) x = x.view(-1, 5*5*40) x = self.qfc(x) return x 下面的 freeze 函数会在统计完 min、max 后对一些变量进行固化： 12345678910class Net(nn.Module): def freeze(self): self.qconv1.freeze() self.qrelu1.freeze(self.qconv1.qo) self.qmaxpool2d_1.freeze(self.qconv1.qo) self.qconv2.freeze(qi=self.qconv1.qo) self.qrelu2.freeze(self.qconv2.qo) self.qmaxpool2d_2.freeze(self.qconv2.qo) self.qfc.freeze(qi=self.qconv2.qo) 由于我们在量化网络的时候，有些模块是没有定义 qi 的，因此这里需要传入前面层的 qo 作为当前层的 qi。 最后是 quantize_inference 函数，就是实际 inference 的时候用到的函数： 1234567891011121314class Net(nn.Module): def quantize_inference(self, x): qx = self.qconv1.qi.quantize_tensor(x) qx = self.qconv1.quantize_inference(qx) qx = self.qrelu1.quantize_inference(qx) qx = self.qmaxpool2d_1.quantize_inference(qx) qx = self.qconv2.quantize_inference(qx) qx = self.qrelu2.quantize_inference(qx) qx = self.qmaxpool2d_2.quantize_inference(qx) qx = qx.view(-1, 5*5*40) qx = self.qfc.quantize_inference(qx) out = self.qfc.qo.dequantize_tensor(qx) return out 这里我们会将输入 x 先量化到 int8，然后就是全量化的定点运算，得到最后一层的输出后，再反量化回 float 即可。 训练全精度网络 这一部分代码在 train.py 中，我们用 mnist 数据集来训练上面的网络： 1234567891011121314151617181920device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')train_loader = torch.utils.data.DataLoader( datasets.MNIST('data', train=True, download=True, transform=transforms.Compose([ transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,)) ])), batch_size=batch_size, shuffle=True, num_workers=1, pin_memory=True)test_loader = torch.utils.data.DataLoader( datasets.MNIST('data', train=False, transform=transforms.Compose([ transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,)) ])), batch_size=test_batch_size, shuffle=True, num_workers=1, pin_memory=True)model = Net().to(device) 具体训练细节比较简单，这里不再赘述。 训练完成后，我测试得到的准确率在 98% 左右。 后训练量化 这一部分代码在 post_training_quantize.py 中。 我们先加载全精度模型的参数： 12model = Net()model.load_state_dict(torch.load('ckpt/mnist_cnn.pt')) 然后对网络进行量化： 1model.quantize(num_bits=8) 接下来就是用一些训练数据来估计 min、max： 123456def direct_quantize(model, test_loader): for i, (data, target) in enumerate(test_loader, 1): output = model.quantize_forward(data) if i % 200 == 0: break print('direct quantization finish') 简单起见，我们就跑 200 个迭代。 然后，我们把量化参数都固定下来，并进行全量化推理： 1234567891011model.freeze()def quantize_inference(model, test_loader): correct = 0 for i, (data, target) in enumerate(test_loader, 1): output = model.quantize_inference(data) pred = output.argmax(dim=1, keepdim=True) correct += pred.eq(target.view_as(pred)).sum().item() print('\\nTest set: Quant Model Accuracy: &#123;:.0f&#125;%\\n'.format(100. * correct / len(test_loader.dataset)))quantize_inference(model, test_loader) 由于很多细节都封装在量化网络的模块中了，因此外部调用的代码跟全精度模型其实很类似。 我自己测试了 bit 数为 1～8 的准确率，得到下面这张折线图： 发现，当 bit &gt;= 3 的时候，精度几乎不会掉，bit = 2 的时候精度下降到 69%，bit = 1 的时候则下降到 10%。 这一方面是 mnist 分类任务比较简单，但也说明神经网络中的冗余量其实非常大，所以量化在分类网络中普遍有不错的效果「不过 bit =3 或 4 的时候效果依然这么好，让我依稀觉得代码里面应该有 bug，后续还要反复检查」。 总结 这篇文章主要补充了卷积层量化的细节，包括 bias 的量化，以及实际 inference 时一些优化的操作。并梳理了完整的卷积网络量化的流程。然后重点用 pytorch 从零搭建一个量化模型来帮助大家理解其中的细节，以及后训练量化算法的过程。代码是参考了这篇文章，加上自己拍脑袋构思的，存在很多不足之处，而且应该有不少 bug 存在，也欢迎大家指正。 之后的文章将继续讲述量化感知训练的流程，并补充其他量化的细节「例如 conv+relu 的合并等」，感谢大家赏脸关注。 参考 How to Quantize an MNIST network to 8 bits in Pytorch from scratch (No retraining required) Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference","raw":null,"content":null,"categories":[{"name":"深度学习","slug":"深度学习","permalink":"https://jermmy.github.io/categories/深度学习/"}],"tags":[{"name":"深度学习","slug":"深度学习","permalink":"https://jermmy.github.io/tags/深度学习/"}]},{"title":"神经网络量化入门--基本原理","slug":"2020-6-13-network-quantization-1","date":"2020-06-13T11:47:47.000Z","updated":"2020-07-05T03:21:18.000Z","comments":true,"path":"2020/06/13/2020-6-13-network-quantization-1/","link":"","permalink":"https://jermmy.github.io/2020/06/13/2020-6-13-network-quantization-1/","excerpt":"最近打算写一个关于神经网络量化的入门教程，包括网络量化的基本原理、离线量化、量化训练，以及全量化模型的推理过程，最后我会用 pytorch 从零构建一个量化模型，帮助读者形成更深刻的理解。\n之所以要写这系列教程，主要是想帮助初次接触量化的同学快速入门。笔者在刚开始接触模型量化时走了很多弯路，并且发现网上的资料和论文对初学者来说太不友好。目前学术界的量化方法都过于花俏，能落地的极少，工业界广泛使用的还是 Google TFLite 那一套量化方法，而 TFLite 对应的大部分资料都只告诉你如何使用，能讲清楚原理的也非常少。这系列教程不会涉及学术上那些花俏的量化方法，主要是想介绍工业界用得最多的量化方案 (即 TFLite 的量化原理，对应 Google 的论文 Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference )\n话不多说，我们开始。这一章中，主要介绍网络量化的基本原理，以及推理的时候如何跑量化模型。","text":"最近打算写一个关于神经网络量化的入门教程，包括网络量化的基本原理、离线量化、量化训练，以及全量化模型的推理过程，最后我会用 pytorch 从零构建一个量化模型，帮助读者形成更深刻的理解。 之所以要写这系列教程，主要是想帮助初次接触量化的同学快速入门。笔者在刚开始接触模型量化时走了很多弯路，并且发现网上的资料和论文对初学者来说太不友好。目前学术界的量化方法都过于花俏，能落地的极少，工业界广泛使用的还是 Google TFLite 那一套量化方法，而 TFLite 对应的大部分资料都只告诉你如何使用，能讲清楚原理的也非常少。这系列教程不会涉及学术上那些花俏的量化方法，主要是想介绍工业界用得最多的量化方案 (即 TFLite 的量化原理，对应 Google 的论文 Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference ) 话不多说，我们开始。这一章中，主要介绍网络量化的基本原理，以及推理的时候如何跑量化模型。 背景知识 量化并不是什么新知识，我们在对图像做预处理时就用到了量化。回想一下，我们通常会将一张 uint8 类型、数值范围在 0~255 的图片归一成 float32 类型、数值范围在 0.0~1.0 的张量，这个过程就是反量化。类似地，我们经常将网络输出的范围在 0.0~1.0 之间的张量调整成数值为 0~255、uint8 类型的图片数据，这个过程就是量化。所以量化本质上只是对数值范围的重新调整，可以「粗略」理解为是一种线性映射。(之所以加「粗略」二字，是因为有些论文会用非线性量化，但目前在工业界落地的还都是线性量化，所以本文只讨论线性量化的方案)。 不过，可以明显看出，反量化一般没有信息损失，而量化一般都会有精度损失。这也非常好理解，float32 能保存的数值范围本身就比 uint8 多，因此必定有大量数值无法用 uint8 表示，只能四舍五入成 uint8 型的数值。量化模型和全精度模型的误差也来自四舍五入的 clip 操作。 这篇文章中会用到一些公式，这里我们用 \\(r\\) 表示浮点实数，\\(q\\) 表示量化后的定点整数。浮点和整型之间的换算公式为： \\[ r = S(q-Z) \\tag{1} \\] \\[ q = round(\\frac{r}{S}+Z) \\tag{2} \\] 其中，\\(S\\) 是 scale，表示实数和整数之间的比例关系，\\(Z\\) 是 zero point，表示实数中的 0 经过量化后对应的整数，它们的计算方法为： \\[ S = \\frac{r_{max}-r_{min}}{q_{max}-q_{min}} \\tag{3} \\] \\[ Z = round(q_{max} - \\frac{r_{max}}{S}) \\tag{4} \\] \\(r_{max}\\)、\\(r_{min}\\)分别是 \\(r\\) 的最大值和最小值，\\(q_{min}\\)、\\(q_{max}\\)同理。这个公式的推导比较简单，很多资料也有详细的介绍，这里不过多介绍。需要强调的一点是，定点整数的 zero point 就代表浮点实数的 0，二者之间的换算不存在精度损失，这一点可以从公式 (2) 中看出来，把 \\(r=0\\) 代入后就可以得到 \\(q=Z\\)。这么做的目的是为了在 padding 时保证浮点数值的 0 和定点整数的 zero point 完全等价，保证定点和浮点之间的表征能够一致。 矩阵运算的量化 由于卷积网络中的卷积层和全连接层本质上都是一堆矩阵乘法，因此我们先看如何将浮点运算上的矩阵转换为定点运算。 假设 \\(r_1\\)、\\(r_2\\) 是浮点实数上的两个 \\(N \\times N\\) 的矩阵，\\(r_3\\) 是 \\(r_1\\)、\\(r_2\\) 相乘后的矩阵： \\[ r_3^{i,k}=\\sum_{j=1}^N r_1^{i,j}r_2^{j,k} \\tag{5} \\] 假设 \\(S_1\\)、\\(Z_1\\) 是 \\(r_1\\) 矩阵对应的 scale 和 zero point，\\(S_2\\)、\\(Z_2\\)、\\(S_3\\)、\\(Z_3\\)同理，那么由 (5) 式可以推出： \\[ S_3(q_3^{i,k}-Z_3)=\\sum_{j=1}^{N}S_1(q_{1}^{i,j}-Z_1)S_2(q_2^{j,k}-Z_2) \\tag{6} \\] 整理一下可以得到： \\[ q_3^{i,k}=\\frac{S_1 S_2}{S_3}\\sum_{j=1}^N(q_1^{i,j}-Z_1)(q_2^{j,k}-Z_2)+Z_3 \\tag{7} \\] 仔细观察 (7) 式可以发现，除了\\(\\frac{S_1 S_2}{S_3}\\)，其他都是定点整数运算。那如何把 \\(\\frac{S_1 S_2}{S_3}\\) 也变成定点运算呢？这里要用到一个 trick。假设 \\(M=\\frac{S_1 S_2}{S_3}\\)，由于 \\(M\\) 通常都是 (0, 1) 之间的实数 (这是通过大量实验统计出来的)，因此可以表示成 \\(M=2^{-n}M_0\\)，其中 \\(M_0\\) 是一个定点实数。注意，定点数并不一定是整数，所谓定点，指的是小数点的位置是固定的，即小数位数是固定的。因此，如果存在 \\(M=2^{-n}M_0\\)，那我们就可以通过\\(M_0\\)的 bit 位移操作实现 \\(2^{-n}M_0\\)，这样整个过程就都在定点上计算了。 很多刚接触量化的同学对这一点比较疑惑，下面我就用一个简单的示例说明这一点。我们把 \\(M=\\frac{S_1 S_2}{S_3}\\) 代入 (7) 式可以得到： \\[ q_3^{i,k}=M\\sum_{j=1}^N(q_1^{i,j}-Z_1)(q_2^{j,k}-Z_2)+Z_3=MP+Z_3 \\tag{8} \\] 这里面 \\(P\\) 是一个在定点域上计算好的整数。 假设 \\(P=7091\\)，\\(M=0.0072474273418460\\) (\\(M\\) 可以通过 \\(S\\) 事先计算得到)，那下面我们就是要找到一个 \\(M_0\\) 和 \\(n\\)，使得 \\(MP=2^{-n}M_0 P\\) 成立。我们可以用一段代码来找到这两个数： 12345678910111213M = 0.0072474273418460P = 7091def multiply(n, M, P): result = M * P Mo = int(round(2 ** n * M)) # 这里不一定要四舍五入截断，因为python定点数不好表示才这样处理 approx_result = (Mo * P) &gt;&gt; n print(\"n=%d, Mo=%d, approx=%f, error=%f\"%\\ (n, Mo, approx_result, result-approx_result))for n in range(1, 16): multiply(n, M, P) 输出： 123456789101112131415n=1, Mo=0, approx=0.000000, error=51.391507n=2, Mo=0, approx=0.000000, error=51.391507n=3, Mo=0, approx=0.000000, error=51.391507n=4, Mo=0, approx=0.000000, error=51.391507n=5, Mo=0, approx=0.000000, error=51.391507n=6, Mo=0, approx=0.000000, error=51.391507n=7, Mo=1, approx=55.000000, error=-3.608493n=8, Mo=2, approx=55.000000, error=-3.608493n=9, Mo=4, approx=55.000000, error=-3.608493n=10, Mo=7, approx=48.000000, error=3.391507n=11, Mo=15, approx=51.000000, error=0.391507n=12, Mo=30, approx=51.000000, error=0.391507n=13, Mo=59, approx=51.000000, error=0.391507n=14, Mo=119, approx=51.000000, error=0.391507n=15, Mo=237, approx=51.000000, error=0.391507 可以看到，在 n=11、\\(M_0=15\\) 的时候，误差就已经在 1 以内了。因此，只要 \\(M_0P\\) 的数值范围在 21(32-11) 个 bit 内，就可以通过对 \\(M_0P\\) 右移 \\(n\\) 个 bit 来近似 \\(MP\\) 了，而这个误差本身在可以接受的范围内。这样一来，(8) 式就可以完全通过定点运算来计算，即我们实现了浮点矩阵乘法的量化。 卷积网络的量化 有了上面矩阵乘法的量化，我们就可以进一步尝试对卷积网络的量化。 假设一个这样的网络： 这个网络只有三个模块，现在需要把 conv、fc、relu 量化。 假设输入为 \\(x\\)，我们可以事先统计样本的最大值和最小值，然后计算出 \\(S_x\\)(scale) 和 \\(Z_x\\)(zero point)。 同样地，假设 conv、fc 的参数为 \\(w_1\\)、\\(w_2\\)，以及 scale 和 zero point 为 \\(S_{w1}\\)、\\(Z_{w1}\\)、\\(S_{w2}\\)、\\(Z_{w2}\\)。中间层的 feature map 为 \\(a_1\\)，\\(a_2\\)，并且事先统计出它们的 scale 和 zero point 为 \\(S_{a1}\\)、\\(Z_{a1}\\)、\\(S_{a2}\\)、\\(Z_{a2}\\)。 卷积运算和全连接层的本质都是矩阵运算，因此我们可以把卷积运算表示成 (这里先忽略加 bias 的操作，这一步同样可以量化，不过中间有一些 trick，我们在之后的文章再仔细研究)： \\[ a_1^{i,k}=\\sum_{j=1}^N x^{i,j}w_1^{j,k} \\tag{9} \\] 根据之前的转换，我们可以得到： \\[ q_{a1}^{i,k}=M\\sum_{j=1}^N(q_x^{i,j}-Z_x)(q_{w1}^{j,k}-Z_{w1})+Z_{a1} \\tag{10} \\] 其中 \\(M=\\frac{S_{w1}S_{x}}{S_{a1}}\\)。 得到 conv 的输出后，我们不用反量化回 \\(a_1\\)，直接用 \\(q_{a1}\\) 继续后面的计算即可。 对于量化的 relu 来说，计算公式不再是 \\(q_{a2}=max(q_{a1}, 0)\\)，而是 \\(q_{a2}=max(q_{a1},Z_{a1})\\)，并且 \\(S_{a1}=S_{a2}\\)，\\(Z_{a1}=Z_{a2}\\) (为什么是这样，这一点留作思考题)。另外，在实际部署的时候，relu 或者 bn 通常是会整合到 conv 中一起计算的，这一点在之后的文章再讲解。 得到 \\(q_{a2}\\) 后，我们可以继续用 (8) 式来计算 fc 层。假设网络输出为 \\(y\\)，对应的 scale 和 zero point 为 \\(S_y\\)、\\(Z_y\\)，则量化后的 fc 层可以用如下公式计算： \\[ q_{y}^{i,k}=M\\sum_{j=1}^N(q_{a2}^{i,j}-Z_{a2})(q_{w2}^{j,k}-Z_{w2})+Z_{y}\\tag{11} \\] 然后通过公式 \\(y=S_y(q_y-Z_y)\\) 把结果反量化回去，就可以得到近似原来全精度模型的输出了。 可以看到，上面整个流程都是用定点运算实现的。我们在得到全精度的模型后，可以事先统计出 weight 以及中间各个 feature map 的 min、max，并以此计算出 scale 和 zero point，然后把 weight 量化成 int8/int16 型的整数后，整个网络便完成了量化，然后就可以依据上面的流程做量化推理了。 总结 这篇文章主要介绍了矩阵量化的原理，以及如何把矩阵量化运用到卷积网络中，实现全量化网络的计算。这中间忽略了很多细节，比如 relu 和 conv 的合并、激活函数的量化、量化训练的流程等。后面的文章会继续补充一些细节，并通过从零搭建一个 pytorch 的量化模型来帮助读者更好地理解中间的过程。 参考 神经网络量化简介 Building a quantization paradigm from first principles Post Training Quantization General Questions 量化训练：Quantization Aware Training in Tensorflow（一） How to Quantize an MNIST network to 8 bits in Pytorch from scratch (No retraining required) Aggressive Quantization: How to run MNIST on a 4 bit Neural Net using Pytorch TensorFlow Lite 8-bit quantization specification Post-training quantization","raw":null,"content":null,"categories":[{"name":"深度学习","slug":"深度学习","permalink":"https://jermmy.github.io/categories/深度学习/"}],"tags":[{"name":"深度学习","slug":"深度学习","permalink":"https://jermmy.github.io/tags/深度学习/"}]},{"title":"让计算机审美，这可能吗?","slug":"2019-8-6-is-it-possibile-for-computer-to-be-aesthetic","date":"2019-08-06T14:37:29.000Z","updated":"2019-11-13T14:19:40.000Z","comments":true,"path":"2019/08/06/2019-8-6-is-it-possibile-for-computer-to-be-aesthetic/","link":"","permalink":"https://jermmy.github.io/2019/08/06/2019-8-6-is-it-possibile-for-computer-to-be-aesthetic/","excerpt":"这一个月来一直在研究计算机美学 (photo aesthetic) 的课题，因为有一个需求是帮助用户筛选出一些拍的比较好的图片。这段时间陆陆续续看了很多相关的文章，也一直在思考这个问题：让计算机来对图片进行审美，到底有没有可能？毕竟审美是一件很主观的事情，美的定义本身也不清晰，让需要明确指令的计算机来做一件人类都不明确的事情，这看起来就不太现实。\n本文会记录一下我最近看过的一些文章，总结一下这个领域的研究思路，以及我个人的一些想法。","text":"这一个月来一直在研究计算机美学 (photo aesthetic) 的课题，因为有一个需求是帮助用户筛选出一些拍的比较好的图片。这段时间陆陆续续看了很多相关的文章，也一直在思考这个问题：让计算机来对图片进行审美，到底有没有可能？毕竟审美是一件很主观的事情，美的定义本身也不清晰，让需要明确指令的计算机来做一件人类都不明确的事情，这看起来就不太现实。 本文会记录一下我最近看过的一些文章，总结一下这个领域的研究思路，以及我个人的一些想法。 什么是计算机美学 狭义上讲，计算机美学 (photo aesthetic) 的研究内容是教计算机对图片审美，可以是输出一个分数，也可以是对图片分好坏，抑或是其他评价手段均可以。 当然，更广义的讲，凡是涉及到审美相关的领域都可以归结到计算机美学的范畴。从这个角度出发，可以衍生出大量的课题和应用，比如，让计算机扣图、生成一些非常专业的摄影那个图片等等。 题外话，与计算机审美相关的，还有另一个课题叫图像质量评估 (image quality assessment)。我的理解是，计算机美学偏向于主观感受，而图像质量评估则偏向于客观感受 (比如噪声、饱和度等客观因素)。前者由于太过主观，所以评价指标一般是跟数据集中已有的评分进行比较，而后者则有一些客观的评价标准 (如 PSNR，SSIM)。 下面，我们就根据不同的应用类型，看看各个相关领域的课题都是怎么玩的。 美学评分 最容易想到的应用自然是让计算机给一张图片进行打分。这里的打分可以是输出一个分数，也可以是对图片进行分类，抑或是输出一个美的等级等各种评价手段。前几年很火各种的测颜值 APP，就是美学评分最直接的应用。 这个任务现在来看似乎很简单，无非是将图片丢入一个模型后，再输出一个分数即可。但在没有深度学习的时代，人们通常只能计算一些手工特征（类似颜色直方图、饱和度等），再用一个分类器或回归模型来训练，效果并不好，因而只能是一些实验室 demo，或者在论文中灌灌水。而等到卷积网络大行其道后，美学评分也像其他任务一样，被研究人员从角落里捡起来，加入到各种刷分比赛中，甚至已经被工业界落地成产品。 要想让计算机对图像进行打分，就得先搞清楚美的定义是什么。遗憾的是，对于美的定义，连人类自己都模糊不清，一千个摄影师，就有一千张不同的摄影技巧。因此，为了推动这个领域的研究，人们构造了很多数据集，并且让很多人来对每张图片进行打分，从统计的意义上讲，只要打分的人数足够多，那么平均所有人的评分后，得到的均值就是符合大部分人审美的分数。目前常用的几个数据集包括 CUHK-PQ、AVA、AADB 等，其中被用得比较广的当属 AVA 了，因为这个数据集的图片数量异常庞大，且标签也比较丰富，因此可以挖的点更多。 接下来，美学打分的任务就是在这些数据集上刷分。 下面就简单讲几篇典型的文章，虽然灌水严重，但也不乏启发性很强的好文。 1. RAPID: Rating Pictorial Aesthetics using Deep Learning (ACM MM2014) 这应该算是最早使用深度学习做美学评分的论文了。在图像美学中，图片的整体布局 (global) 和细节内容 (local) 都是需要考虑的因素。为了让网络能同时捕获这些信息，这篇论文除了将整张图片 (global) 输入网络外，还从图片中随机抠出很多 patch (local) 输入网络，然后将二者的信息结合起来进行分类。 除此以外，由于这篇论文是在 AVA 上做的实验，作者发现 AVA 数据集除了评分外，还有一些 style 相关的属性分数，所以他们也探索了这部分信息的作用。由于不是每张图片都有 style 标签，所以这里面还用了迁移学习的方法来训练一个 pretrain 模型。具体细节不表。 当然，随着网络结构的发展，这篇论文的实验结果早已被超过，但它用 patch 作为输入的做法由于效果不错，因此一直被之后的论文采用。 2. Deep multi-patch aggregation network for image style, aesthetics, and quality estimation (ICCV 2015) 还是上一篇论文的作者，估计他们在之前的尝试中，发现 patch 对这种美学评分的任务效果很好，因此又专门针对 patch 做了一次研究，提出了 DMA 网络。这一次，他们只尝试了用 patch 作为输入，并设计了几种方法来融合这些 patch 的信息： 融合 patch 的方法与 MVCNN 很类似，从实验效果来看，将多个 patch 的特征进行融合，比单独输入 patch 的方式，效果提升十分明显。 3. A-Lamp: Adaptive Layout-Aware Multi-Patch Deep Convolutional Neural (CVPR 2017) 这篇论文咋一看标题和摘要，感觉能给很多的 insight，但实际看下去，发现有点 engineering。它是在 DMA-Net (也就是上一篇论文) 的基础上进一步改进的。 在 DMA-Net 中，作者探究了 patch 融合的威力，但他们默认这些 patch 是从图片中随机扣出来的，这样就没法保证 patch 之间不会重叠以及它们能覆盖住图中的关键信息。而这篇论文最大的改进之处就在于，它先用一个显著性检测模型抽取出一些更有代表性的 patch，然后再输入网络中提取特征。 显著性模型抽取到的 patch 可能很多，所以作者设计了几条原则，让程序可以自动筛选出需要的 patch，保证 patch 之间的重叠尽可能小，并尽可能覆盖整幅图的信息。 从实验效果来看，选择合适的 patch，对效果会有显著地提升 (图中 New-MP-Net 就是论文的方法)： 不过，这种方法需要用另一个显著性模型来抽取 patch，在模型方法上不是特别创新，在实际工程中由于计算量太大，也比较难接受。 另外，作者认为图片中物体之间的位置关系会影响图片的布局，而布局又进一步影响审美，所以他们又用另一个显著性模型找出图中的显著性物体，再手工提取出这些物体的位置信息，作为图片的 layout 属性。不过，我感觉这一步并不是很说得通，而且提取特征的步骤也非常的工程 (虽然论文包装得很好)，所以这一步就不展开讲了。 4. Attention-based Multi-Patch Aggregation for Image Aesthetic Assessment (ACM MM 2018) 这篇文章同样是针对 patch 的融合方法进行改进。它的 idea 非常简单直接，之前的 patch 融合操作都是采用了 max、min 或者 sort 等操作进行融合，而这篇论文则采用了喜闻乐见的 Attention 机制来计算每个特征对应的 weight，然后根据 weight 把所有特征融合起来。感兴趣的读者可以细看一下原文。 实验方面，也延续了一代更比一代强的传统，在 AVA 数据集上的准确率一举超过了以往的方法： 5. Composition-preserving Deep Photo Aesthetics Assessment (CVPR 2016) 前面说了这么多篇，都是 patch 相关的，现在来看看其他不一样的思路。 在 CNN 中，我们需要将图片调整到固定尺寸后才能输入网络，但这样就破坏了图片的布局。所以，为了保证图片的布局没有变化，就需要将最原始的图片输入到网络中，同时又要让网络能够处理这种变化的尺寸。 为此，这篇论文提出了一种 Adaptive Spatial Pooling 的操作： 这种操作根据动态地将不同 size 的 feature map 处理成指定的 size 大小。这个操作本质上就是 SPPNet，唯一的区别是 SPPNet 是用一个网络处理图片，并结合多个 Adaptive Spatial Pooling 得到多个 size 的 feature map，而这篇论文则是多个网络结合各自的 Adaptive Spatial Pooling，相当于每个网络提取的 low level 特征会略有差别。 另外，可能作者觉得自己直接拿别人的东西来改网络，工作量不太够，所以他们把场景信息 (scene) 也加入网络中学习。具体地，他们用一个场景分类网络提取图片的场景信息，再融合前面得到的特征作为图片总的特征： 从实验结果来看，保持图片的尺寸，可以让网络学出更好的美学特征 (下图中 VGG-Crop 等实验是将原图进行 crop 等操作后再输入网络)： 至于场景信息，从实验结果来看，加成作用并不明显。 6. Deep Aesthetic Quality Assessment with Semantic Information (TIP 2017) 不同的场景往往会有不同的拍照方式和审美标准，因此，把场景的语义信息也融合到网络中，对结果或多或少会有提升作用。上一篇论文虽然也考虑了场景信息，但它用了两个网络来分别提取美学特征和场景特征，因此特征融合的效果未必很好。 这篇论文同样考虑了场景信息，但它用 MTCNN 网络做了一番尝试： 从实验结果也可以看出，这种场景的语义信息可以提高美学分类的准确性 (注：\\(\\delta\\) 表示对好图和坏图的容忍度，具体请参考原论文。STCNN 表示没有加入场景语义信息)： 另外，这篇文章针对多任务学习提出了一种 Multi-Task Relationship Learning(MTRL) 的优化框架，分类准确率提高了 0.5 个点左右。 7. Photo Aesthetics Ranking Network with Attributes and Content Adaptation (ECCV 2016) 在之前的论文中，有研究人员已经发现场景信息可以辅助网络学出更好的特征，这篇论文的作者也意识到这一点，不过，他们进一步挖掘出更多的属性特征，包括三分线、内容是否有趣等，为此，他们还特意构建了一个数据集 AADB。这个数据集有一个很有意思的点，就是它记录了每个标注员的 id。对于美学标注这种十分主观的任务来说，不同人的评价标准是不一致的。对于 A、B 两张图片，第一个人可能觉得 A 的分数更高，而另一个人可能相反。这样的标注结果对模型的训练是有害的，尤其是在一些 ranking 相关的任务中。举个例子，假设甲、乙、丙三个人分别对 A、B、C 三张图片打分，结果如下： 甲 乙 丙 平均 A 5 5 1 3.66 B 1 2 5 2.66 C 2 5 5 4 从平均分可以看出，A、C 的评分很接近，而 C 的分数要明显高于 B，因此单从平均分排序的话，A 和 C 可能被分为一档，而 B 是单独一档。但单独看每个人的打分，情况却是这样的： 甲 \\(A &gt; B = C\\) （A 的评分明显更高，B、C 仅相差一分） 乙 \\(A = C &gt; B\\) 丙 \\(B = C &gt; A\\) 因此如果单纯用平均分让网络进行排序，可能会丢失很多信息。所以，在 rank loss 中，作者针对每个标注人员各自的评分采样 pair，比如，对于甲而言，采样到 A、B 两张图的评分就是 (5，1)，而不是平均分的 (3.66，2.66)。这样，网络对 A、B 两张图的区分能力可能会更大。当然，从甲乙丙各自的评分来看，不同人的评分可能是矛盾的，因此这种采样方式也未必能训练好网络，但这种思路还是值得借鉴的，只是方法可能可以再改进一下。 这篇文章总共采用了 regression loss 和 rank loss 两种损失函数，因此从网络结构上看，是一个明显的 multi-task 的结构： 实验结果请参考原论文。 8. NIMA: Neural Image Assessment (TIP 2018) 前面的文章要么是在网络结构上改善，要么是引入新的信息帮助网络进行训练，这篇文章从损失函数的角度，改进模型的训练。 在 AVA 数据集中，每张图片都被分为 10 个分数等级，并由 250 位以上的标注员进行打分，每个人可以选择 1～10 中的一档。因此，每张图片的分数其实可以用一个柱状图来表示。 在之前的论文中，研究人员都是将这 10 档分数分为好坏两段 (0～5 和 5～10)，这么做其实是把很好 (差) 的和比较好 (差) 的等同看待，从信息的角度来说是比较浪费的。 因此，为了利用上这部分信息，这篇论文在损失函数上进行了改进。作者让网络输出这 10 档分数的直方图分布： 再采用 Earth Move Distance 来衡量网络输出与真实的直方图之间的距离，以这个距离作为损失函数进行训练。Earth Move Distance 可以理解为 Wasserstein Distance 的离散版，可以用来衡量两个分布 (直方图) 的距离。具体计算方法请参考论文。 在实验效果上，也基本超过了当时最好的方法： 这篇文章来自 Google。相比起前面提到的论文，它最大的优势在于整个框架非常简单直接，效果不错，在工程上应用性更强。这也是 Google 一贯的作风。 9. Predicting Aesthetic Score Distribution through Cumulative Jensen-Shannon Divergence (AAAI2018) 这篇论文同样是在损失函数上做文章。与 NIMA 这篇论文相似之处在于，作者同样注意到要对 AVA 的直方图信息进行利用。因此，他们根据 JS Divergence 提出了一种衡量两个分布 (直方图) 差异的方法，并以此作为损失函数优化网络。具体细节请参考原文。 图片美化 前一节讲的美学评分其实是计算机美学里的关键问题，是众多美学任务中的基本任务。让计算机学会给图片打分，其实就是让计算机对好的和坏的图片做出大致的度量。一旦计算机可以学会这种度量，就可以进一步引导其他相关的任务。比如，对一张很差的图片进行美化。 1. Aesthetic-Driven Image Enhancement by Adversarial Learning (ACM MM 2018) 这篇论文，顾名思义，就是从美学的角度对图像进行增强。在以往大多数的研究中，图像增强一般需要成对的样本 (即需要 ground) 进行训练，但这需要大量的标注工作 (比如让专业人员用 PS 将低质量的图片处理成高质量的图片，当然有一种 trick，即反过来，我们先搜集大量高质量的图片，再用软件随机加噪声或者降低饱和度等等，从而批量制造低质量的图片)。图像增强的本质就是将一种图片转换成另一种图片 (类似 image-to-image)，之所以需要成对训练，就在于计算机不明白低质量和高质量这两种模态的差异，因此只能用成对的图片让它学会这种图片与图片之间的映射。而如果计算机本身能识别出好图和坏图，那可以认为它已经知道了如何去度量高质量和低质量图片的差异，这样的话，就可以用弱监督学习的方式，不需要借助成对图片进行训练了。 (通常用 GAN 作为训练方法)。 本文的核心思想就在于此。它采用 GAN 作为训练方式 (这也是这类弱监督学习的常用方法)。作者从颜色变换的角度，设计了一个生成网络对低质量图片进行增强，然后，他们又设计了一个判别器网络来分辨生成的图片以及其他真实的高清图片。 这里的判别器本身不是一个随机初始化的网络，而是已经训练过，能很好地识别低质量和高质量图片的模型。因此，这个判别器本身可以度量这两个分布之间的差异，它可以进一步引导生成器学习，从而摆脱对 ground truth 的依赖。 2. Creatism: A deep-learning photographer capable of creating professional work (arXiv 2017) 这篇论文出自 Google 之手，事实上并没有正式发表，但从结果来看，确实非常的惊艳👇 这篇论文主要做的是一个自动后期处理工具。说的直白点，就是学习一个滤波器对图片进行增强。论文最值得借鉴的做法就是把整个图像增强的任务分解为一系列互不影响的滤波器，通过把这些滤波器串联起来，完成最后的增强效果。这其中，有的滤波器负责调整饱和度，有的负责调整光照，有的裁剪图片调整构图，等等。而为了让这些滤波器互不影响，在训练的时候，需要针对不同的滤波器，用不同的数据进行训练。举个例子，在训练饱和度相关的滤波器时，我们先用一些图像处理工具，只针对饱和度这一点在原图上进行修改，这样就得到修改前和修改后的一对图片，再根据这对图片来训练饱和度相关的滤波器。其他滤波器也同理。这种用一对图片、并针对某个因素单独训练的方法，相比上一篇论文而言，可以降低模型学习的难度。当然，这样训练的前提条件是我们手头上要有质量很高的图片，如此一来才能通过一些图像处理软件进行处理，批量得到质量很差的图片，从而用一对图片进行训练。 论文的另一个难点在于对图片进行打光处理。在自然环境下，物体反射的光并不是均匀的，受材质、光源等各种因素影响，总是有部分地方的光比较亮，有部分地方的光比较暗。比如下面这张图片，由于云遮挡了阳光，导致部分山体的光线偏弱。适当的光照，可以让图片质量提升一个档次。 不过，这里我们没法再套用前面的方法来生成一对图片样本了。因为光照本身是一个很复杂的东西，如何用软件来批量添加或去除光照，本身就可以当作一个课题研究了。既然没法用软件来处理，那就只能换种训练方式了。论文采用 GAN 的弱监督学习方法，来训练光线处理的滤波器。举个例子，我们假设现在手上拥有的图片都是一些光照处理非常好的图片，接着，我们可以用图像处理工具大幅度的降低这些图片的亮度。注意，这种降低对所有像素是「一视同仁」的，并不会刻意区分哪些区域应该更低，哪些区域应该更亮。然后，我们用一个生成网络来生成一个光照的 mask，把它作用到这些亮度很低的图片上进行打光，并用一个判别器网络来区分原图和这些打光后的图片。通过这种博弈的方式来训练生成网络。只要训练得好，最后生成器就能生成出一个打光效果最好的 mask，这个 mask 会告诉我们哪些地方亮度要更高，哪些地方亮度要更低。下图分别是打光前、mask 以及打光后的图片效果： 更具体的细节，还请参考原论文。 自动裁剪 自动裁剪，就是在图片中裁出一块区域，这块区域要么构图很好，要么包含最主要的内容，总之，裁出来的这块图片总体上比原图要更加赏心悦目。在计算机美学出现的同时，图片自动裁剪作为一种衍生品也在不断发展。工业界中已经有相关的产品落地，比如 iphone xs 的相册中，有一个功能是对相册图片进行裁剪，并把裁剪出来的图片以幻灯片的形式做成视频，还有常见的缩略图功能也会用到自动裁剪的技术。 目前，自动裁剪技术的实现方案局限在两种思路上。第一种，借用注意力机制 (或者显著性物体检测)，裁剪出图片中最重要的部分；第二种，借助计算机美学评分，在众多滑动窗口中选出美学评分最高的窗口进行裁剪。 1. Learning the Change for Automatic Image Cropping (CVPR 2013) 这篇论文可以算是深度学习兴起前，传统方法的典型作品了。其思路就是人工定义很多的规则抽取 feature，再用 SVM 等工具进行训练，测试的时候再暴力遍历很多窗口，把分数最高的作为最好的裁剪窗口。在特征提取上，这篇论文融合了显著性检测和美学评分两种思路，先把图片分成前后景，再根据前后景之间的颜色差异、纹理差异、显著区域的位置关系等多种规则提取特征 (由于这篇论文关注的是前后景之间的差异，所以叫 learning the change)，然后训练模型。在测试时，结合一些启发式搜索的策略，可以剔除掉大量候选窗口，以此提高算法的速度。 另外，这篇论文还构建了一个包含 1000 张图片的数据集。 2. Automatic Image Cropping for Visual Aesthetic Enhancement Using Deep Neural Networks and Cascaded Regression (TMM 2017) 这篇论文算是深度学习在自动抠图方面的早期尝试了 (虽然论文发表于 2017 年，但由于是期刊论文，所以实际成稿时间会早很多)。它的基本思路非常简单，就是先初始化一个矩形框，用 CNN 提取矩形框内图像的特征 (由于矩形框大小不一，为此还用了 SPPNet 的技术)，之后用回归模型来拟合特征和 ground truth 中矩形框的坐标。这里的 CNN 是在 AVA 和 CUHK-PQ 等数据集上预训练过的。在回归模型中，采用的损失函数是 MSE Loss。这里面，CNN 和回归模型的训练是分开的，之所以要这样做，是因为当时的数据量非常有限，论文当时实验的数据集就是上一篇论文采集的 1000 张图片，因此并不具备训练 CNN 大模型的条件。在回归模型方面，作者采用了 gradient boosting 的方法逐次训练若干个回归器，每个回归器的输出是对矩形框位置的微调系数，由于是 boosting 的思路，所以每个回归器也是在上一个回归器的基础上进一步训练微调的。 具体细节可参考原文。在数据量突飞猛进的今天，学术界和工业界更普遍的做法是用一个完整的模型实现端到端的训练。 3. Deep Cropping via Attention Box Prediction and Aesthetics Assessment (ICCV 2017) 回想我们人类自己在裁剪图片的时候，都是先找出最重要的一块区域，框起来，再逐步调整这个裁剪框。这篇论文把这个过程建模为两步：首先，让网络找出图片中最重要的区域，并微调，由此得到一系列候选框，紧接着，用一个美学评分模型对这些候选框进行挑选，找出评分最高的候选框。这个思路跟其他论文的做法没有本质区别，但由于它在找候选框的过程中借用了物体检测的思想，跟其他基于显著性检测或者启发式搜索的方法不同，再加上它把整个论文的 motivation 跟人类自己的操作流程结合起来，因此显得很新颖。 论文的算法框架分为 ABP(Attention Box Prediction) 和 AA(Aesthetics Assessment) 两个网络。前一个网络用于生成候选框，后一个网络用于打分。ABP 网络的训练需要大量的候选框 ground truth 作参考，由于作者是从注意力的角度出发定义的候选框，因此他们采用了一个视觉注意力相关的数据集 SALICON 来生成 ground truth。除了包围注意力区域的候选框外 (下图 c)，再根据 IOU 生成更多的正候选框 (注意力区域) 和负候选框 (背景区域) (下图 d)，以此获得大量的训练样本。 注意力区域和候选框生成 之后，他们用这些候选框来训练 ABP 网络： ABP网络 整个网络的结构和损失函数的设计，像极了 Faster RCNN。有了 ABP 网络后，我们就可以根据预测的概率分数选出最好的候选框，不过，为了防止这一步出差错，作者把这个候选框适当增大，获得更多候选框，之后，就是用 AA 网络来预测每个候选框的分数。 这篇论文的工作量有点大，毕竟 Faster RCNN 本身就不是简单的东西。但看完后我总感觉有点南辕北辙。论文是先预测 Attention Map 所在的候选矩形框，然后再用 AA 网络挑选最好的，那为何不直接让网络预测 Attention Map，再根据 Attention Map 来生成候选矩形框呢？没想通。。 4. A2-RL: Aesthetics Aware Reinforcement Learning for Image Cropping (CVPR 2018) 这篇论文从强化学习的思路入手解决自动裁剪的问题。 自动裁剪的过程可以理解为不断地调整矩形框，直到获得较高的美学评分。这就是一个不断试错的过程，和强化学习本身的工作原理非常类似。由于我对强化学习一窍不通，具体算法请查看原文。 算法框架 5. Reliable and Efficient Image Cropping: A Grid Anchor based Approach (CVPR 2019) 这篇论文的核心思想都在题目的 Reliable 和 Efficient 两个词中，即裁剪出来的区域必须是真的好看的（Reliable），而且裁剪的过程要足够的高效，不能花太长的时间（Efficient）。 那如何评价是否 Reliable 呢？之前的方法都是将模型输出的裁剪框跟数据集中的 ground truth 计算 IoU 以及裁剪框四个坐标点的差值，这很明显是采纳了物体检测中的评价标准。但对于图片裁剪来说，这并不是一个特别好的评价标准。理由见下面这张图的最后一列。 最后一列中，最底下的图片跟中间的图片有更高的IoU值，但它裁剪出来的内容很差。 因此，论文提出了两个新的评估指标：SRCC (Spearman’s rank-order correlation) 和 \\(Acc_{K/N}\\) (return K of top-N accuracy)。 SRCC 是衡量两个排序相关性常用的指标。虽然 IoU 没法衡量裁剪框的可靠性，但对于同一张图片来说，模型输出的所有裁剪框之间的好坏关系大体还是要跟 ground truth 匹配上的。在实际使用时，需要把 ground truth 中标好的矩形框让模型来一一打分，然后用 SRCC 计算这些矩形框的分数高低跟真实标定的分数的一致性。 \\(Acc_{K/N}\\) 则是这篇论文提出的一种计算方式。在实际应用中，用户更在意的可能是算法输出的若干个裁剪框中有多少个是真正合理的。因此，可以比对一下 ground truth 标记分数最高的前 N 个裁剪框以及模型计算得到的分数最高的前 K 个裁剪框，看看这个交集有多大。交集越大，证明模型输出的裁剪框质量越好，模型的可靠性就越高。 解决 Reliable 问题还需要有一个质量非常高的数据集，毕竟算法是没法知道美丑的，只能通过数据本身来告诉它。论文构建了一个包含 1236 张图片，总共标记有 106860 个裁剪框的数据集（GAICD），其中每个裁剪框都包含有若干个人的打分，根据他们的平均值作为每个裁剪框的美学分数。 针对算法效率的问题，这篇论文采用的方法是人为制定一些规则来限制裁剪框的位置。由于图片中的主体或者主要内容一般位于中间位置，因此可以把裁剪框的位置「大致」固定在中间，并设定它的面积应该大于某个阈值。由此可以将裁剪框的搜索面积大比例缩小。 将裁剪框左上角和右下角的点限制在蓝色区域内 下图是论文算法的总体框架，由于裁剪框已经事先定义好了，因此只要逐个预测裁剪框内的分数即可。总体模型就是一个回归模型。作者先用一个网络提取整张图片的特征，并在最后的特征图上，根据裁剪框内的区域计算分数。由于分数预测的模块比较小，即使要计算的矩形框很多，总体花费的时间并不大。这里用到一个 trick：由于构图这个东西除了考虑裁剪框内的因素外，还需要考虑框内的元素在整幅图中的效果，即需要兼顾 local 和 global 的信息，因此在计算分数的时候，作者把裁剪框内的 feature 以及整幅图 (挖掉裁剪框内的部分) 的 feature 结合在一起，共同输入网络中计算 (不过这样没法体现位置信息，因此对这种做法我持保留态度)。 算法框架 从实验结果来看，这篇论文的算法效率有很大的优势，裁剪效果也不差： 从结果来看，这篇论文的算法效率很感人 自动构图 自动构图 (Compose) 与自动裁剪 (Crop) 本质上属于一个东西，都是让计算机从图片中找出更好看的区域。但相比较而言，自动构图更侧重于美学中的构图因素。 1. Learning to Compose with Professional Photographs on the Web (ACM MM 2017) 这篇论文的思路基于一个简单的假设：专业摄影师拍出来的图片一般具备比较好的构图，而如果从他们的图片中随机抠出一块，那抠出的图片大概率就毁了。也就是说，原图在构图方面的分数应该高于抠出来的图片。而这种比较的方式，可以很方便地用 Siamese Network 和 hinge loss 实现。另外，这篇论文另一个讨人喜欢的地方在于，它几乎不需要标注数据，只需要在网上爬取很多专业图片，再随机抠图就可以快速构造大量训练样本，因此成本近乎为零，即使精度不高也可以接受。 当然，这篇论文的训练方式只能让网络知道哪种图片的构图好，而无法自动从原图中抠出构图好的图块，因此，在抠图方面，采用的是滑动窗口的策略，并根据网络输出的分数决定哪个窗口最好。 2. Good View Hunting: Learning Photo Composition from Dense View Pairs (CVPR 2018) 论文效果图，左边是原图，右边是裁剪的结果。 上一篇论文虽然方法简单，但由于训练数据太粗糙，所以效果不可能很好。为此，这篇论文的作者自己制作了一个数据集 CPC。这个数据集中的数据也是采用一对图片 (准确的说，是从图片里抠出的 patch) 进行比较 (rank) 的形式标注的 (实际标注步骤分两步，高效合理，值得借鉴)，整个数据集总共包含一百多万对图片的标注。 在抠图方面，上一篇论文采用的是滑动窗口这种传统的方法，但这种暴力搜索的方法耗时耗力。受近几年很多物体检测方法的启发，作者采用了 MultiBox 的思路来让网络快速搜索出构图分数更高的窗口。在物体检测中，网络是根据 bounding box 训练的，但在抠图任务里，bounding box 的引导作用并不大。换句话说，两个位置大小相差不大的窗口，其构图分数可能相差很大，但两个相差很大的窗口，却可能有着类似的分数。而且，我们已有的标注数据就只有上面提到的 ranking 数据。 最容易想到的训练方式，就是像上一篇论文一样，直接用 ranking 数据训练网络，再对一个个窗口做判断，不过这样一来这篇论文就没意义了。为了能结合物体检测的优势，作者采用了知识蒸馏的方法，先用 ranking 数据训练一个评分网络 (VEN)，再用这个网络训练一个窗口区域选择网络 (VPN)。通过这种巧妙的方法，可以得到一个既能快速抽取窗口的、又能比较准确地给出构图分数的网络。在训练过程中，作者采用了一个跟 ranking 相关的损失函数 MPSE(Mean Pairwise Square Error)，也很值得借鉴。 训练框架 如果有一天，计算机可以在你拍照的时候告诉你哪个角度构图最好，那就是一个成熟的构图了。 美学评价 要是有一种技术，可以告诉我们一副图像拍的如何就好了。随着计算机美学和看图说话技术的发展，这项技术或许将出现在我们日常生活中。 1. Aesthetic Critiques Generation for Photos (ICCV 2017) 开启美学评价这项工作 (坑) 的是台湾的学者。这篇文章顾名思义，就是让机器生成一些跟图片美学相关的评价。作为第一个吃螃蟹的人，如何定义问题是关键的一步。当我们在评价一张图片的时候，我们一般在想什么。大多数情况下，无非是图片的构图好不好，色彩饱和度够不够，主体突不突出等等。遵循这个思路，作者把原问题分解了一下，他们选定几个维度 (比如饱和度、色彩光照等)，让机器只针对这几个角度，分别进行评价。这样一来，这个任务跟一般的 image caption 任务就比较类似了。一般的 image caption 任务通常是描述图片中的物体在做什么事情，而本文的任务是描述图像中的某些属性 (饱和度等) 效果怎么样 (好/差)。 有了基本思路后，作者在一个图像评论网站上爬取了图片和评论数据，并制作了该任务首个数据集 PCCD，这个数据集中包含 4000+ 图片，每张图片对应 7 句描述 (分别对应 7 个维度，包括总体印象、光照、饱和度等)，同时每张图片会有 7 个打分 (也分别对应这 7 个维度)。除此以外，论文还定义了一个比较适合该任务的评价标准 SPICE，感兴趣的读者可以参考原论文。下图是数据集中的两个样本： 每张图片都包含7句描述，分别对应7个维度，每个维度包含一个打分 在算法框架上，论文基本是参照了 image caption 中一些比较成功的模型。最简单也最容易想到的方法就是直接套用 CNN-LSTM 框架，并针对美学任务本身做一些定制操作 (比如把 CNN 换成针对美学评分的 CNN)。当然实际操作时会遇到很多问题，论文遇到的最大的问题就是生成的评价很单一 (比如总是生成针对饱和度的评价)，这一点可能跟数据集中的数据分布有关。另一个思路则是专门针对不同的维度训练不同 CNN-LSTM 模型 (AO, aspect-oriented)，如下图所示： 先用一个 CNN 预测图像的美学类型 (饱和度或者构图等)，再把 CNN 抽取的特征输入 LSTM 生成 caption，训练的时候可以根据 CNN 预测的美学类型，用对应类型的评价作为 ground truth 来训练模型。这种方法本身应该是有效的，但却过于繁琐，所以论文又给出了另一种方法，将这些不同维度的模型融合成一个统一的框架 (AF, aspect-fusion)，见下图： 它的思路是把训练好的单维度模型 (AO) 提取到的隐变量 (hidden state) 抽取出来，然后用 Attention 机制一顿操作后生成新的 LSTM 的隐变量 (hidden state)， 下面是部分实验结果： 成功的结果 从生成的句子来看，还有鼻子有眼的，而且基本跟图片内容很 match。当然也不乏失败的结果： 失败的结果 总之，作为一个新挖的坑，这篇论文给了一个很好的研究思路，也给之后的研究提供了足够的提升空间。 2. Aesthetic Attributes Assessment of Images (ACM MM 2019) 上一篇论文虽然挖好了坑，但挖的不够深。这篇论文从两个角度进一步完善该任务：1) 扩充一个更大的数据集；2) 让网络可以同时输出多个维度属性的描述 (上一篇论文会根据 CNN 预测的最大概率的属性来生成描述，因此每幅图只会生成一句描述)。 首先，要构建一个更大的数据集。PCCD 中只包含了 4000+ 的图片，数量上比起 AVA 等经典美学数据集 (250000+) 来说，实在太小。所以这篇论文又从 DPChallenge 上面爬取了三十万图片以及相应的描述，然后从 PCCD 中选取了几个属性以及关键词来清洗和整理这些数据，最后得到 150000+ 条数据。下面是该任务几个数据集的对比： 这个数据集和 PCCD 相比，除了规模更大之外，另一个区别在于每张图片的每个属性不在提供评分，因为图片数量太多了，想给每个属性一一评分几乎不可能。不过好在每条评论都会对应唯一的属性，这样多少也算是一种弱标注吧。 有了这个数据后，就可以跑更大更浮夸的模型了。给个模型框架图感受一下： 作者是在 PCCD 上先根据每个属性的评分预训练一番，再在自己的数据上 finetune 的。但由于两个数据集规模相差巨大，我很怀疑这种预训练的效果。从算法框架来看，依然是经典的 CNN+LSTM+Attention 的组合。 下面是论文模型在 PCCD 上的实验结果，从结果来看，论文在 SPICE 指标上的性能都要高出一截。当然啦，论文的模型是在大数据集上训练的，所以跟前一篇论文的结果 (AO, AF) 没有可比性，但大数量对模型的效果应该有很大的帮助。 由于这个课题有一定应用前景，跟自然语言处理的联系也比较紧密，算是一个跨模态任务，可以挖的点很多，可以预见，未来会有更多的工作填这个坑。 总结 总的来说，计算机美学由于主观性非常强，因此难点在于如何把问题定义好。比如美学评价里面，如何对图片进行评价就是最根本的问题，而自动裁剪任务中，又需要研究怎样裁剪得到的图片最好。根据定义好的问题，收集数据集，再建立相关的模型进行学习。这个任务另一个难点在于建模本身非常的困难，因为我们不清楚美学本身的数学模型长什么样子，因此也就很难在模型中加入一些显性的知识作为引导。熟悉这块工作的读者也会发现，在机器学习尤其是深度学习的背景下，几乎所有计算机美学的模型都是针对数据集进行构建的，而这些模型本身是否真的在学习鉴赏图片，nobody knows. 另外，看了这么多论文后，也可以对各类研究工作归个类。打个比方就是：一流的工作挖坑，二流的工作用推土机填坑，三流的工作修推土机。 参考 干掉柯洁的下一步，阿尔法狗创始人又要毁掉这个行业（深度学习） 深度学习在摄影技术中的应用与发展 【技术综述】图像美学质量评价调研报告","raw":null,"content":null,"categories":[{"name":"计算机视觉","slug":"计算机视觉","permalink":"https://jermmy.github.io/categories/计算机视觉/"}],"tags":[{"name":"计算机视觉","slug":"计算机视觉","permalink":"https://jermmy.github.io/tags/计算机视觉/"},{"name":"深度学习","slug":"深度学习","permalink":"https://jermmy.github.io/tags/深度学习/"},{"name":"论文","slug":"论文","permalink":"https://jermmy.github.io/tags/论文/"}]},{"title":"PyTorch中的C++扩展","slug":"2019-5-12-pytorch-cpp-extension","date":"2019-05-12T15:47:07.000Z","updated":"2019-06-02T07:01:23.000Z","comments":true,"path":"2019/05/12/2019-5-12-pytorch-cpp-extension/","link":"","permalink":"https://jermmy.github.io/2019/05/12/2019-5-12-pytorch-cpp-extension/","excerpt":"今天要聊聊用 PyTorch 进行 C++ 扩展。\n在正式开始前，我们需要了解 PyTorch 如何自定义 module。这其中，最常见的就是在 python 中继承 torch.nn.Module，用 PyTorch 中已有的 operator 来组装成自己的模块。这种方式实现简单，但是，计算效率却未必最佳，另外，如果我们想实现的功能过于复杂，可能 PyTorch 中那些已有的函数也没法满足我们的要求。这时，用 C、C++、CUDA 来扩展 PyTorch 的模块就是最佳的选择了。\n由于目前市面上大部分深度学习系统（TensorFlow、PyTorch 等）都是基于 C、C++ 构建的后端，因此这些系统基本都存在 C、C++ 的扩展接口。PyTorch 是基于 Torch 构建的，而 Torch 底层采用的是 C 语言，因此 PyTorch 天生就和 C 兼容，因此用 C 来扩展 PyTorch 并非难事。而随着 PyTorch1.0 的发布，官方已经开始考虑将 PyTorch 的底层代码用 caffe2 替换，因此他们也在逐步重构 ATen，后者是目前 PyTorch 使用的 C++ 扩展库。总的来说，C++ 是未来的趋势。至于 CUDA，这是几乎所有深度学习系统在构建之初就采用的工具，因此 CUDA 的扩展接口是标配。\n本文用一个简单的例子，梳理一下进行 C++ 扩展的步骤，至于一些具体的实现，不做深入探讨。","text":"今天要聊聊用 PyTorch 进行 C++ 扩展。 在正式开始前，我们需要了解 PyTorch 如何自定义 module。这其中，最常见的就是在 python 中继承 torch.nn.Module，用 PyTorch 中已有的 operator 来组装成自己的模块。这种方式实现简单，但是，计算效率却未必最佳，另外，如果我们想实现的功能过于复杂，可能 PyTorch 中那些已有的函数也没法满足我们的要求。这时，用 C、C++、CUDA 来扩展 PyTorch 的模块就是最佳的选择了。 由于目前市面上大部分深度学习系统（TensorFlow、PyTorch 等）都是基于 C、C++ 构建的后端，因此这些系统基本都存在 C、C++ 的扩展接口。PyTorch 是基于 Torch 构建的，而 Torch 底层采用的是 C 语言，因此 PyTorch 天生就和 C 兼容，因此用 C 来扩展 PyTorch 并非难事。而随着 PyTorch1.0 的发布，官方已经开始考虑将 PyTorch 的底层代码用 caffe2 替换，因此他们也在逐步重构 ATen，后者是目前 PyTorch 使用的 C++ 扩展库。总的来说，C++ 是未来的趋势。至于 CUDA，这是几乎所有深度学习系统在构建之初就采用的工具，因此 CUDA 的扩展接口是标配。 本文用一个简单的例子，梳理一下进行 C++ 扩展的步骤，至于一些具体的实现，不做深入探讨。 PyTorch的C、C++、CUDA扩展 关于 PyTorch 的 C 扩展，可以参考官方教程或者这篇博文，其操作并不难，无非是借助原先 Torch 提供的 &lt;TH/TH.h&gt; 和 &lt;THC/THC.h&gt; 等接口，再利用 PyTorch 中提供的 torch.util.ffi 模块进行扩展。需要注意的是，随着 PyTorch 版本升级，这种做法在新版本的 PyTorch 中可能会失效。 本文主要介绍 C++（未来可能加上 CUDA）的扩展方法。 C++扩展 首先，介绍一下基本流程。在 PyTorch 中扩展 C++/CUDA 主要分为几步： 安装好 pybind11 模块（通过 pip 或者 conda 等安装），这个模块会负责 python 和 C++ 之间的绑定； 用 C++ 写好自定义层的功能，包括前向传播 forward 和反向传播 backward； 写好 setup.py，并用 python 提供的 setuptools 来编译并加载 C++ 代码。 编译安装，在 python 中调用 C++ 扩展接口。 接下来，我们就用一个简单的例子（z=2x+y）来演示这几个步骤。 第一步 安装 pybind11 比较简单，直接略过。我们先写好 C++ 相关的文件： 头文件 test.h 12345678#include &lt;torch/extension.h&gt;#include &lt;vector&gt;// 前向传播torch::Tensor Test_forward_cpu(const torch::Tensor&amp; inputA, const torch::Tensor&amp; inputB);// 反向传播std::vector&lt;torch::Tensor&gt; Test_backward_cpu(const torch::Tensor&amp; gradOutput); 注意，这里引用的 &lt;torch/extension.h&gt; 头文件至关重要，它主要包括三个重要模块： pybind11，用于 C++ 和 python 交互； ATen，包含 Tensor 等重要的函数和类； 一些辅助的头文件，用于实现 ATen 和 pybind11 之间的交互。 源文件 test.cpp 如下： 12345678910111213141516171819202122232425#include \"test.h\"// 前向传播，两个 Tensor 相加。这里只关注 C++ 扩展的流程，具体实现不深入探讨。torch::Tensor Test_forward_cpu(const torch::Tensor&amp; x, const torch::Tensor&amp; y) &#123; AT_ASSERTM(x.sizes() == y.sizes(), \"x must be the same size as y\"); torch::Tensor z = torch::zeros(x.sizes()); z = 2 * x + y; return z;&#125;// 反向传播// 在这个例子中，z对x的导数是2，z对y的导数是1。// 至于这个backward函数的接口（参数，返回值）为何要这样设计，后面会讲。std::vector&lt;torch::Tensor&gt; Test_backward_cpu(const torch::Tensor&amp; gradOutput) &#123; torch::Tensor gradOutputX = 2 * gradOutput * torch::ones(gradOutput.sizes()); torch::Tensor gradOutputY = gradOutput * torch::ones(gradOutput.sizes()); return &#123;gradOutputX, gradOutputY&#125;;&#125;// pybind11 绑定PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) &#123; m.def(\"forward\", &amp;Test_forward_cpu, \"TEST forward\"); m.def(\"backward\", &amp;Test_backward_cpu, \"TEST backward\");&#125; 第二步 新建一个编译安装的配置文件 setup.py，文件目录安排如下： 12345└── csrc ├── cpu │ ├── test.cpp │ └── test.h └── setup.py 以下是 setup.py 中的内容： 1234567891011121314151617181920from setuptools import setupimport osimport globfrom torch.utils.cpp_extension import BuildExtension, CppExtension# 头文件目录include_dirs = os.path.dirname(os.path.abspath(__file__))# 源代码目录source_cpu = glob.glob(os.path.join(include_dirs, 'cpu', '*.cpp'))setup( name='test_cpp', # 模块名称，需要在python中调用 version=\"0.1\", ext_modules=[ CppExtension('test_cpp', sources=source_cpu, include_dirs=[include_dirs]), ], cmdclass=&#123; 'build_ext': BuildExtension &#125;) 注意，这个 C++ 扩展被命名为 test_cpp，意思是说，在 python 中可以通过 test_cpp 模块来调用 C++ 函数。 第三步 在 cpu 这个目录下，执行下面的命令编译安装 C++ 代码： 1python setup.py install 之后，可以看到一堆输出，该 C++ 模块会被安装在 python 的 site-packages 中。 完成上面几步后，就可以在 python 中调用 C++ 代码了。在 PyTorch 中，按照惯例需要先把 C++ 中的前向传播和反向传播封装成一个函数 op（以下代码放在 test.py 文件中）： 1234567891011121314from torch.autograd import Functionimport test_cppclass TestFunction(Function): @staticmethod def forward(ctx, x, y): return test_cpp.forward(x, y) @staticmethod def backward(ctx, gradOutput): gradX, gradY = test_cpp.backward(gradOutput) return gradX, gradY 这样一来，我们相当于把 C++ 扩展的函数嵌入到 PyTorch 自己的框架内。 我查看了这个 Function 类的代码，发现是个挺有意思的东西： 1234567891011121314151617181920212223242526272829303132333435363738class Function(with_metaclass(FunctionMeta, _C._FunctionBase, _ContextMethodMixin, _HookMixin)): ... @staticmethod def forward(ctx, *args, **kwargs): r\"\"\"Performs the operation. This function is to be overridden by all subclasses. It must accept a context ctx as the first argument, followed by any number of arguments (tensors or other types). The context can be used to store tensors that can be then retrieved during the backward pass. \"\"\" raise NotImplementedError @staticmethod def backward(ctx, *grad_outputs): r\"\"\"Defines a formula for differentiating the operation. This function is to be overridden by all subclasses. It must accept a context :attr:`ctx` as the first argument, followed by as many outputs did :func:`forward` return, and it should return as many tensors, as there were inputs to :func:`forward`. Each argument is the gradient w.r.t the given output, and each returned value should be the gradient w.r.t. the corresponding input. The context can be used to retrieve tensors saved during the forward pass. It also has an attribute :attr:`ctx.needs_input_grad` as a tuple of booleans representing whether each input needs gradient. E.g., :func:`backward` will have ``ctx.needs_input_grad[0] = True`` if the first input to :func:`forward` needs gradient computated w.r.t. the output. \"\"\" raise NotImplementedError 这里需要注意一下 backward 的实现规则。该接口包含两个参数：ctx 是一个辅助的环境变量，grad_outputs 则是来自前一层网络的梯度列表，而且这个梯度列表的数量与 forward 函数返回的参数数量相同，这也符合链式法则的原理，因为链式法则就需要把前一层中所有相关的梯度与当前层进行相乘或相加。同时，backward 需要返回 forward 中每个输入参数的梯度，如果 forward 中包括 n 个参数，就需要一一返回 n 个梯度。所以，在上面这个例子中，我们的 backward 函数接收一个参数作为输入（forward 只输出一个变量），并返回两个梯度（forward 接收上一层两个输入变量）。 定义完 Function 后，就可以在 Module 中使用这个自定义 op 了： 123456789import torchclass Test(torch.nn.Module): def __init__(self): super(Test, self).__init__() def forward(self, inputA, inputB): return TestFunction.apply(inputA, inputB) 现在，我们的文件目录变成： 123456├── csrc│ ├── cpu│ │ ├── test.cpp│ │ └── test.h│ └── setup.py└── test.py 之后，我们就可以将 test.py 当作一般的 PyTorch 模块进行调用了。 测试 下面，我们测试一下前向传播和反向传播： 123456789101112131415import torchfrom torch.autograd import Variablefrom test import Testx = Variable(torch.Tensor([1,2,3]), requires_grad=True)y = Variable(torch.Tensor([4,5,6]), requires_grad=True)test = Test()z = test(x, y)z.sum().backward()print('x: ', x)print('y: ', y)print('z: ', z)print('x.grad: ', x.grad)print('y.grad: ', y.grad) 输出如下： 12345x: tensor([1., 2., 3.], requires_grad=True)y: tensor([4., 5., 6.], requires_grad=True)z: tensor([ 6., 9., 12.], grad_fn=&lt;TestFunctionBackward&gt;)x.grad: tensor([2., 2., 2.])y.grad: tensor([1., 1., 1.]) 可以看出，前向传播满足 z=2x+y，而反向传播的结果也在意料之中。 CUDA扩展 虽然 C++ 写的代码可以直接跑在 GPU 上，但它的性能还是比不上直接用 CUDA 编写的代码，毕竟 ATen 没法并不知道如何去优化算法的性能。不过，由于我对 CUDA 仍一窍不通，因此这一步只能暂时略过，留待之后补充～囧～。 参考 CUSTOM C EXTENSIONS FOR PYTORCH CUSTOM C++ AND CUDA EXTENSIONS Pytorch拓展进阶(一)：Pytorch结合C以及Cuda语言 Pytorch拓展进阶(二)：Pytorch结合C++以及Cuda拓展","raw":null,"content":null,"categories":[{"name":"PyTorch","slug":"PyTorch","permalink":"https://jermmy.github.io/categories/PyTorch/"}],"tags":[{"name":"深度学习","slug":"深度学习","permalink":"https://jermmy.github.io/tags/深度学习/"},{"name":"PyTorch","slug":"PyTorch","permalink":"https://jermmy.github.io/tags/PyTorch/"}]},{"title":"论文笔记：Mask R-CNN","slug":"2019-4-12-paper-notes-mask-rcnn","date":"2019-04-12T15:41:11.000Z","updated":"2019-05-15T15:02:03.000Z","comments":true,"path":"2019/04/12/2019-4-12-paper-notes-mask-rcnn/","link":"","permalink":"https://jermmy.github.io/2019/04/12/2019-4-12-paper-notes-mask-rcnn/","excerpt":"之前在一次组会上，师弟诉苦说他用 UNet 处理一个病灶分割的任务，但效果极差，我看了他的数据后发现，那些病灶区域比起整张图而言非常的小，而 UNet 采用的损失函数通常是逐像素的分类损失，如此一来，网络只要能够分割出大部分背景，那么 loss 的值就可以下降很多，自然无法精细地分割出那些细小的病灶。反过来想，这其实类似于正负样本极不均衡的情况，网络拟合了大部分负样本后，即使正样本拟合得较差，整体的 loss 也已经很低了。\n发现这个问题后，我就在想可不可以先用 Faster RCNN 之类的先检测出这些病灶区域的候选框，再在框内做分割，甚至，能不能直接把 Faster RCNN 和分割部分做成一个统一的模型来处理。后来发现，这不就是 Mask RCNN 做的事情咩～囧～\n这篇文章，我们就从无到有来看看 Mask RCNN 是怎么设计出来的。\n\n\n","text":"之前在一次组会上，师弟诉苦说他用 UNet 处理一个病灶分割的任务，但效果极差，我看了他的数据后发现，那些病灶区域比起整张图而言非常的小，而 UNet 采用的损失函数通常是逐像素的分类损失，如此一来，网络只要能够分割出大部分背景，那么 loss 的值就可以下降很多，自然无法精细地分割出那些细小的病灶。反过来想，这其实类似于正负样本极不均衡的情况，网络拟合了大部分负样本后，即使正样本拟合得较差，整体的 loss 也已经很低了。 发现这个问题后，我就在想可不可以先用 Faster RCNN 之类的先检测出这些病灶区域的候选框，再在框内做分割，甚至，能不能直接把 Faster RCNN 和分割部分做成一个统一的模型来处理。后来发现，这不就是 Mask RCNN 做的事情咩～囧～ 这篇文章，我们就从无到有来看看 Mask RCNN 是怎么设计出来的。 回顾 Faster RCNN 首先，我们简单回忆一下 Faster RCNN 的结构，看看如何针对它进行拓展。上面这个框架图中，虚线框内就是 Faster RCNN 的大致结构了。算法过程可以粗略分为以下几步： 将图片输入 CNN 中，得到 feature map； 用一个 RPN 网络在 feature map 提取出候选框（region proposals）。这一步对应 RPN 网络分支； 用另一个 CNN 进一步对该 feature map 进行特征提取，结合候选框得到很多 RoI，然后在每个 RoI 内用 RoI Pooling 提取特征，之后接上 FC 层分别预测框内物体的类别以及做 bounding box 微调。这一步对应 Fast RCNN 网络分支。 整个流程其实非常简洁。既然我们是想在候选框内进一步做分割，那么很自然的想法，就是在原 Faster RCNN 选出来的 RoI 中，根据 classification score 和 bounding box regression 选出得分最高的 RoI，并对这些 RoI 的框进行微调。这样，这些 RoI 就是最可能包含物体，同时定位也更为准确的 RoI 了，之后继续在这些 RoI 内做分割即可。另外，这个想法还能沿用之前得到的 feature map，避免卷积重复计算，可以说是一举两得。 事实上，Mask RCNN 采用的也是这一套思路。这种先检测物体再做分割的两步走策略，江湖人称 two stage，而 UNet 这种一步到位的分割方法，则被称为 one stage。显然，一步到位的 UNet 实现起来简单，大部分情况下效果也还行，但论精度，还是 Mask RCNN 更胜一筹，毕竟有候选框作为先验支撑。 这里要注意另一个问题，虽然 UNet 和 Mask RCNN 都是处理分割，但前者又称为 semantic segmentation，后者称为 instance segmentation。两者的区别可以用下面这张图体现，semantic segmentation 在分割的时候对同一类物体一视同仁，而 instance segmentation 则需要把每个个体都单独分割出来。因此，把 UNet 和 Mask RCNN 进行对比其实不太公平。instance segmentation 由于需要单独分割每个个体，因此基本上所有针对 instance segmentation 的方法都需要先用一个候选框把物体找出来，之后再分割。 下面，我们就来看看 Mask RCNN 是怎么在 Faster RCNN 的基础上实现分割的。 Mask RCNN Mask RCNN 的整体流程图可以参考文章开头那个框架图。它在 Faster RCNN 的基础上，延伸出了一个 Mask 分支。根据 Faster RCNN 计算出来的每个候选框的分数，筛选出一大堆更加准确的 RoI（对应图中 selected RoI），然后用一个 RoI Align 层提取这些 RoI 的特征，计算出一个 mask，根据 RoI 和原图的比例，将这个 mask 扩大回原图，就可以得到一个分割的 mask 了。 RoI Align 之后打算新开一篇文章针对代码细讲，本文只稍微提及 RoI Align 背后的机理，暂且就将它当作一个黑盒。 现在，我们从零出发，假设让我们来设计 Mask RCNN，这中间每一步要如何操作，以及会面临哪些问题。 RoI 如何到 Mask 首先第一个问题是如何处理这些 RoI 的特征，输出分割的 mask。在 UNet 和 FCN 中，是先将图片通过卷积操作得到 feature map，再通过 deconvolution（或者 upsample + convolution）的方法将 feature map 的尺寸还原成原图大小。因此，我们也可以借用同样的思路，先根据 RoI 的尺寸换算回原图，看看这个 RoI 对应到原图上的尺寸有多大，再将 RoI 内的 feature map 上采样成对应尺寸的 mask，然后接一个 FCN 网络将 mask 的通道数处理成 \\(K\\) 即可（假设总共有 \\(K\\) 种类别）。由于上采样是可以求导的，因此反向传播依然有效。 写到这里，我突然觉得，Fast RCNN 中提出的 RoI Pooling 是不是也可以换成这种上/下采样 + 卷积的方式来得到一个固定大小的 feature map，这样，之后的 FC 层的维度也是可以匹配上的。而且 RoI Pooling 本质上也类似一种下采样的操作，只不过采样的方式是取邻域中最大的数值。但转念一想，这其实就是在问 Pooling 操作能不能用下采样来代替，从大佬们设计的网络结构中普遍采用 Pooling 而不是下采样来看，应该是 Pooling 的效果会更好。这个问题就此打住，暂且认为 Pooling 的效果好于下采样。 可以看出，我们设计的这个 mask 分支跟 UNet 或 FCN 的思路其实一样。不过，直接下采样可能会导致 feature map 中一些重要的信息丢失，因此，我们可以沿用 RoI Pooling 的思路，将 RoI 内的那块 feature map 处理成指定大小的 feature map，然后采用 conv/deconv 的方法进一步转换成 \\(K \\times H \\times W\\) 的 mask，其中 \\(K\\) 表示物体的类别。最后把这个 mask 按照 RoI 对应的 bounding box 换算回原图中即可。这一步在训练上也可以类比 FCN，先根据 bounding box 找到 ground truth 中对应的那块 mask，再按比例缩成跟网络输出的 mask 一样大小，然后根据分类损失或者 MSE 构造 Loss 函数即可。 大部分人都能走到这一步，但也就仅仅走到这一步而已。这个流程简单直接，而且也能 work，但实操后会发现效果不佳。这里就涉及到论文中提及的 misalignment 问题。 事实上，分割对模型精度的要求比分类以及检测要高得多，因为前者需要逐像素的标注类别信息。这意味着，如果 RoI 中的 feature map 跟原图中对应的区域存在偏差，就可能导致计算出来的 mask 跟 ground truth 是对不齐的。我们用一张图来说明： 原图中的小男孩有两个 bounding box 框住他，我们用卷积操作对图像进行 downsample，然后根据 feature map 和原图的比例，推算出这两个 bounding box 对应在 feature map 上的位置和大小。结果，很不幸这两个框的位置四舍五入后刚好对应同一块 feature map。接着，RoI Pooling 和 FCN 会对这块 feature map 进行处理得到 mask，再根据 ground truth 计算 Loss。两个框对应的 ground truth 当然是不一样的。这个时候，网络就左右为难，同样一个 feature map，居然要拟合两个不同的结果，它左右为难一脸懵逼，直接导致模型无法收敛。 这就是所谓的 misalignment。问题的根源在于 bounding box 缩放时的取整。当然，RoI Pooling 本身在 pooling 的时候也是存在取整误差的。 既然问题出在取整上，那么，很自然想到的解决思路就是放弃取整，直接根据推算得到的浮点数来处理 bounding box。如此一来，bounding box 对应到的 feature map 上就会有一些点的坐标不是整数，于是，我们需要重新确定这些点的特征值。而这一步也是 RoI Align 的主要工作。其具体的做法是采用双线性插值，根据相邻 feature map 上的点来插值一个新的特征向量。如下图所示： 图中，我们先在 bounding box 中采样出几个点，然后用双线性插值计算出这几个点的向量，之后，再按照一般的 Pooling 操作得到一个固定大小的 feature map。具体的细节，之后开一篇新的文章介绍。 论文中通过 RoI Align 和 FCN 将 RoI 内对应的 feature map 处理成固定大小的 mask（\\(K \\times m \\times m\\)，\\(K\\) 表示分割的类别数目），然后将该 mask 还原回原图后，就可以得到对应的分割掩码了。 Loss 的设计 在损失函数的设计方面，除了原本 Faster RCNN 中的分类损失和 bounding box 回归损失外，我们还需要针对 mask 分支设计一个分割任务的损失函数。最容易想到的函数自然是 FCN 和 UNet 中用到的 Softmax + Log 的多分类损失。如下图所示： Mask 中的每个点都是一个 \\(K\\) 维的向量，我们把 ground truth 中对应的那个 mask 也缩放到 \\(m \\times m\\) 大小，然后就可以针对每个点的向量做多分类损失。 不过，作者在做实验的时候估计是发现这种方式训练的网络收敛不好，进而发现这个损失函数会出现所谓的 class competition 的问题。 class competition，顾名思义，就是不同类别之间存在竞争关系，这种竞争关系直接导致的结果就是网络在训练过程中，回传的梯度存在前后不一致的地方。 打个比方，在 Faster RCNN 做 object detection 的时候，已经把某一块 RoI 识别为汽车，但这个 RoI 内可能存在其他物体的一部分，因此分割的 mask 中，除了要将汽车分割出来外，还要把另外那个物体也分割出来。这就导致这样的情况，在 object detection 的分支中，这块 RoI 整体被识别为汽车，但在 segmentation 的时候，这块 RoI 一部分被识别为汽车，一部分又要当作其他物体，如此一来，这两个分支回传到前面的梯度多少存在冲突，而前面的特征提取网络可是共享的，结果网络在学习的时候就可能出现左右为难的情况。不然，单纯从 Mask 分支来看，feature map 上每个点（包括 RoI Align 插值的点）本来就和 ground truth 是一一对应的，彼此之间又哪有 competition 之说呢？当然，以上只是我看论文时的想法，并没有做具体的实验，所以也不一定正确。 之后我又想，在 object detection 的时候，那些 bounding box 本身就是有重叠的，换句话说，RoI 之间也是有重叠的，如果两个 RoI 被识别为不同的物体，那么重叠那部分不也是冲突的吗？这个时候应该找个例子看看重叠那部分的特征图是什么样子的。不过，我个人的想法是，网络对这些重叠的区域可能会起到抑制作用。比如说，一辆汽车前面被一辆自行车挡着，那么汽车的 bounding box 多少会覆盖到自行车，而自行车的 bounding box 也多少包含了汽车的一部分，但这个交集相比各自的 bounding box 而言，可能不是主体作用，网络在对汽车的 RoI 做识别的时候，更多的会把注意力放在非重叠的那部分汽车上，而重叠那部分，虽然有一点点汽车的东西，但由于有自行车的遮挡，起到的作用不会太大。 最后需要声明一下，这个想法完全是我个人瞎猜的，并没有做实验证明. 既然多分类效果不好，那我们就尝试二分类。如下图所示： 二分类的话，我们只考虑一种类别，比如，如果 ground truth 中标记了这个 bounding box 中是个人的话，那我们就只针对人的 mask 进行分割，而对这个 bounding box 中其他可能存在的物体一律忽视。上图中，假设人的类别是第 \\(K\\) 类，那么，我们就只在第 \\(K\\) 个 mask 上和 ground truth 中人的 mask 做 sigmoid 的二分类损失。如果某个点被标注是人的一部分，就识别为 1，否则全部识别为 0。 有同学可能会有疑惑，二分类只考虑一种物体，而把其他物体的部分直接忽略掉，如果出现一种极端情况，比如有一个人的一只手出现在一个汽车的 bounding box 中，然后网络计算这个人的 RoI 的时候，bounding box 刚好没有把这只手包含进来，而在汽车那个 RoI 里面又不会对这只手做分割，这样的话，这只手不就直接漏掉了吗？确实，这种情况下，这只手会被直接忽略。不过，这种情况属于 bounding box regression 没有做好，因此不在本文讨论范围内。 总结一下，损失函数可以表示为： \\[ L=L_{cls}+L_{box}+L_{mask} \\] \\(L_{cls}\\) 和 \\(L_{box}\\) 是 Faster RCNN 中的损失函数，而 \\(L_{mask}\\) 则是 mask 分支中的 sigmoid 二分类损失。 特征提取 特征提取部分其实可以有多种选择，具体哪种选择好，可能要依据具体的任务来确定。论文尝试了 ResNet、FRN、ResNeXt 等网络。这一部分我没有去细究，因为这里变数比较大，针对不同的场景可以适当调整，因此这一块就不细谈了。 训练和预测 训练阶段依赖 Faster RCNN 的输出结果。首先根据 Faster RCNN 找出一大堆 RoI，再根据 classification score 对这些 RoI 进行排序，选出分类分数最高的前 \\(N\\) 个 RoI，然后根据 ground truth 中的 mask 和这个 RoI 取个交集，这个交集作为 \\(L_{mask}\\) 的 target。实际预测的时候，同样先根据 Faster RCNN 选出前 100 个分数最高的 RoI，然后计算每个 RoI 的 mask。不过，由于这些 mask 是根据二分类损失训练出来的，因此，我们要根据 Faster RCNN 提供的每个 RoI 的类别，在 mask 中找出对应的那一层作为最终分割的结果。 从这个训练过程也可以找出一些不足的地方。比如，挑选 RoI 送入 mask 分支那一步，这个挑选的结果完全依赖于 Faster RCNN 计算的分数，一旦 Faster RCNN 出了差错，给一些很重要的 RoI 打了很低的分，那么这些 RoI 就可能被忽略掉，之后分割就没它们什么事了。因此，有人提出了一些改进，认为应该对这个筛选的打分机制进行修改，不应该完全依赖于 Faster RCNN 的结果，比如，Mask Scoring RCNN 就在打分中加入了 ground truth 的 mask 的 IoU 分数，从而把那些容易被忽略的 RoI 找出来。这有点像是难样本挖掘了。 实验 何凯明在论文中一直强调 Mask RCNN 是 without bells and whistles，意思就是 Mask RCNN 的算法中没有什么花里胡哨的东西，都是实打实的干货，无需特殊的调参技巧，经得起时间的考验。为此论文中还提供了很多对比实验来一一验证每个模块的作用。 首先是 RoIAlign 和 RoIPooling 的对比： 在 instance segmentation 和 object detection 上都有不小的提升。如此看来，RoIAlign 其实就是一个更加精准的 RoIPooling，把前者放到 Faster RCNN 中，对结果的提升应该也会有帮助。 sigmoid 和 softmax 的对比 这里同样可以取得不小的提升。 特征网络的选择 总体来说，加上 FPN 网络的效果会更好，可能因为 FPN 综合考虑了不同尺寸的 feature map 的信息，因此能够把握一些更精细的细节。 另外，论文还针对人体关键点检测做了一个实验，来体现 Mask RCNN 框架的通用性，这部分内容我还不太熟悉，就先略过了。 总结 总的来说，Mask RCNN 这种先检测物体，再分割的思路，简单直接，在建模上也更有利于网络的学习。而其中，我认为两个最重要的改进点，分别是 RoIAlign 和采用 sigmoid 二分类损失。这两个改进的目标都是让网络在学习的时候能保持一致性，使得输入到输出之间的映射关系更加简单直接。 参考 令人拍案称奇的Mask RCNN A Brief History of CNNs in Image Segmentation: From R-CNN to Mask R-CNN Mask RCNN tutorial Mask Scoring RCNN Mask RCNN","raw":null,"content":null,"categories":[{"name":"计算机视觉","slug":"计算机视觉","permalink":"https://jermmy.github.io/categories/计算机视觉/"}],"tags":[{"name":"计算机视觉","slug":"计算机视觉","permalink":"https://jermmy.github.io/tags/计算机视觉/"},{"name":"深度学习","slug":"深度学习","permalink":"https://jermmy.github.io/tags/深度学习/"},{"name":"论文","slug":"论文","permalink":"https://jermmy.github.io/tags/论文/"}]},{"title":"从2018走来","slug":"2019-1-1-from-2018-to-2019","date":"2019-01-03T07:28:55.000Z","updated":"2019-01-06T13:20:59.000Z","comments":true,"path":"2019/01/03/2019-1-1-from-2018-to-2019/","link":"","permalink":"https://jermmy.github.io/2019/01/03/2019-1-1-from-2018-to-2019/","excerpt":"过去一直没有做年终总结的习惯，因为我之前的习惯是脚踏实地的做完眼前的小事情，也从来没给自己确立过什么目标，因此年终的时候也就平平无奇，既没有那种水到渠成的坦然，也没有那种吹尽狂沙始到金的欣慰。但 2018 确实给我带来了不少成长，也是我这些年来过得最不淡定的一年，有些不淡定可能会一直延续到 2019。在我真正能做到拨云见日之前，希望这篇文章能为今后的决定提供一些勇气和思路。\n\n\n","text":"过去一直没有做年终总结的习惯，因为我之前的习惯是脚踏实地的做完眼前的小事情，也从来没给自己确立过什么目标，因此年终的时候也就平平无奇，既没有那种水到渠成的坦然，也没有那种吹尽狂沙始到金的欣慰。但 2018 确实给我带来了不少成长，也是我这些年来过得最不淡定的一年，有些不淡定可能会一直延续到 2019。在我真正能做到拨云见日之前，希望这篇文章能为今后的决定提供一些勇气和思路。 说要总结，其实也就两件事情，其一是关于实习找工作，其二关于找女朋友。 关于实习 今年应该是我真正意义上的第一次实习，之前虽然也在一些创业团队里面混过，但都是小打小闹，上不了台面。实习的公司是一家独角兽公司，说来还得感谢导师找的内推，不然以我的资历大概又是一面游的节奏。实习那四个月的时间做过两个项目，其中一个做的不是很好，导致后来的转正也一波三折，不过好在结局还算美好，希望来年不要再出什么幺蛾子了。 实习的过程让我发现自己还算是动手和搜索能力比较强的人。实习前我一直是在实验室忙自己的毕设，由于是自己选的课题，加上师兄以及其他同学都在导师的公司做量化投资的东西，因此有那么大半年时间，实验室就我一个人。当时我们实验室还有其他两个老师的学生，看着他们每天都其乐融融地看论文做实验讨论 idea，我总是有一种被孤立的自卑感。当时我自己做的课题是关于三维模型的检索问题，用到了深度学习的东西，那个时候我也是刚入门深度学习，学了一丢丢 tensorflow，然后就一个人哼哧哼哧的搞了起来。这个一个人「瞎搞」的过程其实是非常痛苦的。首先，你没有来自外部的助力，当时我也是刚了解一点深度学习，遇到了网络不收敛的问题，甚至不知道有 BN 这种东西存在；其次，面对各种大的小的困难，再对比其他同学的成就，你可能会陷入一种自我怀疑的境况。当那些本来跟你一条起跑线的人都陆续发了顶会，而你还在那里焦头烂额的时候，你不可能淡定得下来。在忙活了大半年后，导师估计也看不下去了，就给我丢了几篇论文，叫我吸取点东西，还找了学校一个做过类似项目的博士师兄和我聊了一下 (因为他自己也不知道我的课题要从哪方面找突破点)。聊完之后我有了一种醍醐灌顶的感觉，因为我之前其实有了几个 idea，但发现效果并不怎么好，经过师兄点拨后，我发现我的想法是对的，只是数据处理上需要点 trick。 上面这段惨痛的经历其实是想说明一件事情，科研这种东西一定要经常性的讨论，一来是不同课题的 idea 可能是可以互相借鉴的，这样可以加快你的产出，二来是可以让你自己坚定自己的想法，或者及时修正方向。不排除有些人能力强到可以一个人 solo，就像我的研究课题，我相信即使没有人指点，再给我一段时间可能也是做得出来的，但那样你就慢了一大截，这是对能力的浪费。 当然，最痛苦的还是找实习的过程，像我这种没论文没比赛的同学，找人内推大厂连简历都过不了。T 厂后来正式春招的时候还把我的简历调剂到客户端开发岗，我一开始有点不情愿，但想想有总比没有好，于是就去尝试了一下。一面二面的面试官都非常 nice，而且面试过程也非常轻松，并没有涉及很难的技术问题，二面的面试官还问我什么时候能入职。然而幸福来的太突然，二面完还没回到学校就被拒了。后面 A 厂的面试被调到一个数据挖掘的部门，一面跪，头条的笔试直接跪等等，心态从一开始的崩溃到麻木，欲哭无泪。后来导师说那你安心做论文，我找人帮你内推到另一家公司。这是一家不太容易进的公司，但由于导师的 buffer 加持，我面了一面就进去了，现在想想，虽然前面科研的过程非常辛苦，但我还是非常感谢我的导师。 实习的时候，我终于有不是一个人在战斗的感觉。而之前那段「瞎搞」的经历让我对信息的搜索能力得到了锻炼，因此很多东西实现起来也比较快 (这应该是我这种弱鸡最后能侥幸转正的原因了)。实习的时候基本每周都有 seminar，在读了那么多论文，也听别人讲了那么多论文后，我才找到了一种正确看论文的感觉。这里还得感谢其他实习生对我的帮助与指点，在我看论文感到困惑时，他们都非常热心地帮我指出了问题的本质，让我一次次地顿悟。这种顿悟的感觉对一个人的成长帮助太大了。这里我又要吐槽一下现在各种深度学习的论文，有些看起来封装得无与伦比的文章，本质上干得都是数据拟合的工作，各种网络节点的连线搭桥，真正起效果的反而可能是数据处理上的 trick。另外，我发现有些暂时看不懂的论文，实在没必要太钻牛角尖了，直接用起来就完了。比如我之前一直对变分编码器 (VAE) 的原理感到很困惑，曾经花了一个月的时间钻研各类博客和论文，但都没有什么更深的理解，后来实习的时候跟别人讨论了一下，发现其实大家对这个东西的本质也不是特别清楚，这反倒让我觉得有点欣慰，不再纠结自己理解能力的问题。再比如，我之前花了很大的功夫研究 BN(Batch Normalization) 的原理，但一位清华的同事跟我说，BN 的前提假设是数据服从高斯分布，但这个假设本身他就不认可，因此他对论文的原理是质疑的态度，他的话让我开始去反思一些论文的理论本身，有时候你对着一篇论文纠结了好久，抓耳挠腮就是不明所以，但事实上你的不明所以并不是你笨，而是论文本身就写得比较绕，或者说理论本身就是有问题的。当然，这里我没资格评价 BN 的正确与否，只是想说，看论文的时候要学会抓住主要的脉络，同时秉承一颗质疑的心，而不是一味地去跪舔哪怕是一篇顶会文章。 实习的过程，对比之前实验室的经历，让我深刻体会到一个道理：不要太执着于自己的能力，多跟别人 argue，不然你可能就是永远被埋没的金子了。 关于女朋友 关于找女朋友，我这里也没法给需要的人士带去各种安慰或者建议，毕竟我是属于运气比较好，但目前依然摸着石头过河的青铜玩家。 说是女朋友，其实还差一截，因为我也是几个星期前才表示我想追她，所以我们还是暧昧中的男女同学而已。五年前刚认识她的时候，我们都还刚上大一，但那个时候我已经在心里给她留了位置。本科期间偶尔碰过几次面，但我都秉持着淡如水的交往。那几年我感觉自己状态一直很差，在学业、社交、家庭方面都遇到不少挫折，对自己的未来很迷，感觉一切都是那么不确定，我没有信心认为自己是个值得被爱的人。当时连她的微信都没想要，倒不是不敢，只是我秉承的理念是「不要在错误的时间遇到对的人」。我想，我应该先努力让自己变得优秀起来。当然这段时间也会很担心其他男生把她拱走，但我想，如果我们真有缘的话，最后还是有机会相遇的。现在想想，当时就是内心戏多又太怂😷。本科毕业的时候，我曾经以为再也没机会见到她了，心想这可能会成为这辈子的遗憾之一吧。后来实习的时候，我终于在她生日那天发了张贺卡，幸运的是她也回了我，然后才加了她微信。之后约她吃了顿饭，才了解到她有过一个前男友，不过已经分手了，后来也有其他男生追求她，但都无疾而终。在某次游玩的时候，我跟她表达了想追她的愿望，她先是很惊愕，吐槽我这个直男可能不适合她，但最终还是答应了。这段故事其实是想说明，喜欢一个人的时候就该抓住机会上去，像我这种一直等待的，那是踩了狗屎运才捡到宝了。 不过，作为一个恋爱经验为零的直男，面对一个王者级的女生，只会是经常性地被吐槽。比如说，每天睡前一定要找她聊天，然后互相说晚安睡觉，一开始我维持着单身狗的惯性，一直是她主动找我，被吐槽了好几次。又比如说，有一次她来学校找我玩，晚上回去的时候，想送她到地铁站，她说不用了自己可以，然后我果真就自己先回宿舍了，那天晚上被吐槽说不懂女生心理，难道嘴上说不要就真的不要？再比如说，挑礼物不合适被吐槽，聊天没 get 到被吐槽，出去玩的时候不知道吃什么被吐槽。不过，最严重的一次，发生在跨年的时候，当时我刚从死党家里回学校，而她跟几个好闺蜜在一起跨年，我那天有点累，想着她跟她闺蜜在一块应该很开心，就准备第二天再给她发新年祝福，于是就先睡觉了。结果，第二天刚醒就看到信息说我昨晚没给她发信息，不及格了，我还以为她是闹情绪，就发了红包安慰了一下，结果下午的时候突然发现语气有点不对劲，说了一些诸如：对她认知不足啊，没做好准备啊，不想打扰了啊，没什么期待了啊，重点不在新年祝福啊，不勉强啊之类的。我当时被吓得一脸懵逼，感觉自己可能抢救不过来了，心想如果她真的想离开一会，就等她气消了我再重新追吧。然后那天很晚的时候她又说「跟你们程序员谈恋爱实在是麻烦，你太笨了，没想拒绝你」，我一个天旋地转，心想这是哪出跟哪出啊，那天晚上彻夜难眠，肠子都悔青了，大脑里一直回放着林俊杰的「可惜没如果」。 虽然如此，可当你真的喜欢上一个人的时候，再怎么累你都会迎难而上，再怎么委屈你都会觉得是自己的错。每次当我有一堆破事要处理还得安慰她哄她的时候，心里也是很烦躁，可看到她笑后整个心都化了，感觉让她受一点委屈都会于心不忍。 当然，作为一个理性工科男，我也在思考自己的原动力在哪。如果相处时间长了，不再有新鲜感后，该怎么去保鲜？以后工作了，该如何平衡利用自己的时间？以后遇到很大的挫折时该怎么处理？这些目前都没有很好的答案，是 2019 年需要深思的问题。 最后 这篇口水文是写给自己看的。这一路走来，我有一种很明显的先苦后甜的感觉，虽然上半年过得很揪心很迷惘，但好歹我坚持住了，并且这种坚持让我在下半年慢慢走入正轨。秋招的时候依然亚历山大 (顶着实习、论文、找工作三座大山)，不过之前崩溃过一次的心态，让我觉得这个过程也不是那么心累了。 关于女朋友的话题，我只有一句话：遇到喜欢的就赶紧上，否则被猪拱走后悔死你。","raw":null,"content":null,"categories":[{"name":"杂感","slug":"杂感","permalink":"https://jermmy.github.io/categories/杂感/"}],"tags":[{"name":"杂感","slug":"杂感","permalink":"https://jermmy.github.io/tags/杂感/"}]},{"title":"One Day in 桂林","slug":"2018-12-2-one-day-in-guilin","date":"2018-12-02T11:59:09.000Z","updated":"2018-12-02T14:24:33.000Z","comments":true,"path":"2018/12/02/2018-12-2-one-day-in-guilin/","link":"","permalink":"https://jermmy.github.io/2018/12/02/2018-12-2-one-day-in-guilin/","excerpt":"这两天导师让我到桂林参加一个学术会议，不过由于会议太水，加之我出发得太晚，等到桂林的时候，会议差不多结束了。我想我也不能无功而返，就趁着公费出差的机会，在这个号称「山水甲天下」的城市，以一个单身狗该有的心态逛了一圈。\n\n\n\n「语文课本中的象鼻山」\n\n","text":"这两天导师让我到桂林参加一个学术会议，不过由于会议太水，加之我出发得太晚，等到桂林的时候，会议差不多结束了。我想我也不能无功而返，就趁着公费出差的机会，在这个号称「山水甲天下」的城市，以一个单身狗该有的心态逛了一圈。 「语文课本中的象鼻山」 不得不说，比起广深这样的大城市，桂林才算得上是一座真正意义上的「岭南」城市。因为「岭」这种东西，在桂林实在是太常见了。在市中心，在城市外围，几乎随处可见。这对于要到城市边缘才能见到白云山、凤凰山的同学来说，在这里生活时刻都会有度假的感觉。 「依偎在花下」 据官方的旅游秘籍介绍，来桂林不去象鼻山等于白来。我从桂电金鸡岭校区一路徒步过去，一个小时的功夫就奠定了今天微信运动的霸主地位，中间横穿了整个桂林市区，绕过了大半个七星景区，并跨过了一次小东江和漓江。到的时候我就在庆幸，要是我有女朋友的话，她肯定会喊累，这样我们就没法看到桂林的地标了。 「单身的原因，就在脚下」 说起象鼻山，初中某一本语文课本是打过广告的。网上的人都说象鼻山公园的门票性价比太低，50+ 的门票，进去爬个山就没了。我觉得咱们单身狗还是不能因为钱而放弃一些机会，再说大老远走过来，不拍个照来个到此一游，那就白走那一万多步了。进到象鼻山后，要说它留给我最大的印象，肯定是那股酸酸怪怪的味道。那是我刚从山上下来的时候，在一个垃圾桶旁我立刻闻到一股酸酸的、略微刺鼻的味道。我想该不会是这个垃圾桶里丢了什么水果腐烂了吧。再走过去一点才发现，居然是一个酒窖。然后就听人说，整座象鼻山其实是空的，里面就是一个温度低、通风好的纯天然酒窖。路过的几个中年大叔大妈都忍不住凑到酒窖门前赞不绝口，侧面衬托出了我对酒文化的无知。 「在腐烂中升华」 从象鼻山过去还有一个叫「爱情岛」的地方，在这个旅游淡季，我在岛上没有感受到很强烈的爱情的酸臭味 (中年大妈大叔居多)，也很少见到其他异性单身狗，确信这次的门票果然打水漂了，于是就悻悻地离开了。 「感觉快淹死了」 象山再往前还有「两江四湖」、「伏波山」、「叠彩山」、「靖江王府」好多景点，每个景点费用差不多，不过我实在是走累了，就到「正阳步行街」喝杯奶茶休息了一下，顺道拿出准备已久的 ipad 看了篇论文，感觉到一阵满足。 晚上在附近吃了一顿号称非常正宗的桂林米粉，不过，正如对酒味的迟钝一样，我还没领悟出这种正宗的精髓所在。吃完饭，就到了买手信的时候了。我进了一家专门卖手信的店，发现桂林的手信都是一些「桂花蜜」、「桂花糕」、「桂花干」、「三花酒」之类，感觉非常对胃口。我一直认为，能吃的手信最实在了。在问了桂花蜜和桂花干的价格后，发现这么一小瓶就要四五十块左右，顿觉有诈，便打了个哈哈离开了。然后我到了旁边一个像超市一样的小商店，发现里面也有同样的东西，结果一瓶才 20 块钱，惊想一墙之隔的两家店，差距为什么这么大。于是就兴奋了连买了四瓶。在回去的公交车上，我又在想，这两家店会不会是一伙的，就抓住顾客贪便宜的心理发财，还是说旁边批发价的超市卖的是假货，不然它们又为什么能挨在一起相安无事…… 「非专业人士和手机拍的漓江，大晚上有跳下去的冲动」 小学生作文到此结束。","raw":null,"content":null,"categories":[{"name":"杂感","slug":"杂感","permalink":"https://jermmy.github.io/categories/杂感/"}],"tags":[{"name":"杂感","slug":"杂感","permalink":"https://jermmy.github.io/tags/杂感/"}]},{"title":"RNN，写起来真的烦","slug":"2018-11-25-how-to-write-rnn-in-pytorch-and-tensorflow","date":"2018-11-25T09:49:42.000Z","updated":"2019-07-01T08:53:33.000Z","comments":true,"path":"2018/11/25/2018-11-25-how-to-write-rnn-in-pytorch-and-tensorflow/","link":"","permalink":"https://jermmy.github.io/2018/11/25/2018-11-25-how-to-write-rnn-in-pytorch-and-tensorflow/","excerpt":"曾经，为了处理一些序列相关的数据，我稍微了解了一点递归网络 (RNN) 的东西。由于当时只会 tensorflow，就从官网上找了一些 tensorflow 相关的 demo，中间陆陆续续折腾了两个多星期，才对 squence to sequence，sequence classification 这些常见的模型和代码有了一些肤浅的认识。虽然只是多了时间这个维度，但 RNN 相关的东西，不仅是模型搭建上，在数据处理方面的繁琐程度也比 CNN 要高一个 level。另外，我也是从那个时候开始对 tensorflow 产生抵触心理，在 tf 中，你知道 RNN 有几种写法吗？你知道 dynamic_rnn 和 static_rnn 有什么区别吗？各种纷繁复杂的概念无疑加大了初学者的门槛。后来我花了一两天的时间转向 pytorch 后，感觉整个世界瞬间清净了 (当然了，学 tf 的好处就是转其他框架的时候非常快，但从其他框架转 tf 却可能生不如死)。pytorch 在模型搭建和数据处理方面都非常好上手，比起 tf 而言，代码写起来更加整洁干净，而且开发人员更容易理解代码的运作流程。不过，在 RNN 这个问题上，新手还是容易犯嘀咕。趁着这一周刚刚摸清了 pytorch 搭建 RNN 的套路，我准备记录一下用 pytorch 搭建 RNN 的基本流程，以及数据处理方面要注意的问题，希望后来的同学们少流点血泪…\n至于 tf 怎么写 RNN，之后有闲再补上 (我现在是真的不想回去碰那颗烫手的山芋😩)\n\n\n","text":"曾经，为了处理一些序列相关的数据，我稍微了解了一点递归网络 (RNN) 的东西。由于当时只会 tensorflow，就从官网上找了一些 tensorflow 相关的 demo，中间陆陆续续折腾了两个多星期，才对 squence to sequence，sequence classification 这些常见的模型和代码有了一些肤浅的认识。虽然只是多了时间这个维度，但 RNN 相关的东西，不仅是模型搭建上，在数据处理方面的繁琐程度也比 CNN 要高一个 level。另外，我也是从那个时候开始对 tensorflow 产生抵触心理，在 tf 中，你知道 RNN 有几种写法吗？你知道 dynamic_rnn 和 static_rnn 有什么区别吗？各种纷繁复杂的概念无疑加大了初学者的门槛。后来我花了一两天的时间转向 pytorch 后，感觉整个世界瞬间清净了 (当然了，学 tf 的好处就是转其他框架的时候非常快，但从其他框架转 tf 却可能生不如死)。pytorch 在模型搭建和数据处理方面都非常好上手，比起 tf 而言，代码写起来更加整洁干净，而且开发人员更容易理解代码的运作流程。不过，在 RNN 这个问题上，新手还是容易犯嘀咕。趁着这一周刚刚摸清了 pytorch 搭建 RNN 的套路，我准备记录一下用 pytorch 搭建 RNN 的基本流程，以及数据处理方面要注意的问题，希望后来的同学们少流点血泪… 至于 tf 怎么写 RNN，之后有闲再补上 (我现在是真的不想回去碰那颗烫手的山芋😩) 什么是 RNN 虽然说我们用的是 API，但对于 RNN 是什么东西还是得了解一下吧。对于从没接触过 RNN 的小白来说，karpathy 这篇家喻户晓的文章是一定要读一下的，如果想更加形象地了解它的工作机制，可以搜一些李宏毅的深度学习教程。 RNN 其实也是一个普通的神经网络，只不过多了一个 hidden state 来保存历史信息。跟一般网络不同的是，RNN 网络的输入数据的维度通常是 \\([batch\\_size \\times seq\\_len \\times input\\_size ]\\)，它多了一个序列长度 \\(seq\\_len\\)。在前向过程中，我们会把样本 \\(t\\) 个时间序列的信息不断输入同一个网络 (见上图)，因为是重复地使用同一个网络，所以称为递归网络。 关于 RNN，你只需要记住一个公式：\\(h_t = \\tanh(w_{ih} x_t + b_{ih} + w_{hh} h_{(t-1)} + b_{hh})\\)。这也是 pytorch 官方文档中给出的最原始的 RNN 公式，其中 \\(w_{*}\\) 表示 weight，\\(b_{*}\\) 表示 bias，\\(x_t\\) 是输入，\\(h_t\\) 是隐藏状态。回忆一下，普通的神经网络只有 \\(w_{ih} x_t + b_{ih}\\) 这一部分，而 RNN 无非就是多加了一个隐藏状态的信息 \\(w_{hh} h_{(t-1)} + b_{hh}\\) 而已。 普通网络都是一次前向传播就得到结果，而 RNN 因为多了 sequence 这个维度，所以需要跑 n 次前向。我们用 numpy 的写法把 RNN 的工作流程总结一下，就得到了如下代码 (部分抄自 karpathy 的文章)： 1234567891011121314151617181920212223# 这里要啰嗦一句，karpathy在RNN的前向中还计算了一个输出向量output vector，# 但根据RNN的原始公式，它的输出只有一个hidden state，至于整个网络最后的output vector，# 是在hidden state之后再接一个全连接层得到的，所以并不属于RNN的内容。# 包括pytorch和tf框架中，RNN的输出也只有hidden state。理解这一点很重要。class RNN: # ... def step(self, x, hidden): # update the hidden state hidden = np.tanh(np.dot(self.W_hh, hidden) + np.dot(self.W_xh, x)) return hiddenrnn = RNN()# x: [batch_size * seq_len * input_size]x = get_data()seq_len = x.shape[1]# 初始化一个hidden state，RNN中的参数没有包括hidden state，# 只包括hidden state对应的权重W和b，# 所以一般我们会手动初始化一个全零的hidden statehidden_state = np.zeros()# 下面这个循环就是RNN的工作流程了，看到没有，每次输入的都是一个时间步长的数据，# 然后同一个hidden_state会在循环中反复输入到网络中。for i in range(seq_len): hidden_state = rnn(x[:, i, :], hidden_state) 过来人血泪教训：一定要看懂上面的代码再往下读呀。 pytorch 中的 RNN 好了，现在可以进入本文正题了。我们分数据处理和模型搭建两部分来介绍。 数据处理 pytorch 的数据读取框架方便易用，比 tf 的 Dataset 更有亲和力。另外，tf 的数据队列底层是用 C++ 的多线程实现的，因此数据读取和预处理都要使用 tf 内部提供的 API，否则就失去多线程的能力，这一点实在是令人脑壳疼。再者，过来人血泪教训，tf 1.4 版本的 Dataset api 有线程死锁的bug，谁用谁知道😈。而 pytorch 基于多进程的数据读取机制，避免 python GIL 的问题，同时代码编写上更加灵活，可以随意使用 opencv、PIL 进行处理，爽到飞起。 pytorch 的数据读取队列主要靠 torch.utils.data.Dataset 和 torch.utils.data.DataLoader 实现，具体用法这里略过，主要讲一下在 RNN 模型中，数据处理有哪些需要注意的地方。 在一般的数据读取任务中，我们只需要在 Dataset 的 __getitem__ 方法中返回一个样本即可，pytorch 会自动帮我们把一个 batch 的样本组装起来，因此，在 RNN 相关的任务中，__getitem__ 通常返回的是一个维度为 \\([seq\\_len \\times input\\_size]\\) 的数据。这时，我们会遇到第一个问题，那就是不同样本的 \\(seq\\_len\\) 是否相同。如果相同的话，那之后就省事太多了，但如果不同，这个地方就会成为初学者第一道坎。因此，下面就针对 \\(seq\\_len\\) 不同的情况介绍一下通用的处理方法。 首先需要明确的是，如果 \\(seq\\_len\\) 不同，那么 pytorch 在组装 batch 的时候会首先报错，因为一个 batch 必须是一个 n-dimensional 的 tensor，\\(seq\\_len\\) 不同的话，证明有一个维度的长度是不固定的，那就没法组装成一个方方正正的 tensor 了。因此，在数据预处理时，需要记录下每个样本的 \\(seq\\_len\\)，然后统计出一个均值或者最大值，之后，每次取数据的时候，都必须把数据的 \\(seq\\_len\\) 填充 (补0) 或者裁剪到这个固定的长度，而且要记得把该样本真实的 \\(seq\\_len\\) 也一起取出来 (后面有大用)。例如下面的代码： 12345678def __getitem__(self, idx): # data: seq_len * input_size data, label, seq_len = self.train_data[idx] # pad_data: max_seq_len * input_size pad_data = np.zeros(shape=(self.max_seq_len, data.shape[1])) pad_sketch[0:data.shape[0]] = data sample = &#123;'data': pad_data, 'label': label, 'seq_len': seq_len&#125; return sample 这样，你从外部拿到的 batch 数据就是一个 \\([batch\\_size \\times max\\_seq\\_len \\times input\\_size]\\) 的 tensor。 模型搭建 RNN 拿到数据后，下面就要正式用 pytorch 的 RNN 了。从我最开始写的那段 RNN 的代码也能看出，RNN 其实就是在一个循环中不断的 forward 而已。但直接循环调用其实是非常低效的，pytoch 内部会用 CUDA 的函数来加速这里的操作，对于直接调 API 的我们来说，只需要知道 RNN 返回给我们的是什么即可。让我们翻开官方文档： class torch.nn.RNN(*args, **kwargs) Parameters: input_size, hidden_size, num_layers, … Inputs: input, h_0 input of shape (seq_len, batch, input_size) h_0 of shape (num_layers * num_directions, batch, hidden_size) Outputs: output, h_n output of shape (seq_len, batch, num_directions * hidden_size) h_n (num_layers * num_directions, batch, hidden_size) 这里我只摘录初始化参数以及输入输出的 shape，记住这些信息就够了，下面会讲具体怎么用。注意，shape 里面有一个 num_directions，这玩意表示这个 RNN 是单向还是双向的，简单起见，我们这里默认都是单向的 (即num_directions=1)。 现在借用这篇文章中的例子做讲解。 首先，我们初始化一个RNN: 123456batch_size = 2max_length = 3hidden_size = 2n_layers = 1# 这个RNN由两个全连接层组成，对应的两个hidden state的维度是2，输入向量维度是1rnn = nn.RNN(1, hidden_size, n_layers, batch_first=True) 然后，假设我们的输入数据是这样子的： 123456789101112x = torch.FloatTensor([[1, 0, 0], [1, 2, 3]]).resize_(2, 3, 1)x = Variable(x) # [batch, seq, feature], [2, 3, 1]seq_lengths = np.array([1, 3]) # list of integers holding information about the batch size at each sequence stepprint(x)&gt;&gt;&gt; tensor([[[ 1.], [ 0.], [ 0.]], [[ 1.], [ 2.], [ 3.]]]) 可以看到输入数据的维度是 \\([2 \\times 3 \\times 1]\\)，也就是 \\(batch\\_size=2\\)，\\(seq\\_len=3\\)，\\(input\\_size=1\\)。但要注意一点，第一个样本的 \\(seq\\_len\\) 的有效长度其实是 1，后面两位都补了 0。那么，在实际计算的时候，第一个样本其实只要跑 1 遍 forward 即可，而第二个样本才需要跑 3 遍 forward。 pack_padded_sequence 那如何让RNN知道不同样本的序列长度不一样呢？幸运的是，pytorch 已经提供了很好的接口来处理这种情况了。如果输入样本的 \\(seq\\_len\\) 长度不一样，我们需要把输入的每个样本重新打包 (pack)。具体来讲，pytorch 提供了 torch.nn.utils.rnn.pack_padded_sequence 接口，它会帮我们把输入转为一个 PackedSequence 对象，而后者就包含了每个样本的 \\(seq\\_len\\) 信息。pack_padded_sequence最主要的输入是输入数据以及每个样本的 \\(seq\\_len\\) 组成的 list。需要注意的是，我们必须把输入数据按照 \\(seq\\_len\\) 从大到小排列后才能送入 pack_padded_sequence。我们继续之前的例子： 12345678910111213141516171819202122232425# 对seq_len进行排序order_idx = np.argsort(seq_lengths)[::-1]print('order_idx:', str(order_idx))order_x = x[order_idx.tolist()]order_seq = seq_lengths[order_idx]print(order_x)&gt;&gt;&gt; order_idx: [1 0] tensor([[[ 1.], [ 2.], [ 3.]], [[ 1.], [ 0.], [ 0.]]])# 经过以上处理后，长序列的样本调整到短序列样本之前了# pack itpack = pack_padded_sequence(order_x, order_seq, batch_first=True)print(pack)&gt;&gt;&gt;PackedSequence(data=tensor([[ 1.], [ 1.], [ 2.], [ 3.]]), batch_sizes=tensor([ 2, 1, 1])) 理解这里的 PackedSequence 是关键。 前面说到，RNN 其实就是在循环地 forward。在上面这个例子中，它每次 forward 的数据是这样的： 第一个序列中，由于两个样本都有数据，所以可以看作是 \\(batch\\_size=2\\) 的输入，后面两个序列只有第一个样本有数据，所以可以看作是 \\(batch\\_size=1\\) 的输入。因此，我们其实可以把这三个序列的数据分解为三个 batch 样本，只不过 batch 的大小分别为 2，1，1。到这里你应该清楚 PackedSequence 里的 data 和 batch_size 是什么东西了吧，其实就是把我们的输入数据重新整理打包成 data，同时根据我们传入的 seq list 计算 batch_size，然后，RNN 会根据 batch_size 从打包好的 data 里面取数据，然后一遍遍的执行 forward 函数。 理解这一步后，主要难点就解决了。 RNN的输出 从文档中可以看出，RNN 输出两个东西：output 和 h_n。其中，h_n 是跑完整个时间序列后 hidden state 的数值，其中 num_layers 表示这个 RNN 有多少层神经元，比如，一个由两层 500 维的全连接层组成的 RNN，其 num_layers 就是 2。但 output 又是什么呢？之前不是说过原始的 RNN 只输出 hidden state 吗，为什么这里又会有一个 output？其实，这个 output 并不是我们理解的网络最后的 output vector，而是每次 forward 后计算得到的 hidden state。毕竟 h_n 只保留了最后一步的 hidden state，但中间的 hidden state 也有可能会参与计算，所以 pytorch 把中间每一步输出的 hidden state 都放到 output 中（当然，只保留了 hidden state 最后一层的输出），因此，你可以发现这个 output 的维度是 (seq_len, batch, num_directions * hidden_size)。 不过，如果你之前用 pack_padded_sequence 打包过数据，那么为了保证输入输出的一致性，pytorch 也会把 output 打包成一个 PackedSequence 对象，我们将上面例子的数据输入 RNN ，看看输出是什么样子的： 12345678910# initializeh0 = Variable(torch.randn(n_layers, batch_size, hidden_size))# forwardout, _ = rnn(pack, h0)print(out)&gt;&gt;&gt; PackedSequence(data=tensor([[ -0.3207, -0.4567], [ 0.6665, 0.0530], [ 0.4456, 0.1340], [ 0.3373, -0.3268]]), batch_sizes=tensor([ 2, 1, 1])) 输出的 PackedSequence 中包含两部分，其中 data 才是我们要的 output。但这个 output 的 shape 并不是 (seq_len, batch, num_directions * hidden_size)，因为 pytorch 已经把输入数据中那些填充的 0 去掉了，因此输出来的数据对应的是真实的序列长度。我们要把它重新填充回一个方方正正的 tensor 才方便处理，这里会用到另一个相反的操作函数 torch.nn.utils.pad_packed_sequence： 12345678910111213# unpackunpacked = pad_packed_sequence(out)out, bz = unpacked[0], unpacked[1]print(out, bz)&gt;&gt;&gt; tensor([[[ -0.3207, -0.4567], [ 0.6665, 0.0530]], [[ 0.4456, 0.1340], [ 0.0000, 0.0000]], [[ 0.3373, -0.3268], [ 0.0000, 0.0000]]]) tensor([ 3, 1]) 现在，这个 output 的 shape 就是一个标准形式了。 不过我一般更习惯 batch_size 作为第一个维度，所以可以稍微调整下： 12345678910111213141516171819# seq_len x batch_size x hidden_size --&gt; batch_size x seq_len x hidden_sizeout = out.permute((1, 0, 2))print(\"output\", out)print(\"input\", order_x)&gt;&gt;&gt; output tensor([[[-0.1319, -0.8469], [-0.3781, -0.8940], [-0.4869, -0.9621]], [[-0.8569, -0.7509], [ 0.0000, 0.0000], [ 0.0000, 0.0000]]]) intput tensor([[[ 1.], [ 2.], [ 3.]], [[ 1.], [ 0.], [ 0.]]]) 现在，输入输出就一一对应了。之后，你可以从 output 中取出你需要的 hidden state，然后接个全连接层之类的，得到真正意义上的 output vector。取出 hidden state 一般会用到 torch.gather 函数，比如，如果我想取出最后一个时间序列的 hidden state，可以这样写 (这段代码就不多解释了，请查一下 torch.gather 的用法，自行体会)： 12345678910111213# bz来自上面的例子, bz=tensor([ 3, 1])bz = (bz - 1).view(bz.shape[0], 1, -1)print(bz)bz = bz.repeat(1, 1, 2)print(bz)out = torch.gather(out, 1, bz)print(out)&gt;&gt;&gt; tensor([[[2]], [[0]]]) tensor([[[2, 2]], [0, 0]]) tensor([[[-0.4869, -0.9621]], [[-0.8569, -0.7509]]]) 对了，最后要注意一点，因为 pack_padded_sequence 把输入数据按照 \\(seq\\_len\\) 从大到小重新排序了，所以后面在计算 loss 的时候，要么把 output 的顺序重新调整回去，要么把 target 数据的顺序也按照新的 \\(seq\\_len\\) 重新排序。当 target 是 label 时，调整起来还算方便，但如果 target 也是序列类型的数据，可能会多点体力活，可以参考这篇文章进行调整。 参考 tensor flow dynamic_rnn 与rnn有啥区别？ pytorch官方文档 读PyTorch源码学习RNN（1） why do we “pack” the sequences in pytorch? pytorch对可变长度序列的处理 The Unreasonable Effectiveness of Recurrent Neural Networks Understanding LSTM Networks pytorch RNN 变长输入 padding","raw":null,"content":null,"categories":[{"name":"NLP","slug":"NLP","permalink":"https://jermmy.github.io/categories/NLP/"}],"tags":[{"name":"深度学习","slug":"深度学习","permalink":"https://jermmy.github.io/tags/深度学习/"},{"name":"pytorch","slug":"pytorch","permalink":"https://jermmy.github.io/tags/pytorch/"},{"name":"tensorflow","slug":"tensorflow","permalink":"https://jermmy.github.io/tags/tensorflow/"},{"name":"nlp","slug":"nlp","permalink":"https://jermmy.github.io/tags/nlp/"},{"name":"rnn","slug":"rnn","permalink":"https://jermmy.github.io/tags/rnn/"}]},{"title":"论文笔记：Learning warped guidance for blind face restoration","slug":"2018-10-3-paper-note-learning-warped-guidance-for-blind-face-restoration","date":"2018-10-03T08:18:38.000Z","updated":"2018-12-16T01:35:31.000Z","comments":true,"path":"2018/10/03/2018-10-3-paper-note-learning-warped-guidance-for-blind-face-restoration/","link":"","permalink":"https://jermmy.github.io/2018/10/03/2018-10-3-paper-note-learning-warped-guidance-for-blind-face-restoration/","excerpt":"这篇论文主要是讲人脸修复的，所谓人脸修复，其实就是将低清的，或者经过压缩等操作的人脸图像进行高清复原。这可以近似为针对人脸的图像修复工作。在图像修复中，我们都会假设退化的图像是高清图像经过某种函数映射后得到的（比如，由高清图像得到一张模糊的图像可能是使用了高斯模糊核），因此，图像修复的本质就是把这个函数映射找出来。由于神经网络可以近似任意函数，因此在深度学习时代，图像修复已经是一个被解决得比较好的问题了。比如，在图像去噪或者超分任务中，U-Net、FCN 之类的网络结构已经成为标配了。\n不过，针对人脸的图像修复则是一个更为严苛的任务。原因主要是以下两点：\n\n对于一般的图像，大家可能不会太在意细节恢复得好还是差，但对于人脸来说，由于这是人类最熟悉的部分，因此人脸中的很多细节，如一些皱纹、酒窝等都需要恢复出来才能让人满意，因此，这是一个粒度更细的图像修复任务。\n另外，通常的图像修复都是针对一种退化场景设计的，比如，在去噪任务中，可能就只是针对某种或某几种噪声而言，而不考虑图像模糊等其他因素，因此任务相对简单。但如果退化的种类太多，退化函数本身可能会非常复杂，即使神经网络也未必能近似出来。正如标题中 blind 所言，退化函数的类型、数量我们是无法事先获悉的。事实上，论文考虑了 jpeg 压缩、高斯模糊、高斯噪声、图片放缩等退化方式，并且对每种方式进行随机组合，因此退化函数是非常复杂的。\n\n\n\n","text":"这篇论文主要是讲人脸修复的，所谓人脸修复，其实就是将低清的，或者经过压缩等操作的人脸图像进行高清复原。这可以近似为针对人脸的图像修复工作。在图像修复中，我们都会假设退化的图像是高清图像经过某种函数映射后得到的（比如，由高清图像得到一张模糊的图像可能是使用了高斯模糊核），因此，图像修复的本质就是把这个函数映射找出来。由于神经网络可以近似任意函数，因此在深度学习时代，图像修复已经是一个被解决得比较好的问题了。比如，在图像去噪或者超分任务中，U-Net、FCN 之类的网络结构已经成为标配了。 不过，针对人脸的图像修复则是一个更为严苛的任务。原因主要是以下两点： 对于一般的图像，大家可能不会太在意细节恢复得好还是差，但对于人脸来说，由于这是人类最熟悉的部分，因此人脸中的很多细节，如一些皱纹、酒窝等都需要恢复出来才能让人满意，因此，这是一个粒度更细的图像修复任务。 另外，通常的图像修复都是针对一种退化场景设计的，比如，在去噪任务中，可能就只是针对某种或某几种噪声而言，而不考虑图像模糊等其他因素，因此任务相对简单。但如果退化的种类太多，退化函数本身可能会非常复杂，即使神经网络也未必能近似出来。正如标题中 blind 所言，退化函数的类型、数量我们是无法事先获悉的。事实上，论文考虑了 jpeg 压缩、高斯模糊、高斯噪声、图片放缩等退化方式，并且对每种方式进行随机组合，因此退化函数是非常复杂的。 论文方法 为了让任务简单一些，论文用了一张引导图来引入更多先验信息，这里所说的引导图是同一个人在不同姿势以及环境下的高清图片。这个想法也是很正常的，试想一下，对于一张严重模糊的人脸来说，我们如何确定恢复出来的图像是否有酒窝、有眼角纹呢？在没有任何先验的情况下，这是一个 mission impossible，因此，有了一张引导图后，在修复过程中就显得有据可考了。 不过，由于引导图中人脸的姿势和退化图是不一致的，这对于模型来说引导的作用可能偏弱（毕竟神经网络又不知道引导图中的五官位置在哪），因此，论文先对引导图进行矫正，再用这张矫正的引导图去引导网络修复退化的图片。所以，论文的架构总的来说分为两部分：WarpNet 和 RecNet。 WarpNet 关于如何对引导图的人脸进行矫正的问题，熟悉 STN(Spatial Transform Network) 的读者一定有所了解了。其实就是计算出矫正 (warp) 后的图像中每个像素点对应原图的位置，再根据双线性插值得到新的图像。论文用一个 WarpNet 来做这件事情。这个网络的输入是两张叠在一起 (channel通道) 的图片 &lt;退化图，引导图&gt;，网络的结构采用的 auto-encoder，而训练的依据是 warp 之后人脸的关键点要跟退化图人脸的关键点对齐，也就是图中的 landmark loss。论文采用的数据集其实是不同人的多张高清图片，作者从这些图片中选出正脸的图片作为引导图，然后将其他图片随机采用某种算法 (jpeg压缩等) 退化后得到退化图。因此，可以事先采用其他算法得到每张人脸的关键点，然后用这些关键点来做 landmark loss。 举个接地气的例子。我们可以让网络的输出为一个像素坐标的二维向量矩阵 \\(R^{H \\times W \\times 2}\\)，这里的 \\(H\\)、\\(W\\) 分指图片的高宽，2 是因为每个像素点需要对应 (x, y) 两个数值作为像素位置。例如，如果我们检测到引导图上有个关键点在脸颊的位置是 (20, 10) ，而同样是脸颊这个点，在退化图上的位置是 (22, 14)，那么 WarpNet 算出来的矩阵在 (22, 14) 这个点的值理论上就应该是 (20, 10)。之后，我们就可以根据插值的方式，将 warp 后的图片在 (22, 14) 这个点的像素值填充为原图在 (20, 10) 这个点的像素值。这个矩阵就是论文中所说的 flow field。而 landmark loss 其实就是对网络输出来的矩阵做一次回归，让矩阵这些位置上的向量值和我们理论上需要的向量值做一次 MSE Loss(Mean Square Error)。 不过，关键点的数量毕竟是有限的，这样训练出来的网络虽然可以保证关键点的部位是对齐的，但其他部位却可能偏得很厉害，正因如此，论文中还使用 TV Loss(Total Variation Loss)。这个 Loss 主要是保证 warp 后的图像能平滑一点。比如说，眼珠 warp 后的新像素位置与原来的位置相比偏离了 (10, 20)，那么眼珠周围的像素 warp 后的像素位置与原像素偏离的距离应该也是 (10, 20) 左右，这样一来，只要保证每个像素偏移的距离都差不多，整幅图像就相对平滑一点 (所谓平滑，就是说 warp 后的人脸不会太过扭曲)。具体实现的时候可以通过 flow field 矩阵的梯度来实现。关于 TV Loss 的更多信息，请参考其他资料。 RecNet 训练完 WarpNet 后，就来到论文的重头戏 RecNet。 和其他图像超分的方法类似，RecNet 采用的是 U-Net 结构，它的输入与 WarpNet 类似，也是将 warp 后的图片和退化图叠在一起。在 Loss 选择上，主要是考虑了 MSE Loss 和 VGG Loss。前者是 auto-encoder 中常用的 loss，即通过和 Ground truth 计算逐像素的均方误差来恢复高清图像，而后者通常又称为感知误差 (perceptual loss)，这是借鉴了图像风格化中的思路，即根据一个已经训练好的 VGG 网络，将网络输出的图片和 Ground truth 都输入 VGG 后，对 feature map 做 MSE Loss (选择哪一层 feature map 需要进行实验尝试，一般会用第三层后的)，目的是希望二者在内容上能保持一致。 但是，仅仅靠这两个 Loss 的话，就只能做到普通的图像修复或超分的效果，对于一些更细节的纹理，这两个 Loss 实在是恢复不出来的 (均方误差其实是在拟合一个整体平均值，而纹理这种高频的信息往往会被当作异常点，因此拟合效果不佳)。再者，人脸中的细节还得靠引导图来提供，如果引导图中有纹理，那么就应该恢复出这些纹理，反之亦然。这类似于条件概率，要给网络加一个开关的作用。那如何引导 RecNet 学习呢？论文中采用了 cGAN(Conditional GAN) 的思路，我觉得这个想法也是水到渠成的，首先，MSE Loss 难以拟合出最真实的人脸数据分布，但 GAN 其实可以把这个分布拟合得更好，另外，用了 Condition 后，也可以起到引导的作用 (其实 RecNet 中将引导图和退化图 concat 在一起，本身就很类似 cGAN)。 论文中总共用了两个 GAN，一个是作用在整幅图像上 (Global cGAN)，一个作用在人脸区域 (Local GAN)。Global cGAN 中判别器的输入是三张图片的叠加，真实样本是 (引导图、warp 图、Ground truth)，假样本则是 (引导图、warp 图、网络输出)。而 Local GAN 论文一笔带过了，我猜测应该就是个普通的 GAN，即真实样本是 Ground truth，而假样本是网络输出。 然而，对论文的这种设计方式，我个人持保留意见。首先，cGAN 的想法是正确的，但我认为这个 GAN 的作用应该是要让 RecNet 可以更好地恢复出纹理，因此判别器对纹理的识别能力应该要尽可能的强，而纹理这种东西和图像内容是无关的，也就是说，对于图像中的每个小块，它都能起到判别作用，因此，我觉得判别器的输入应该是一个个图片块，而不是整张图像或整个人脸区域，所以，我倾向于用 Patch cGAN 来做。 实验 说来惭愧，我自己在复现这篇论文的时候，发现训练出来的模型成了一个普通的去噪/超分模型，对于纹理的修复效果很差，引导图几乎不起作用 (也就是说，不管引导图是什么，重建出来的图片几乎是一样的)。我觉得可能 RecNet 在设计上还可以再优化一下，比如把 content 和 style 分出来之类的。当然，可能是我的能力和大佬们有差距，做不出论文的效果。所以，实验的结果就只能请各位看官自己去论文里面找了。 ========== Update 2018.11.01 ============== 经过一个月的奋斗，博主终于复现出了原论文的效果😭。这里面的关键是训练的时候要加大退化的力度，增加退化图片的比例，这样，网络在重建图像的时候，为了获得更好的重建效果，会更多地考虑引导图的信息，从而加大引导图对应的权重系数。另一方面，我发现如果退化的程度较轻，网络会更多地从退化图本身获取信息，淡化引导图的影响，相当于起到开关的作用，而加不加 cGAN 的影响其实不大，这个发现倒是让我有点意外。 参考 Learning Warped Guidance for Blind Face Restoration","raw":null,"content":null,"categories":[{"name":"计算机视觉","slug":"计算机视觉","permalink":"https://jermmy.github.io/categories/计算机视觉/"}],"tags":[{"name":"计算机视觉","slug":"计算机视觉","permalink":"https://jermmy.github.io/tags/计算机视觉/"},{"name":"深度学习","slug":"深度学习","permalink":"https://jermmy.github.io/tags/深度学习/"},{"name":"论文","slug":"论文","permalink":"https://jermmy.github.io/tags/论文/"}]},{"title":"如何在手机上跑深度神经网络","slug":"2018-8-4-how-to-run-deep-learning-on-mobile","date":"2018-08-04T05:20:37.000Z","updated":"2018-08-28T01:32:43.000Z","comments":true,"path":"2018/08/04/2018-8-4-how-to-run-deep-learning-on-mobile/","link":"","permalink":"https://jermmy.github.io/2018/08/04/2018-8-4-how-to-run-deep-learning-on-mobile/","excerpt":"这天，老板跟你说，希望能在手机上跑深度神经网络，并且准确率要和 VGG、GoogleNet 差不多。\n接到这个任务后你有点懵逼，这些网络别说计算量大，就连网络参数也要 100MB 的空间才存得下，放在手机上跑？开玩笑呗。\n老板又说，怎么实现是你的事，我要的只是这个功能。\n你默默地点了点头。","text":"这天，老板跟你说，希望能在手机上跑深度神经网络，并且准确率要和 VGG、GoogleNet 差不多。 接到这个任务后你有点懵逼，这些网络别说计算量大，就连网络参数也要 100MB 的空间才存得下，放在手机上跑？开玩笑呗。 老板又说，怎么实现是你的事，我要的只是这个功能。 你默默地点了点头。 初步尝试：MobileNet v1 问题出在哪 要在手机上跑深度网络，需要在模型参数和计算量上进行优化。 那深度神经网络的计算量和参数量主要体现在哪呢？这里以 VGG16 为例： 第一层卷积： [224 x 224 x 3] –&gt; [224 x 224 x 64]，卷积核大小为 3 x 3（简单起见，这里计算量的计算忽略激活函数） 计算量为：\\(3 \\times 3 \\times 3 \\times 224 \\times 224 \\times 64 \\approx 8.7 \\times 10^7\\) 参数量为：\\(3 \\times 3 \\times 3 \\times 64 = 1728\\) 第二层卷积：[112 x 112 x 64] –&gt; [112 x 112 x 128]，卷积核大小为 3 x 3。 计算量为：\\(3 \\times 3 \\times 64 \\times 112 \\times 112 \\times 128 \\approx 9.2 \\times 10^8\\) 参数量为：\\(3 \\times 3 \\times 64 \\times 128 = 73728\\) …… 第一层全连接层：[14 x 14 x 512] –&gt; [4096]。 计算量为：\\(14 \\times 14 \\times 512 \\times 4096 \\approx 4.1 \\times 10^8\\) 参数量为：\\(4096 \\times 1000 = 4096000\\) …… 两相对比，同时考虑到网络中卷积层比全连层多，就不难发现深度卷积网络中的计算量主要由卷积层承包，而参数则集中在全链接层。因此，要想对模型做优化，可以在卷积层的计算上做点手脚，同时减小全连接层的维度。 Separable Convolution 虽然找到了问题所在，但具体要如何优化卷积层的计算量呢？幸运的是，你在搜索的过程中发现已经有人针对这个问题给出了解决方案：Separable Convolution。这是一种对卷积运算进行分解的方法。 以下例子摘自文末链接：卷积神经网络中的Separable Convolution 假设现在需要做这样一个卷积操作：[64 x 64 x 3] –&gt; [64 x 64 x 4]，那么通常的操作是这样的（假设卷积核大小为 3 x 3）： 这种做法的计算量为：\\(3 \\times 3 \\times 3 \\times 64 \\times 64 \\times 4 = 442368\\)， 参数量为：\\(3 \\times 3 \\times 3 \\times 4 = 108\\)。 而 Separable Convolution 会将该操作分解为两步：Depthwise Convolution 和 Pointwise Convolution。 Depthwise Convolution 的过程其实非常简单，顾名思义，Depthwise 就是每个通道单独做一遍卷积： 这种做法的效果是：[64 x 64 x 3] –&gt; [64 x 64 x 3]，由于是 Depthwise 的，所以只需要三个 [3 x 3 x 1] 的 filter 即可。 因此计算量为：\\(3 \\times 3 \\times 64 \\times 64 \\times 3=110592\\)， 参数量为：\\(3 \\times 3 \\times 3 = 27\\)。 不过 Depthwise 将不同通道之间的联系断开了，而且输出的通道数与输入是一样的。为了得到 [64 x 64 x 4] 的输出，还需要经过 Pointwise Convolution。 Pointwise Convolution 的过程在 Depthwise 之后进行，它是用一个 [1 x 1] 的卷积核把 [64 x 64 x 3] 的 feature map 转换为 [64 x 64 x 4]： 计算量为：\\(1 \\times 1 \\times 64 \\times 64 \\times 3 \\times 4=49152\\)， 参数量为：\\(1 \\times 1 \\times 3 \\times 4 = 12\\)。 我们发现，通过 Separable Convolution 这种分解的方法也可以拼凑出一个 [64 x 64 x 4] 的 feature map， 而这种方法的计算量为：\\(110592 + 49152=159744\\)，而总的参数量为：\\(27 + 12 = 39\\)。 对比原先的 442368 (计算量) 和 108 (参数量)，简直实惠了好多。 于是，你通过这种套路构造出了一个适合手机端运行的深度网络，并简化了全连接层的参数： 图中的 Conv dw 指的就是 Depthwise Convolution。由于是为手机设计的网络，因此你取了个形象的名字：Mobilenet。 不过，这个网络的精度会不会下降呢？你赶紧在 ImageNet 数据集上做了实验： 这个结果实在是太感人了，精度几乎和 GoogleNet 相当，但计算量却只有后者的三分之一，参数量也减少了三分之一（当然也可能是图像分类这个问题相对简单）。 为了方便对模型大小的进一步调整，你提供了两个额外的参数： \\(\\alpha\\)、 \\(\\rho\\)。\\(\\alpha\\) 又称为 Width Multiplier，主要用来控制 feature map 的 channel 数目，因为在某些任务中，很多 feature map 的 channel 可能包含很少的信息，因此少一些，而有些情况可能需要更多的 channel。\\(\\alpha=1\\) 时就是上文中提出的基准网络。\\(\\rho\\) 则是图像的分辨率，由它控制输入图片的大小。 进阶：ShuffleNet v1 Separable Convolution 其实就是 MobileNet v1 的精华了，个人认为，MobileNet v1 能取得成功主要还是那些大网络在处理简单任务时存在大量的冗余，所以 MobileNet v1 用更少的参数量拼凑出同样大小的 feature map 时，性能并没有明显下降。 而 ShuffleNet v1 则是在此基础上进一步压榨卷积操作，它的重点放在了 Pointwise Convolution 上。Pointwise Convolution 的作用是把 feature map 的所有 channel 信息联系起来，但这种联系可能本身就存在冗余。举个例子，一个 [64 x 64 x 4] 的 feature map，通过 [1 x 1 x 4] 的卷积核后，可以得到 [64 x 64 x 1] 的输出，这个 [1 x 1 x 4] 的卷积核其实就是把原来 feature map 上每个位置的所有 channel 信息（一个 [1 x 1 x 4] 的通道向量）进行加权求和，得到下一层 feature map 上的一个点。不过，真的有必要融和整个通道向量的信息吗？如果只对两个通道的信息进行相加，得到的结果会比四个通道差吗？为了探究这个问题，炼丹师们把原来的 Pointwise Convolution 改造成了 Group Convolution，这个 Group Convolution 其实也不是什么新鲜玩意，当年 AlexNet 刚出来的时候，由于显存不足，就曾将卷积操作分为两组，用两张显卡来装 feature map，这种做法导致更少的参数量和计算量，而且在某些任务中并不会对性能产生很大影响。ShuffleNet v1 的炼丹师显然发现了这一点。 Group Convolution 的操作非常简单，还是举之前的例子：一个 [64 x 64 x 4] 的 feature map，要想进一步得到 [64 x 64 x 2] 的 feature map，直接用 Pointwise Convolution 处理的话，需要一个 [1 x 1 x 4 x 2] 的卷积张量。但用上 Group Convolution 后，我们可以这样操作，用一个 [1 x 1 x 2 x 1] 的卷积张量对原来 feature map 四层通道中的前面两层进行卷积操作，得到一个 [64 x 64 x 1] 的 feature map，之后，用另一个 [1 x 1 x 2 x 1] 的卷积张量继续对后面两层进行卷积操作，同样得到一个 [64 x 64 x 1] 的 feature map，这两块 feature map 拼在一起，最终得到一个 [64 x 64 x 2] 的 feature map。 仔细数数，原来 Pointwise Convolution 的计算量为：\\(1 \\times 1 \\times 64 \\times 64 \\times 4 \\times 2=32768\\)，参数量为：\\(1 \\times 1 \\times 4 \\times 2=8\\)，而现在拆成 Group Convolution 后，计算量为：\\(1 \\times 1 \\times 64 \\times 64 \\times 2 \\times 2=16384\\)，参数量为：\\(1 \\times 1 \\times 2 \\times 2=4\\)，计算量和参数量都减少了一半。 鸡贼的读者可能还发现，如果把 Group Convolution 做到极致，每个 Group 只有一个 channel 的话，就变成 Depthwise + Pointwise Convolution 了，哈哈，原来又是拼凑游戏，笑出声。 不过，仅仅用 Group Convolution，说性能不会影响很多人是不信的，毕竟本身就是 Pointwise Convolution，相邻点之间的信息已经忽略了，要是通道上的信息也忽略太多，难免会存在问题。所以，ShuffleNet v1 的 Shuffle 该登场了。炼丹师为了增强 Group Convolution 的鲁棒性，在对通道进行相加时，故意打乱了通道顺序。这样一来，在上面的例子中，本来是 1、2 通道结合得到一个新的点，就变成了 1、3 通道结合，2、4 通道结合了。 这也就是这篇论文的精华所在： 当然啦，估计是考虑到 Group Convolution 本身损失的信息有点严重，论文又特意加了 ResNet 中的短路连接，算是弥补了一点信息： 下图给出的是论文中关于 Shuffle 操作的实验： Cls err 是 ImageNet 数据集上的错误分类率，数值越小证明结果越好，g 则表示 group 的数量。实验结果给出这样一个信息：当 group 的数量越多时，shuffle 的作用也越明显。这一点也很好理解，因为 group 越多，丢失的信息也越多，这时如果能把 channel 打散，那么不同组之间的 channel 信息就有了交流的通道，能在一定程度上增加鲁棒性。 总结 总的来说，MobileNet v1 作为第一个进行手机端优化的工作，其亮点主要是 Depthwise Convolution 和 Pointwise Convolution。ShuffleNet v1 则是在 MobileNet v1 的基础上加入了 Group Convolution，并通过 Shuffle 的方法提高鲁棒性，同时加入短路连接保持网络的表达能力。 参考 卷积神经网络中的Separable Convolution","raw":null,"content":null,"categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://jermmy.github.io/categories/机器学习/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"https://jermmy.github.io/tags/机器学习/"},{"name":"深度学习","slug":"深度学习","permalink":"https://jermmy.github.io/tags/深度学习/"}]},{"title":"浅析boosting算法","slug":"2018-4-16-boosting-algorithm","date":"2018-04-16T12:46:56.000Z","updated":"2018-04-17T02:33:49.000Z","comments":true,"path":"2018/04/16/2018-4-16-boosting-algorithm/","link":"","permalink":"https://jermmy.github.io/2018/04/16/2018-4-16-boosting-algorithm/","excerpt":"我在很久以前的一篇短文提过一丢丢 boosting，今天翻了下统计学习方法，打算记录一点干货的东西。这篇博文会简单介绍一下 「Adaboost」，并总结一下 Adaboost 方法背后的套路：前向分步加法模型。","text":"我在很久以前的一篇短文提过一丢丢 boosting，今天翻了下统计学习方法，打算记录一点干货的东西。这篇博文会简单介绍一下 「Adaboost」，并总结一下 Adaboost 方法背后的套路：前向分步加法模型。 Adaboost 学过机器学习的同学，想必对 Adaboost 都不会陌生，就算不知道算法原理，也听过算法的名字。正如我之前提到的，Adaboost 采用的思想源于一句古老的俗语「三个臭皮匠，赛过诸葛亮」。不过，与 Bagging 类算法（典型如随机森林）不同的是，Adaboost 希望这三个臭皮匠的分工能逐步提高团队的效益，而不是仅仅起到平等投票的作用。具体到机器学习中，假设我们已经有了几个比较弱的分类器，Adaboost 首先从这些分类器中选出一个分类性能最好的，但这个性能最好的分类器总归有分错的样本，对这些分错的样本，Adaboost 再从剩下的分类器中选一个能把这些样本分得最好的分类器，和刚开始的分类器组合成一个新的分类器，依此类推下去，直到所有弱分类器都组合在一起，成为一个更强的分类器。 算法流程 下面我们用更加数学的语言来表达上面的过程。 假设有一个训练数据集 \\(T={(x_1, y_1), (x_2, y_2), … (x_N, y_N)}\\)，其中 \\(x_i \\in X \\subseteq \\mathbf{R^n}\\)，\\(y_i \\in Y = {-1, +1}\\)（简单起见，假设只有两种分类），并指定一种分类学习算法用于构建弱分类器。接下来我们要用 Adaboost 学习出一个强分类器 \\(G(x)\\)。 Adaboost 算法流程分为以下几步： 先为每个训练样本初始化一个权值分布： \\[ D_1=(w_{1,1},...,w_{1,i},...w_{1,N}), \\ w_{1,i}=\\frac{1}{N}, \\ i=1,2,...,N \\] 将 \\(G(x)\\) 初始化为：\\(G_0(x)=0\\)。 对 \\(m=1,2,…,M\\) 根据学习算法，在权值分布为 \\(D_m\\) 的样本上构建出弱分类器 \\(g_m\\)： \\[ e_m=P(g_m(x_i) \\neq y_i)=\\sum_{i=1}^N w_{m,i}I(g_m(x_i)\\neq y_i) \\] ​ 这里的 \\(I()\\) 表示指示函数，当 \\(g_m(x_i) \\neq y_i\\) 时取 1，否则取 0。 计算它的系数： \\[ \\alpha_m=\\frac{1}{2}\\log{\\frac{1-e_m}{e_m}} \\] （这一步要注意，如果 \\(e_m\\) 都大于 0.5，证明我们已经找不出弱分类器了，这时要么算法停止，要么从头开始） 将得到的分类器加入 \\(G(x)\\) 中：\\(G_m(x)=G_{m-1}(x)+g_m(x)\\)。 更新所有训练数据的权值分布： \\[ D_{m+1}=(w_{m+1,1},...,w_{m+1,i},...,w_{m+1,N}) \\\\ w_{m+1,i}=\\frac{w_{m,i}}{Z_m}\\exp(-\\alpha_m y_i G_m(x_i)) \\] ​ 这里的 \\(Z_m\\) 表示规范化因子：\\(Z_m=\\sum_{i=1}^N{w_{m,i}\\exp(-\\alpha_m y_i G_m(x_i))}\\)。 对这些弱分类器进行线性组合，得到最终的分类器： \\[ G(x)=sign(f(x))=sign(\\sum_{m=1}^M \\alpha_m G_m(x)) \\] 整个算法流程看起来挺复杂的，它是怎么体现之前说的 boosting 的思想的呢？ 首先，在计算分类误差率这一步，由于指示函数 \\(I()\\) 的作用，我们只会挑选出上一步中仍没法正确分对的样本进行判断。在构造出弱分类器后，由于这个弱分类器在这些样本上又能做到 50% 以上的准确率，因此它能使 \\(G(x)\\) 的总体性能提升一点点。新分类器的权重系数的计算函数为 \\(\\alpha_m=\\frac{1}{2}\\log{\\frac{1-e_m}{e_m}}\\)，其图像如下： 这个函数在 \\(e_m &gt; 0.5\\) 时会取到负值，由于开始时所有样本的权重均匀，加之弱分类器准确率超过 50%，因此算法开始执行时 \\(e_m &gt; 0.5\\) 的情况是不会发生的，但随着样本权重的改变，如果之后构造的弱分类器的准确率低于 50%，为了防止分类器权值为负，算法就会终止。如果一个分类器的 \\(e_m\\) 越小，证明它的错分率越低，效果越好，则它的权重会越大。 每轮计算完弱分类器后，算法会根据公式 \\(\\frac{w_{m,i}}{Z_m}\\exp(-\\alpha_m y_i G_m(x_i))\\) 更新样本权重，如果这一轮得到的分类器 \\(G_m\\) 对某个样本的预测错误，证明这个样本仍然是块「硬骨头」，这种情况下，\\(\\exp(-\\alpha_m y_i G_m(x_i))&gt;1\\)，表示该样本的权重相比上一次而言增大了，这样一来，在下一次构造弱分类器时，该样本会得到更多的关注，而对那些已经正确分类的样本，权重反而会降低。因此，在每轮构造弱分类器的过程中，为了得到更好的分类效果，弱分类器会去挑选那些能把「硬骨头」样本分得更好的特征，所以，Adaboost 在某种程度上来说，具备特征选择的作用。 提升树 提升树也属于一种 boosting 方法，只不过由算法的名字我们也可以推测，这种方法采用的弱分类器是「树」。如果要解决分类问题，可以用决策树，而回归问题则可以用回归树（关于决策树和回归树，可以参考其他资料，或者我之前整理的一篇文章）。 既然是 boosting 方法，那么提升树也和 Adaboost 一样，遵循同样的方法论。如果弱分类器采用的是决策树，那么提升树方法其实等同于弱分类器为决策树的 Adaboost，其算法流程和 Adaboost 一样，所以这里就不再展开讲了。下面分析一下回归问题中提升树的算法原理。 在回归问题中，提升树采用的弱分类器（或者说，弱回归器）是 CART。同样地，假设有一个训练数据集 \\(T={(x_1, y_1), (x_2, y_2), … (x_N, y_N)}\\)，其中，\\(x_i \\in X \\subseteq \\mathbf{R^n}\\)，\\(y_i \\in Y \\subseteq \\mathbf{R}\\)。 前向分步加法模型 参考 机器学习 统计学习方法","raw":null,"content":null,"categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://jermmy.github.io/categories/机器学习/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"https://jermmy.github.io/tags/机器学习/"}]},{"title":"KMP算法","slug":"2018-2-13-kmp-algorithm","date":"2018-02-13T03:11:40.000Z","updated":"2018-03-03T09:19:22.000Z","comments":true,"path":"2018/02/13/2018-2-13-kmp-algorithm/","link":"","permalink":"https://jermmy.github.io/2018/02/13/2018-2-13-kmp-algorithm/","excerpt":"这篇文章想简单讲讲 KMP 算法的内容。\nKMP 算法\nKMP 算法由 Knuth–Morris–Pratt 三个人共同提出，它的目的是判断字符串 A 中是否包含另一个字符串 B（如：判断 abababaababacb 中是否包含 ababacb）。","text":"这篇文章想简单讲讲 KMP 算法的内容。 KMP 算法 KMP 算法由 Knuth–Morris–Pratt 三个人共同提出，它的目的是判断字符串 A 中是否包含另一个字符串 B（如：判断 abababaababacb 中是否包含 ababacb）。 KMP 算法流程 KMP 下面演示一下 KMP 的流程。假设我们要判断字符串 A（abababaababacb）中是否包含字符串 B（ababacb）。 我们分别用两个指针 i 和 j 指示 A、B 匹配的位置。 首先比较第一个位置： 1234i: 0A: aB: aj: 0 匹配了 a 跟 a，向前移动指针 i 和 j： 1234i: 01 A: abB: abj: 01 匹配了 b，继续向前移动指针 i、j，直到： 1234i: 01234 5A: ababa bB: ababa cj: 01234 5 按照常规的方法，我们要把 i 从上次起始点 0 移动到 1，而 j 则回到 0 继续匹配。但你是否注意到一个现象：我们已经用 B 的 ababa 匹配了 A 的 ababa，也就是说，我们已经掌握 A、B 前面这部分的信息，那么，对于前面这部分信息能否相互匹配，我们其实已经知道了。 比如说，我们没有必要把 i 和 j 重新调回 1 和 0，因为 A[1] 和 B[0] 肯定是不匹配的。最明智的做法是调成下面这种状态： 1234i: 01234 5A: ababa bB: aba bj: 012 3 i 还是在 5 的位置，而 j 则调整到 3，然后继续后面的匹配工作。 这一步，就是 KMP 的精髓所在（这个 j 的位置如何调整，后面会说）。 继续这个例子，当 i 走到 7 的时候，又没法匹配了： 1234i: 0123456 7A: abababa aB: ababa cj: 01234 5 同样的道理，我们可以把 B 串往后挪动，而保持 A 串不动（其实就是 i 不变，移动 j）。这一次，j 还是调整到 3 的位置： 1234i: 0123456 7A: abababa aB: aba bj: 012 3 我们发现，经过这次调整，依然没法匹配下去，那只能继续挪动 B，直到： 1234i: 01234567A: abababaaB: aj: 0 j 这次被打回原形了。 之后，我们可以一路匹配直到结束： 1234i: 0123456789 10 11 12 13A: abababaaba b a c bB: aba b a c bj: 012 3 4 5 6 到这里，我们先总结一下，假设在每次出现不匹配时，我们已经知道了如何调整 j，那上面的流程可以写成下面的代码： 1234567891011int kmp(string A, string B) &#123; int n = A.length(); for (int i = 0, j = 0; i &lt; n; i++) &#123; if (A[i] == B[j]) j++; if (j == B.length()) return i - j + 1; // 返回 A、B 匹配的起点 // 如果 A 的下一位和 B 不匹配，则不断调整 j，直到匹配或者j回到0为止 while (j &gt; 0 &amp;&amp; A[i + 1] != B[j]) &#123; j = next[j - 1]; // next[j]表示当j+1位出现不匹配时，j应该回到next[j]的位置 &#125; &#125;&#125; 上面这段代码的时间复杂度为 O(n)，具体可以看这篇文章的分析。 next 数组 现在问题来了：我们该如何调整 j 的位置呢？ 看回刚才那个例子，当出现下面这种不匹配的情况时， 1234i: 01234 5A: ababa bB: ababa cj: 01234 5 我们是这样调整 j 的： 1234i: 01234 5A: ababa bB: aba bj: 012 3 由于 B 中的 ababa 和 A 是匹配的，所以 A 前面那一串肯定是 ababa。然后我们才能把 B 前三个字符（B[0 : 2] = aba）移到后面，跟 A 中 ababa 的后三个字符匹配。 前三个字符跟后三个字符匹配！ 前三个字符跟后三个字符匹配！ 前三个字符跟后三个字符匹配！ 这是关键。 ababa 是 A 的字符串，同时也是 B 的字符串，所以这些新位置的计算完全可以仅仅根据 B 来预处理。 现在，我们重新审视这个过程。 假设 B（ababacb） 跟某个字符串进行匹配，在 j = 5 时才发生失配： 1234i:A: ***** *B: ababa cj: 01234 5（01234都是匹配的，5开始不匹配） 这时，我们可以在不管 A 的情况下，将 j 调整为： 1234i:A: ***** *B: aba bj: 012 3 为什么可以这么做？因为 ababa 的后三个字符和前三个字符是相等。 现在，你应该明白 j 的位置要怎么调整了。它本质上是在计算 B 子串中最长且相等的前缀和后缀，是 B 自己对自己的匹配。 通常，我们会用一个部分匹配表来记录这部分信息（即之前代码中的next数组）。 我们继续用一个例子来解释 next 数组的计算流程。为了让例子更具代表性，我们选用 B = abababca。在下面的例子中，我们同样会用 i、j 两个指针来标示，这一次是用 B 来匹配 B（请铭记：next[i] 表示的是 B 的子串 B[0 : i] 中，最长且相等的前缀和后缀的长度，它和前面代码中的注释本质是一样的。⚠️B[0 : i] 包含 B[0] 到 B[i] 总共 i+1 个字符）。 例子中的 n 代表 next 数组。 我们默认 next[0] = 0，因为 B[0] 就只包含一个字符，不存在前缀后缀。 所以匹配从第二个字符开始： 12345i: 01B: ab******B: aj: 0n: 00 B[0 : 1] 中，前缀是 B[0]，后缀是 B[1]，二者不等，所以 next[1] = 0（最长且相等的前后缀的长度为 0）。 因为没有匹配上，我们只能移动 i，固定 j： 12345i: 012B: aba*****B: aj: 0n: 001 终于匹配到了一个，next[2] = 1。 之后，i、j 同时移动： 12345i: 0123B: abab****B: abj: 01n: 0012 在 B[0 : 3] 这个串 (abab) 中，我们继续匹配到 b，现在匹配的前缀和后缀变为 ab，next[3] = 2。 以此类推: 12345i: 012345B: ababab**B: ababj: 0123n: 001234 不过，再往前走一步，情况就复杂了： 12345i: 012345 6B: ababab c*B: abab aj: 0123 4n: 001234 第 6 位，c 这颗老鼠屎，搅坏一锅粥。怎么办呢？只能重新调整 j 了。但是，我们不能一口气将 j 调回 0，因为这一步中，j != 0 告诉我们：c 之前的串是能够匹配的呀。而我们的目的也是要找最长的前缀和后缀，因而，虽然前面千辛万苦找到的 abab 现在是匹配不下去了，我们能不能继续找一个长度小于 4 的匹配串呢？比如，abab 中的前缀 ab 和后缀 ab 也是能匹配的呀。所以，我们将 j 调成这个样子，看能不能挽救一下： 12345i: 012345 6B: ababab c*B: ab aj: 01 2n: 001234 结果还是挽救不了，因为 B[6] 和 B[2] 不相等。但此时，j 还是大于 0，也就是说前面还是有子串是匹配的。不过，眼睛瞄一下也知道，剩下的 ab 本身是不存在匹配情况的，所以这下只能将 j 调回 0 了： 12345i: 0123456B: abababc*B: aj: 0n: 0012340 上面这个「挽救」的过程，其实是求 next 数组中最难理解的地方（而 next 是 KMP 最难理解的地方）。 再回顾一下，我们遇到 c 之后，不是直接将 j 置 0，而是从之前匹配到的子串中，寻找可能的前缀和后缀。在这个例子中，我们已经匹配到的是 abab，因此之后就是找找 abab 身上的前缀跟后缀。不知道你注意到没有，从 abab 身上找前缀后缀的工作，我们在计算 next[3] 的时候就遇过了👇： 12345i: 0123B: abab****B: abj: 01n: 0012 因为这个 abab 本身也是 B 的前缀，而我们之前已经计算出这个前缀的最长且相等的前后缀长度是 2（next[3] = 2）。 但是，尽管我们挽救了 ab 出来，但还是没法进一步匹配下去，所以又要从 ab 身上挽救点东西。但坑爹的是，我们在计算 next[1] 时就已经发现，ab 本身就没有相等的前后缀👇： 12345i: 01B: ab******B: aj: 0n: 00 next[1] = 0，所以，这一次我们是真没办法了，才将 j 调回 0。一旦 j 回到 0，c 之前也就没有匹配的子串了，一切又从头开始。 希望以上这段解释，能让你明白下面这段代码： 123456789101112131415vector&lt;int&gt; getNext(string B) &#123; int n = B.length(); vector&lt;int&gt; next(n, 0); for (int i = 1, j = 0; i &lt; n; i++) &#123; // 注意，这里的j表示长度 while (j &gt; 0 &amp;&amp; B[i] != B[j]) &#123; j = next[j - 1]; &#125; if (B[i] == B[j]) &#123; j++; &#125; next[i] = j; &#125; return next;&#125; 现在，KMP 的基本流程就讲完了。 KMP 算法我主要是看了这篇文章入门的，但其中，对 next 数组的求解过程一直不明白，于是，我又找了其他文章，但坑爹的是，不同作者的讲解思路和风格都不一样，虽然道理是一样的，但在顿悟道理之前，这些不同的文章还是会让初学者很困惑。最后，实在没辙了，我就找了个例子，按照最开始那篇文章的思路，在那使劲捣鼓，总算折腾出一个在我看来还过得去的解释。不过，我的思考方式不可能适合所有人，如果你看到这堆解释后，依然一头雾水，最好的方法是静下心来，拿出纸笔，对着例子捣鼓一段时间，这样的效率会比不断找文章阅读来的高。 参考 KMP算法详解 The Knuth-Morris-Pratt Algorithm in my own words","raw":null,"content":null,"categories":[{"name":"算法","slug":"算法","permalink":"https://jermmy.github.io/categories/算法/"}],"tags":[{"name":"算法","slug":"算法","permalink":"https://jermmy.github.io/tags/算法/"}]},{"title":"Leetcode题解：98. validate binary search tree","slug":"2018-2-8-leetcode-98-validate-binary-search-tree","date":"2018-02-09T00:54:35.000Z","updated":"2018-03-02T03:13:04.000Z","comments":true,"path":"2018/02/09/2018-2-8-leetcode-98-validate-binary-search-tree/","link":"","permalink":"https://jermmy.github.io/2018/02/09/2018-2-8-leetcode-98-validate-binary-search-tree/","excerpt":"题目\n原题：Given a binary tree, determine if it is a valid binary search tree (BST).\nAssume a BST is defined as follows:\n\nThe left subtree of a node contains only nodes with keys less than the node’s key.\nThe right subtree of a node contains only nodes with keys greater than the node’s key.\nBoth the left and right subtrees must also be binary search trees.\n\nExample 1:\n123  2 / \\1   3\nBinary tree [2,1,3], return true.\nExample 2:\n123  1 / \\2   3\nBinary tree [1,2,3], return false.","text":"题目 原题：Given a binary tree, determine if it is a valid binary search tree (BST). Assume a BST is defined as follows: The left subtree of a node contains only nodes with keys less than the node’s key. The right subtree of a node contains only nodes with keys greater than the node’s key. Both the left and right subtrees must also be binary search trees. Example 1: 123 2 / \\1 3 Binary tree [2,1,3], return true. Example 2: 123 1 / \\2 3 Binary tree [1,2,3], return false. 要求 这道题的要求是验证一棵树是否是二叉搜索树，难度中等。其实这应该是一道二叉树相关的基础题。 在寻找思路之前，我们应该明确二叉搜索树的定义： 一个节点的左子树的所有节点都小于（不能等于）该节点的值； 一个节点的右子树的所有节点都大于（不能等于）该节点的值； 左右子树也必须是二叉搜索树。 思路 方法一 根据二叉搜索树的定义，很容易想到一种简单粗暴的方法：对于每一个节点，我们可以计算其左子树的最大值，判断这个最大值是否比当前节点值小，并计算右子树的最小值，判断最小值是否比当前节点大。只要这两点均满足，这棵树肯定是一棵二叉搜索树。 12345678910111213141516171819202122232425262728293031struct TreeNode &#123; int val; TreeNode *left; TreeNode *right; TreeNode(int x) : val(x), left(NULL), right(NULL) &#123;&#125;&#125;;class Solution &#123;public: bool isValidBST(TreeNode* root) &#123; if (root == NULL) return true; if (root-&gt;left != NULL &amp;&amp; maxValue(root-&gt;left) &gt;= root-&gt;val) return false; if (root-&gt;right != NULL &amp;&amp; minValue(root-&gt;right) &lt;= root-&gt;val) return false; return isValidBST(root-&gt;left) &amp;&amp; isValidBST(root-&gt;right); &#125;private: int minValue(TreeNode* root) &#123; TreeNode* cur = root; while (cur-&gt;left != NULL) &#123; cur = cur-&gt;left; &#125; return cur-&gt;val; &#125; int maxValue(TreeNode* root) &#123; TreeNode* cur = root; while (cur-&gt;right != NULL) &#123; cur = cur-&gt;right; &#125; return cur-&gt;val; &#125;&#125;; 方法二 在写这篇博文的时候，方法一是可以通过 leetcode 测试样例的，这多少出乎我的意料。因为方法一其实效率很低，对于树中的每个节点，我们都需要不断的计算左右子树的最大最小值，这里面存在很多重复计算的地方，不过从方法一可以 AC 这一点，也说明 leetcode 的测试样例是比较简单的。 方法二的目的就是为了克服方法一重复计算的毛病。在方法一中，我们每次计算左右子树的最大最小值，其实是为了给出当前节点值的边界，如果当前节点值在这个最大最小值之间，那这个节点是符合条件的。方法二沿用同样的思路，只不过在计算最大最小值的时候，我们不必再遍历左右子树，而是根据当前节点值是左子树的最大值，且是右子树的最小值这一原则，在每次递归调用的时候记录跟踪这个最大最小值。 123456789101112class Solution &#123;public: bool isValidBST(TreeNode* root) &#123; return _isValidBST(root, INT_MIN, INT_MAX); &#125;private: bool _isValidBST(TreeNode* root, int min, int max) &#123; if (root == NULL) return true; if (root-&gt;val &lt;= min || root-&gt;val &gt;= max) return false; return _isValidBST(root-&gt;left, min, root-&gt;val) &amp;&amp; _isValidBST(root-&gt;right, root-&gt;val, max); &#125; 当二叉树比较大时，这种方法的效率比前一种要高。 遗憾的是，方法二现在已经通不过 leetcode 的测试样例了。当节点值刚好是最小值 INT_MIN 或最大值 INT_MAX 时，上面的方法就会出 bug。虽然我们可以把 int 类型改为 long 来避免这种 bug，不过，这种方法换汤不换药，没法根治问题，这才有了第三种方法。 方法三 相比方法二而言，方法三和方法一更相似。这一次，我们放弃使用 min 和 max 跟踪上下边界的做法，而是继续计算左右子树的最大最小值来判断。不过，我们不必用遍历的方法来计算，而是沿用方法二的思路，在每次递归调用时，记录一个最大节点和最小节点。效率上跟方法二一样，只是这一次我们是用树中的节点来确定一个最大最小边界，而不是借助一个外部设定的 INT_MIN 和 INT_MAX，因此可以避免边界检查的问题。 1234567891011121314class Solution &#123;public: bool isValidBST(TreeNode* root) &#123; return _isValidBST(root, NULL, NULL); &#125;private: bool _isValidBST(TreeNode* root, TreeNode* min, TreeNode* max) &#123; if (root == NULL) return true; if (min != NULL &amp;&amp; root-&gt;val &lt;= min-&gt;val) return false; if (max != NULL &amp;&amp; root-&gt;val &gt;= max-&gt;val) return false; return _isValidBST(root-&gt;left, min, root) &amp;&amp; _isValidBST(root-&gt;right, root, max); &#125;&#125;; 参考 Leetcode Validate Binary Search Tree","raw":null,"content":null,"categories":[{"name":"算法","slug":"算法","permalink":"https://jermmy.github.io/categories/算法/"}],"tags":[{"name":"leetcode, 树, 算法","slug":"leetcode-树-算法","permalink":"https://jermmy.github.io/tags/leetcode-树-算法/"}]},{"title":"论文笔记：Fast(er) RCNN","slug":"2018-1-15-paper-notes-fast-er-rcnn","date":"2018-01-15T08:13:14.000Z","updated":"2019-05-08T01:35:24.000Z","comments":true,"path":"2018/01/15/2018-1-15-paper-notes-fast-er-rcnn/","link":"","permalink":"https://jermmy.github.io/2018/01/15/2018-1-15-paper-notes-fast-er-rcnn/","excerpt":"在 RCNN 初步试水取得成功后，研究人员又迅速跟进，针对 RCNN 中的几点不足提出改进，接连推出了 fast-rcnn 和 faster-rcnn。关于这两篇论文，网上相关的文章实在是多如牛毛，因此，本篇博文不打算深入讲解，只是不落俗套地介绍一下它们改进的痛点，基本流程，以及我自己对一些小问题的理解。","text":"在 RCNN 初步试水取得成功后，研究人员又迅速跟进，针对 RCNN 中的几点不足提出改进，接连推出了 fast-rcnn 和 faster-rcnn。关于这两篇论文，网上相关的文章实在是多如牛毛，因此，本篇博文不打算深入讲解，只是不落俗套地介绍一下它们改进的痛点，基本流程，以及我自己对一些小问题的理解。 RCNN 的问题 我们先回忆一下 RCNN 做了哪些事情： Selective Search 选出候选区域（region proposal）； CNN 对这些区域提取特征； SVM 对 CNN 提取的特征进行分类预测； 一个简单的线性回归模型做 bounding box regression（就是对候选区域进行微调）。 原作者之一 rgb 在 Fast RCNN 的论文中就提出了 RCNN 几个很明显的短板。首先，训练是分阶段进行的。为了训练 RCNN，我们需要对 CNN 进行训练，然后，在用它提取的特征对 SVM 进行训练，完了还要训练一个线性回归模型，实在烦琐至极。其次，训练过程很耗费时间和磁盘空间。因为 CNN 是在 Selective Search 挑选出来的候选区域上进行的特征提取，而这些区域很多都是重叠的，换句话说，很多卷积运算都是重复的，另外，CNN 提取的特征需要先保存下来，以便后续对 SVM 的训练，而这些高维度的特征，占据的空间都是几百 G 大小。第三，检测过程很缓慢。这一点和第二点很类似，基本是由卷积运算的重复进行造成的。 Fast RCNN的改进 基本思路 针对 RCNN 这几个短板，很容易想到的一个改进就是对 CNN 卷积操作的结果进行重复利用，也就是说，我们可以先用 CNN 对整幅图片提取特征，得到某一层的特征图（一般是取全联接层前面的那一层），然后，用 Selective Search 对原图提取候选框，根据相应的缩放比例，可以在特征图上找出候选框对应的区域，直接用这些区域的特征作为候选区域的特征即可。这样，我们相当于只在原图上做了一遍卷积操作，而不是每个候选区域做一遍。 除此之外，为了简化训练的流程。作者把 SVM 分类器换成 Softmax，和 CNN 整合成一个网络，同时把 bounding box regression 也整合进网络中（相当于同一个网络同时进行物体判别和区域微调）。这样，可以在保证准确率的同时，提高训练的效率。 Fast RCNN 的改进可以用下面两幅图概括。其中，左图是原 RCNN 的做法，而右图则是 Fast RCNN 的做法。 以上两点基本就是 Fast RCNN 所做的改进了。替换 SVM 这一步是很容易实现的，整合线性模型的操作也可以借助 multi-task CNN 的思想实现，但共享卷积操作却会遇到一个严重的问题。因为卷积得到的特征，最后都需要送入全联接层进行降维等操作，而全联接层输入向量的维度必须是固定。由于我们现在是在 feature map 上，根据 SS 提取的候选区域，截取了一小块区域的特征作为该区域的图片特征，因此肯定不符合全联接层的要求（原本的全联接层是针对整个 feature map 的维度进行计算的）。所以下面重点分析一下论文是怎么处理的。 ROI Pooling Layer 为了让全联接层能够接收 Conv-Pooling 后的特征，我们要么是重新调整 pooling 后的特征维度，使它适应全联接层，要么是改变全联接层的结构，使它可以接收任意维度的特征。后者一个有效的解决方案是 FCN（全卷积网络），不过 Fast RCNN 出来之时还没有 FCN，因此它采用的是前一种思路。 那要如何调整 pooling 后的特征呢？论文提出了一种 ROI Pooling Layer 的方法（ROI 指的是 Region of Interest）。事实上，这种方法并不是 Fast RCNN 的原创，而是借鉴了 SPPNet 的思路。关于 SPPNet，网上资料很多，就不再赘述了，所以我开门见山讲一下 ROI Pooling Layer 是怎么处理的。假设首个全联接层接收的特征维度是 \\(H \\times W \\times D\\)，例如 VGG16 的第一个 FC 层的输入是 \\(7 \\times 7 \\times 512\\)，其中 512 表示 feature map 的层数。那么，ROI Pooling Layer 的目标，就是让 feature map 上的 ROI 区域，在经过 pooling 操作后，其特征输出维度满足 \\(H \\times W\\)。具体做法是，对原本 max pooling 的单位网格进行调整，使得 pooling 的每个网格大小动态调整为 \\(\\frac{h}{H} \\times \\frac{w}{W}\\)（假设 ROI 区域的长宽为 \\(h \\times w\\)）。这样，一个 ROI 区域可以得到 \\(H \\times W\\) 个网格。然后，每个网格内依然采用 max pooling 操作。如此一来，不管 ROI 区域大小如何，最终得到的特征维度都是 \\(H \\times W \\times D\\)。 下图显示的，是在一张 feature map 上，对一个 \\(5 \\times 7\\) 的 ROI 区域进行 ROI Pooling 的结果，最后得到 \\(2 \\times 2\\) 的特征。 这时，可能有人会问，如果 ROI 区域太小怎么办？比如，拿 VGG16 来说，它要求 Pooling 后的特征为 \\(7 \\times 7 \\times 512\\)，如果碰巧 ROI 区域只有 \\(6 \\times 6\\) 大小怎么办？还是同样的办法，每个网格的大小取 \\(\\frac{6}{7} \\times \\frac{6}{7} = 0.85 \\times 0.85\\)，然后，以宽为例，按照这样的间隔取网格： \\([0, 0.85, 1.7, 2.55, 3.4, 4.25, 5.1, 5.95]\\)， 取整后，每个网格对应的起始坐标为：\\([0, 1, 2, 3, 3, 4, 5]\\)。 CNN 学习回归参数 解决 ROI Pooling Layer 后，Fast RCNN 的难点基本就解决了。不过，博主是那种容易钻牛角尖的人，在刚开始看到用 CNN 预测 BBox Regression 时一直疑惑不解。我认为模型拟合的数据之间是要满足因果关系的。假设我们输入的图片中包含一只猫，训练阶段，CNN 在对猫所在的 ROI 矩形区域进行矫正时，它是参考 ground truth 标定的矩形框进行修正的。下一次，假设我们输入同样的图片，只不过图片中猫的位置变化了（猫的姿势等都不变，仅仅是位置变了），那么，CNN 根据 ground truth 进行学习的修正参数，应该跟上一次是一样的，但是，这一次它所参考的 ground truth 却已经换了不同的坐标了，那它又要怎么学习呢？ 在查了跟 Bounding Box Regression 相关的资料后，我才发现自己犯蠢了。其实，Bounding Box Regression 学的是一个微调的坐标参数，是一个相对值。也就是说，不管同一个物体在图片中的位置怎么变，网络要学习的，都是相对真实 ground truth 的坐标偏移和尺寸变化，跟物体的绝对位置没有半毛钱关系。 当模型训练好后，对于某一特征，网络已经知道这种特征应该如何调整矩形框了。说得简单粗暴一点，就是网络已经知道，对于 Selective Search 找出来的这种物体，它的矩形框偏离了多少，该如何调整。 （前面这一段说得比较绕，不过应该也没几个人会被这种问题卡住～囧～） 前面说到，Fast RCNN 将物体检测和微调矩形框的任务用一个网络一起学习。其实，就是让 CNN 学习两个代价函数，其中一个用于物体检测，另一个用于 BBox Regression。 物体检测的函数是常见的 Softmax，而 BBox Regression 则是一个比较特殊的函数： \\[ L_{loc}(t^u,v)=\\sum_{i \\in \\{x,y,w,h\\}}smooth_{L_1}(t_i^u-v_i) \\] 其中， \\[ smooth_{L_1}(x)=\\begin{cases} 0.5x^2 &amp; \\text{if |x|&lt;1} \\\\ |x|-0.5 &amp; \\text{otherwise} \\end{cases} \\] 式中的 \\(|x|\\) 采用的是 \\(L_1\\) 范数。\\(t^u=(t_x^u, t_y^u, t_w^u, t_h^u)\\) 表示预测的矩形框（其实就是 Selective Search 找出来的包含物体的区域），x, y, w, h 分别表示矩形区域的中心坐标以及宽高。而 \\(v=(v_x, v_y, v_w, v_h)\\) 则是 ground truth。 而网络总的代价函数为： \\[ L(p,u,t^u,v)=L_{cls}(p,u)+\\lambda[u \\ge 1]L_{loc}(t^u,v) \\] \\(L_{cls}\\) 是 softmax 对应的分类损失函数，\\(\\lambda\\) 是一个权重，文中取 1，\\([u \\ge 1]\\) 表示只有矩形框中检测到物体才会执行 \\(L_{loc}\\) 函数。 Faster RCNN的进击 Faster RCNN，顾名思义，就是比 Fast RCNN 更快。那 Fast RCNN 中，还有什么地方存在短板呢？研究人员发现，检测部分基本都在一个网络中进行了，但候选区域粗提取的工作（region proposal）还是在 CPU 中进行（用 Selective Search）。而 Selective Search 本质上也是对图像特征的分析，那为什么这块分析的工作不直接利用卷积网络运算的结果呢？而且，如果能把所有工作统一起来共同放在 GPU 中进行，不正了了偏执狂们的一桩心愿吗？！于是，人们开始研究，有没有办法用一个网络来取代 Selective Search。这也导致 Faster RCNN 的诞生。 Region Proposal Network Faster RCNN 提出了一种 Region Proposal Network（RPN），看名字就知道，这个网络是用来提取 region 的。在传统的物体检测算法中，我们一般是用滑动窗口来扫描原图，然后针对每个窗口提取特征。RPN 的思路与之类似，不过，为了共享卷积层的运算，它是在卷积网络的 feature map 上，以每个特征点为中心，用一个 \\(n \\times n\\) 的矩形窗口进行扫描。论文中，n 被设为 3。那我们该如何判断窗口内是否有物体呢？由于卷积网络得到的 feature map 在尺寸上和原图存在一定的比例关系，所以，我们可以把滑动窗口按比例换算回原图，然后对比原图的 ground truth，根据某种事先定好的规则，来判断这个窗口是否包含物体（比如，跟 ground truth 的矩形的 IoU 大于某个阈值就认为包含物体）。在 \\(n \\times n\\) 的窗口之上，论文又用一个 \\(n \\times n\\) 的卷积层，对窗口范围内的 feature map 进行卷积，然后用全联接网络输出二分类的结果（前景还是背景）以及对矩形窗口的粗调整（类似 Fast RCNN 中的 bounding box regression，不过这一步的调整相对粗糙一些）。 上面就是 RPN 的基本思想了。总的来说，可以认为 RPN 就是在滑动窗口上，接着的一个小网络，这个网络会判断窗口内是否有物体，以及会对原图的窗口进行粗调整（原图的窗口是 feature map 上的窗口按比例换算得到的）。 不过，直接根据滑动窗口换算回原图存在一个 bug。试想一下，如果 ground truth 只占这个滑动窗口的一部分（也就是说二者的 IoU 不满足筛选条件），但这一部分又刚好是物体的重要部位，那我们应该认为这个窗口有物体还是没物体呢？ 所以，为了防止这种尴尬的事情发生，或者说，为了防止有些窗口被漏捡，我们在换算回原图的窗口时，要尝试不同的窗口尺寸，而不是规规矩矩按照固定的缩放比例。比如，我们可以稍微将原图的窗口调大一些，或调小一些，或将长宽的比例做调整，总之，就是尽可能 match 到窗口内的 ground truth。论文一共试了 k 种组合（实验中，取了 9 种组合，窗口面积为 {128, 256, 512} x 长宽比为 {1:1, 1:2, 2:1}）。feature map 上的一个点对应一个窗口，这个窗口内的特征输入 RPN 网络后，最终输出 \\(k \\times 2\\) 个分类结果（表示 k 个窗口分别对应前景还是后景）以及 \\(k \\times 4\\) 个窗口粗调整的结果（表示 k 个窗口应该怎样调整）。论文中，这些原图上的窗口又被称为 Anchor，以便和 feature map 上的滑动窗口区分开。注意，feature map 上的滑动窗口尺寸始终是 \\(3 \\times 3\\)，而且每次都只移动一步。有人可能会问，如果滑动窗口对应的 Anchor 中，存在多个物体怎么办？不影响的，因为 RPN 只判断前景跟后景，不做细致分类，而且，RPN 的输出中，k 个窗口会对应 k 个输出。如果有两个 Anchor 对应两个物体，那么，RPN 会将这两个 Anchor 都标记为 前景，并且根据它们各自的输出，微调这两个 Anchor 的位置。 训练的时候，作者随机挑选两张图片，并从每张图片上总共挑出 256 个 ground truth 作为 proposals（包括前景和后景），然后，再根据滑动窗口，挑选出大约 2400 个 Anchors。RPN 的 loss 函数包括两部分： \\[ L({p_i}, {t_i})=\\frac{1}{N_{cls}}\\sum_i L_{cls}(p_i, p_i^*)+\\lambda \\frac{1}{N_{reg}}\\sum_i p_i^*L_{reg}(t_i, t_i^*) \\] 其中， \\(L_{cls}\\) 是一个二分类函数， \\(L_{reg}\\) 则是 bounding box regression 函数（具体的跟 Fast RCNN 一样）， \\(p_i\\) 表示网络找到的 Anchor 区域中存在物体的概率（1 代表前景，0 代表背景），而 \\(t_i\\) 则是每个 Anchor 的矩形框位置和大小参数， \\(p_i^*\\) 和 \\(t_i^*\\) 则是 ground truth 对应的前后景概率以及窗口位置， 归一化项中，\\(N_{cls}\\) 取 batch 的大小（256），\\(N_{reg}\\) 取 Anchors 的数目（约为 2400）。 总的来说，RPN 可以用下面这幅图表示： RPN + Fast RCNN RPN 训练完成后，我们相当于得到一个神经网络版本的 Selective Search。那接下来的工作跟 Fast RCNN 就基本一样了，根据 RPN 找到的 proposal，Fast RCNN 在 feature map 上对这个 proposal 区域的特征进一步分析，判断是什么物体，以及对窗口位置进一步微调。 不过，这其中有很多可以优化的细节。比如，在 RPN 网络之前，我们需要先对图像做卷积操作，而这一部分操作和 Fast RCNN 是可以共享的。这里借用参考博文的一张图来介绍一下整个网络架构。 首先，原始图片会经过一个共享的卷积层，得到 feature map。之后，RPN 在这个 feature map 上按照之前的描述提取 proposal，而 Fast RCNN 部分会继续输入到它的卷积层中，得到更高层的 feature map，然后在这个 feature map 上，根据提取到的 proposal，按照 Fast RCNN 的流程判断物体，以及做 bounding box regression。 训练的时候，RPN 和 Fast RCNN 是分开交替进行训练的，这里面涉及到的 trick 较多，很多文章也都有介绍，我这里就不赘述了。预测部分则是一气呵成，不用再经过其他处理，完全实现了 end-to-end。 参考 边框回归(Bounding Box Regression)详解 RCNN, Fast-RCNN, Faster-RCNN的一些事 SPPNet-引入空间金字塔池化改进RCNN 一箭N雕：多任务深度学习实战 Faster R-CNN: Down the rabbit hole of modern object detection RCNN,Fast RCNN,Faster RCNN 总结","raw":null,"content":null,"categories":[{"name":"计算机视觉","slug":"计算机视觉","permalink":"https://jermmy.github.io/categories/计算机视觉/"}],"tags":[{"name":"计算机视觉","slug":"计算机视觉","permalink":"https://jermmy.github.io/tags/计算机视觉/"},{"name":"深度学习","slug":"深度学习","permalink":"https://jermmy.github.io/tags/深度学习/"},{"name":"论文","slug":"论文","permalink":"https://jermmy.github.io/tags/论文/"}]},{"title":"SVM小白教程（2）：拉格朗日对偶","slug":"2018-1-3-svm-tutorial-2","date":"2018-01-03T02:34:36.000Z","updated":"2018-02-15T14:49:12.000Z","comments":true,"path":"2018/01/03/2018-1-3-svm-tutorial-2/","link":"","permalink":"https://jermmy.github.io/2018/01/03/2018-1-3-svm-tutorial-2/","excerpt":"在上一篇文章中，我们推导出了 SVM 的目标函数： \\[\n\\underset{(\\mathbf{w},b)}{\\operatorname{min}} ||\\mathbf{w}|| \\\\  \\operatorname{s.t.} \\ y_i(\\mathbf{w}^T\\mathbf{x_i}+b) \\ge \\delta, \\ \\ i=1,...,m\n\\] 由于求解过程中，限制条件中的 \\(\\delta\\) 对结果不产生影响，所以简单起见我们把 \\(\\delta\\) 替换成 1。另外，为了之后求解的方便，我们会把原函数中的 \\(||\\mathbf{w}||\\) 换成 \\(\\frac{1}{2}||\\mathbf{w}||^2\\)，优化前者跟优化后者，最终的结果是一致的。这样，我们就得到 SVM 最常见的目标函数： \\[\n\\begin{align}\n&amp;\\underset{(\\mathbf{w},b)}{\\operatorname{min}} \\frac{1}{2}\\mathbf{w}^2 \\tag{1} \\\\  \\operatorname{s.t.} \\ y_i  (\\mathbf{w}^T &amp; \\mathbf{x_i}+b) \\ge 1,  \\ i=1,...,m  \\notag\n\\end{align}\n\\] 现在，我们要开始着手来解这个函数。","text":"在上一篇文章中，我们推导出了 SVM 的目标函数： \\[ \\underset{(\\mathbf{w},b)}{\\operatorname{min}} ||\\mathbf{w}|| \\\\ \\operatorname{s.t.} \\ y_i(\\mathbf{w}^T\\mathbf{x_i}+b) \\ge \\delta, \\ \\ i=1,...,m \\] 由于求解过程中，限制条件中的 \\(\\delta\\) 对结果不产生影响，所以简单起见我们把 \\(\\delta\\) 替换成 1。另外，为了之后求解的方便，我们会把原函数中的 \\(||\\mathbf{w}||\\) 换成 \\(\\frac{1}{2}||\\mathbf{w}||^2\\)，优化前者跟优化后者，最终的结果是一致的。这样，我们就得到 SVM 最常见的目标函数： \\[ \\begin{align} &amp;\\underset{(\\mathbf{w},b)}{\\operatorname{min}} \\frac{1}{2}\\mathbf{w}^2 \\tag{1} \\\\ \\operatorname{s.t.} \\ y_i (\\mathbf{w}^T &amp; \\mathbf{x_i}+b) \\ge 1, \\ i=1,...,m \\notag \\end{align} \\] 现在，我们要开始着手来解这个函数。 拉格朗日乘子法 对于（1）式中的问题，如果限制条件是等号的话，我们是可以直接用拉格朗日乘子法求解的。而为了应对不等号的情况，研究人员提出了 KKT 条件下的拉格朗日乘子法。所谓 KKT 条件，我们可以简单地把它当作拉格朗日乘子法的进阶版，只要原优化问题满足几个特定的条件，就可以仿照拉格朗日乘子法来求解问题。（关于 KKT 条件的具体内容，博主没有仔细研究过）。 而 SVM 原问题，刚好满足这些条件。因此可以直接套用拉格朗日乘子法的流程，首先列出拉格朗日函数： \\[ L(\\mathbf w, b, \\mathbf \\alpha)=\\frac{1}{2}||\\mathbf w||^2-\\sum_{i=1}^n\\alpha_i(y_i(\\mathbf w^T \\mathbf x_i + b)-1) \\\\ s.t. \\alpha_i \\ge 0 \\tag{2} \\] （注意，在 KKT 条件下，需要满足 \\(\\alpha_i \\ge 0\\)） 然后，令 \\(\\frac{\\partial L}{\\partial \\mathbf w}=0\\)，\\(\\frac{\\partial L}{\\partial b}=0\\)，可以得到方程组： \\[ \\frac{\\partial L}{\\partial \\mathbf w}=\\mathbf w-\\sum_{i=1}^n\\alpha_i y_i \\mathbf x_i=0 \\tag{3} \\] \\[ \\frac{\\partial L}{\\partial b}=\\sum_{i=1}^n \\alpha_i y_i=0 \\tag{4} \\] 在约束条件是等式的情况中，我们还会根据 \\(\\frac{\\partial L}{\\partial \\mathbf \\alpha}=0\\) 得到另外几组方程，然后可以解出 \\(\\mathbf w\\) 和 \\(b\\)。 不过，由于现在约束条件是不等式，所以 \\(\\frac{\\partial L}{\\partial \\mathbf \\alpha}\\) 得到的是一堆不等式： \\[ y_i (\\mathbf w \\mathbf x_i+b)-1 \\ge 0 \\ \\ i=1,2,\\dots,N \\] 这样是没法直接解出 \\(\\mathbf w\\) 和 \\(b\\) 的。 为了让方程组的形式更加简单，我们可以联立 (2)(3)(4) 把 \\(\\mathbf w\\) 和 \\(b\\) 消掉（后文有详细的推导过程）： \\[ L(\\mathbf w,b, \\mathbf \\alpha)=\\sum_{i=1}^n \\alpha_i - \\frac{1}{2}\\sum_{i=1}^n \\sum_{j=1}^n \\alpha_i \\alpha_j y_i y_j \\mathbf x_j^T \\mathbf x_i \\tag{5} \\] 到这一步，熟悉优化的同学应该发现，我们已经把原问题转化为拉格朗日对偶问题。换句话说，我们接下来要优化的问题就变为： \\[ \\underset{\\alpha}{\\operatorname{max}} \\sum_{i=1}^n \\alpha_i - \\frac{1}{2}\\sum_{i=1}^n \\sum_{j=1}^n \\alpha_i \\alpha_j y_i y_j \\mathbf x_j^T \\mathbf x_i \\tag{6} \\\\ s.t. \\ a_i \\ge 0, i=1,\\dots,m \\\\ \\sum_{i=1}^m\\alpha_i y_i=0 \\] 拉格朗日对偶问题 博主刚开始接触拉格朗日对偶的时候，一直搞不懂为什么一个最小化的问题可以转换为一个最大化问题。直到看了这篇博文后，才对它有了形象的理解。所以，下面我就根据这篇博文，谈谈我对拉格朗日对偶的理解。 对偶问题 先看一个简单的线性规划问题： \\[ \\underset{x,y}{\\operatorname{min}} x+3y \\\\ s.t. \\ x+y \\ge 2 \\\\ x,y \\ge 0 \\] 要求 \\(x+3y\\) 的最小值，可以通过变换目标函数来获得： \\[ x+y+2y \\ge 2 + 2 \\times 0 = 2 \\] 所以 \\(x+3y\\) 的最小值是 2。 如果将问题泛化： \\[ \\underset{x,y}{\\operatorname{min}} px+qy \\tag{7} \\\\ s.t. \\ x+y \\ge 2 \\\\ x,y \\ge 0 \\] 同样地，通过这种拼凑的方法，我们可以将问题变换为： \\[ \\begin{align} a(x+y) &amp;\\ge 2a \\notag \\\\ bx &amp;\\ge 0 \\notag\\\\ cy &amp;\\ge 0 \\notag\\\\ a(x+y)+bx+cy&amp;=(a+b)x+(a+c)y \\ge 2a \\tag{8} \\end{align} \\] 其中，\\(a,b,c &gt; 0\\)。 （8）式对 \\(\\forall a,b,c &gt; 0\\) 均成立。不管 \\(a+b\\)、\\(a+c\\) 的值是多少，\\((a+b)x+(a+c)y\\) 的最小值都是 \\(2a\\)。因此，我们可以加上约束：\\(a+b=p\\)、\\(a+c=q\\)，这样就得到 \\(px+qy\\) 的最小值为 \\(2a\\)。需要注意的是，\\(2a\\) 是 \\(px+qy\\) 的下界，即这个最小值对 \\(\\forall a\\) 都要成立，所以，需要在约束条件内求出 \\(a\\) 的最大值，才能得出 \\(px+qy\\) 的最小值。 这样一来，问题就转换为： \\[ \\begin{eqnarray} \\underset{a,b,c} {\\operatorname {max}}\\ {2a} \\tag{9} \\\\ s.t. \\ p=a+b \\notag\\\\ q = a+c \\notag\\\\ a,b,c \\ge 0 \\notag \\end{eqnarray} \\] （9）式就是（7）式的对偶形式。 对偶和对称有异曲同工之妙。所谓对偶，就是把原来的最小化问题（7）转变为最大化问题（9）。这种转化对最终结果没有影响，但却使问题更加简单（问题（9）中的限制条件都是等号，而不等号只是针对单个变量 \\(a,b,c\\)，因此可以直接套用拉格朗日乘子法）。 另外，对偶分强对偶和弱对偶两种。借用上面的例子，强对偶指的是 \\(px+qy\\) 的最小值就等于 \\(2a\\) 的最大值，而弱对偶则说明，\\(px+qy\\) 的最小值大于 \\(2a\\) 的最大值。SVM 属于强对偶问题。 线性规划问题的对偶问题 现在，我们把问题再上升到一般的线性规划问题： \\[ \\begin{eqnarray} \\underset{x \\in \\mathbb{R}^n} {\\operatorname{min}} c^Tx \\tag{10} \\\\ s.t. \\ Ax=b \\notag \\\\ Gx \\le h \\notag \\end{eqnarray} \\] 用同样的方法进行转换： \\[ \\begin{align} -u^TAx &amp; =-b^Tu \\notag \\\\ -v^TGx &amp; \\ge -h^Tv \\notag \\\\ (-u^TA-v^TG)x &amp; \\ge -b^Tu-h^Tu \\notag \\end{align} \\] 这样，可以得到该线性问题的对偶形式： \\[ \\underset{u \\in \\mathbb{R}^m,v \\in \\mathbb{R}^r} {\\operatorname{max}} -b^Tu-h^Tu \\tag{11} \\\\ s.t. \\ c= -A^Tu-G^Tv \\\\ v &gt; \\ 0 \\] 这种「拼凑」的转换方法可以用拉格朗日函数作为通用的方法解决。定义原函数如下： \\[ f(x)=c^Tx \\] 引入拉格朗日函数： \\[ L(x,u,v)=f(x)+u^T(Ax-b)+v^T(Gx-h) \\] 其中，\\(v&gt;0\\)。 由于 \\(Ax-b = 0\\)，\\(Gx-h \\le 0\\)，所以必有 \\(f(x) \\ge L(x,u,v)\\)，换句话说，\\(\\underset{x}{\\operatorname{min}}{f(x)} \\ge \\underset{x}{\\operatorname{min}}{L(x,u,v)}\\)。因此，求 \\(f(x)\\) 的最小值就转换为求 \\(L(x,u,v)\\) 的最小值。 \\[ \\begin{align} L(x,u,v)&amp;=(c^T+u^TA+v^TG)x-u^Tb-v^Th \\notag \\end{align} \\] \\(\\underset{x}{\\operatorname{min}}{L(x,u,v)}\\) 在 \\(x\\) 没有任何限制的前提下，是不存在最小值。因此，我们要加上约束条件：\\(c^T+u^TA+v^TG=0\\)，这样，\\(\\underset{x}{\\operatorname{min}}{L(x,u,v)}=-u^Tb-v^Th\\)。如此一来，我们又把原问题转换到（11）中的对偶问题上了。 二次规划问题的对偶问题 由于 SVM 的目标函数是一个二次规划问题（带有平方项），因此我们最后再来看一个二次规划的优化问题。 假设有如下二次规划问题： \\[ \\begin{equation} \\underset{x}{\\operatorname{min}}\\ {\\frac{1}{2}x^TQx+c^Tx} \\notag \\\\ s.t. \\ Ax=b \\notag \\\\ x \\ge 0 \\end{equation} \\] 其中，\\(Q&gt;0\\)（保证有最小值）。 按照线性规划问题的思路，构造拉格朗日函数（注意，构造出来的 \\(L(x,u,v)\\) 必须小于等于原函数 \\(\\frac{1}{2}x^TQx+c^Tx\\)）： \\[ \\begin{equation} L(x,u,v)=\\frac{1}{2}x^TQx+c^Tx-u^Tx+v^T(Ax-b) \\notag \\\\ =\\frac{1}{2}x^TQx+(c+v^TA-u)^Tx+v^Tb \\notag \\end{equation} \\] 由于二次函数 \\(ax^2+bx+c\\) 的最小值在 \\(x=-\\frac{b}{2a}\\) 处取得，因此可以求得函数 \\(L\\) 的最小值： \\[ \\begin{equation} \\underset{x}{\\operatorname{min}} L(x,u,v)=-\\frac{1}{2}(c-u+A^Tv)^TQ^{-1}(c-u+A^Tv)-b^Tv \\end{equation} \\] 这样一来，我们就求得原问题的拉格朗日对偶问题： \\[ \\begin{equation} \\underset{u,v}{\\operatorname{max}}-\\frac{1}{2}(c-u+A^Tv)^TQ^{-1}(c-u+A^Tv)-b^Tv \\notag \\\\ s.t. \\ u&gt;0 \\end{equation} \\] 拉格朗日对偶问题 现在总结一下拉格朗日对偶问题的基本「套路」。 假设原问题为： \\[ \\begin{equation} \\underset{x}{\\operatorname{min}}f(x) \\notag \\\\ s.t. \\ h_i(x) \\le 0, i=1,\\dots,m \\notag \\\\ l_i(x)=0, j=1,\\dots,r \\notag \\end{equation} \\] 则拉格朗日原始问题为： \\[ L(x,u,v)=f(x)+\\sum_{i=1}^m {u_i h_i(x)}+\\sum_{j=1}^r v_j l_j(x) \\] 其中，\\(u_i&gt;0\\)。 之后，我们求出 \\(\\underset{x}{\\operatorname{min}}L(x,u,v)=g(u,v)\\)，将问题转换为对偶问题： \\[ \\begin{equation} \\underset{u,v}{\\operatorname{max}} \\ g(u,v) \\notag \\\\ s.t. \\ u \\ge 0 \\notag \\end{equation} \\] 教材上通常把拉格朗日原始问题表示为 \\(\\underset{x}{\\operatorname{min}}\\underset{u,v}{\\operatorname{max}}L(x,u,v)\\)，而对偶问题表示成 \\(\\underset{u,v}{\\operatorname{max}}\\underset{x}{\\operatorname{min}}L(x,u,v)\\)。它们之间存在如下关系： \\[ \\underset{x}{\\operatorname{min}}\\underset{u,v}{\\operatorname{max}}L(x,u,v) \\ge \\underset{u,v}{\\operatorname{max}}\\underset{x}{\\operatorname{min}}L(x,u,v) \\] SVM的对偶问题 现在看回 SVM。我们将约束条件表述成 \\(y_i (\\mathbf{w}^T\\mathbf{x_i}+b) -1 \\ge 0, \\ i=1, \\dots ,m\\)，然后，按照上面的「套路」，表示出拉格朗日原始问题： \\[ \\begin{align} L(\\mathbf{w},b,\\alpha)= &amp; \\frac{1}{2}\\mathbf{w}^2-\\sum_{i=1}^m{\\alpha_i}[y_i(\\mathbf{w}^T\\mathbf{x_i}+b)-1] \\tag{12} \\\\ s.t. \\ \\alpha_i \\ge &amp;\\ 0, \\ i=1, \\dots, m \\notag \\end{align} \\] 下面要求出 \\(L(\\mathbf{w},b,\\alpha)\\) 关于 \\(\\mathbf{w}\\) 和 \\(b\\) 的最小值，这里可以直接通过偏导求得： \\[ \\nabla_\\mathbf{w} L=\\mathbf{w}-\\sum_{i=1}^m \\alpha_iy_i \\mathbf{x}_i=0 \\tag{13} \\] \\[ \\frac{\\partial L}{\\partial b}=-\\sum_{i=1}^m\\alpha_i y_i=0 \\tag{14} \\] 由（13）式解得： \\[ \\begin{align} \\mathbf{w}=\\sum_{i=1}^m \\alpha_i y_i \\mathbf{x}_i \\tag{15} \\end{align} \\] （15）式代入（12）式得到： \\[ W(\\alpha,b)=\\sum_{i=1}^m\\alpha_i-\\frac{1}{2}\\sum_{i=1}^m\\sum_{j=1}^m\\alpha_i \\alpha_j y_i y_j \\mathbf{x}_i \\mathbf{x}_j-b\\sum_{i=1}^m \\alpha_i y_i \\tag{16} \\] 而（14）式已经表明：\\(\\sum_{i=1}^m\\alpha_i y_i=0\\)，所以（16）式化简为： \\[ W(\\alpha)=\\sum_{i=1}^m\\alpha_i-\\frac{1}{2}\\sum_{i=1}^m\\sum_{j=1}^m\\alpha_i \\alpha_j y_i y_j \\mathbf{x}_i \\mathbf{x}_j \\tag{17} \\] （17）式就是最终版本的对偶形式了（上文的 (6) 式其实也是这样推出来的）。自此，我们得出 SVM 的拉格朗日对偶问题： \\[ \\underset{\\alpha}{\\operatorname{max}} W(\\alpha) \\\\ s.t. \\ a_i \\ge 0, i=1,\\dots,m \\\\ \\sum_{i=1}^m\\alpha_i y_i=0 \\] 解出 \\(\\mathbf \\alpha\\) 后，就可以根据 (15) 式解出 \\(\\mathbf w\\)，然后根据超平面的间隔求出 \\(b\\)。 当然，这个对偶形式的优化问题依然不是那么容易解的，研究人员提出了一种 SMO 算法，可以快速地求解 \\(\\mathbf \\alpha\\)。不过算法的具体内容，本文就不继续展开了。 参考 凸优化-对偶问题 拉格朗日乘子法 简易解说拉格朗日对偶（Lagrange duality） 支持向量机SVM（二） 第7课 支持向量机，为什么能理解SVM的人凤毛麟角？","raw":null,"content":null,"categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://jermmy.github.io/categories/机器学习/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"https://jermmy.github.io/tags/机器学习/"},{"name":"优化理论","slug":"优化理论","permalink":"https://jermmy.github.io/tags/优化理论/"}]},{"title":"SVM小白教程（1）：目标函数","slug":"2017-12-23-svm-tutorial-1","date":"2017-12-23T11:56:35.000Z","updated":"2018-01-07T12:45:55.000Z","comments":true,"path":"2017/12/23/2017-12-23-svm-tutorial-1/","link":"","permalink":"https://jermmy.github.io/2017/12/23/2017-12-23-svm-tutorial-1/","excerpt":"关于 SVM（支持向量机），网上教程实在太多了，但真正能把内容讲清楚的少之又少。这段时间在网上看到一个老外的 svm 教程，几乎是我看过的所有教程中最好的。这里打算通过几篇文章，把我对教程的理解记录成中文。另外，上面这篇教程的作者提供了一本免费的电子书，内容跟他的博客是一致的，为了方便读者，我把它上传到自己的博客中。\n这篇文章主要想讲清楚 SVM 的目标函数，而关于一些数学上的优化问题，则放在之后的文章。\n\n\n","text":"关于 SVM（支持向量机），网上教程实在太多了，但真正能把内容讲清楚的少之又少。这段时间在网上看到一个老外的 svm 教程，几乎是我看过的所有教程中最好的。这里打算通过几篇文章，把我对教程的理解记录成中文。另外，上面这篇教程的作者提供了一本免费的电子书，内容跟他的博客是一致的，为了方便读者，我把它上传到自己的博客中。 这篇文章主要想讲清楚 SVM 的目标函数，而关于一些数学上的优化问题，则放在之后的文章。 什么是 SVM SVM 的全称是 Support Vector Machine，中文名支持向量机。 关于 SVM 是什么这个问题，知乎上有一篇通俗易懂的文章，说到底，SVM 的提出主要是为了解决二分类的问题。下面会从最简单的数学入手，一步步揭开 SVM 的面纱。 超平面 什么是超平面 在正式开讲之前，需要先讲一下超平面（hyperplane）的概念，这是 SVM 中一个相当重要的概念。 在初中的时候，我们就知道 \\(ax+b-y=0\\) 表示的是一条直线。在机器学习里面，为了方便用向量的形式来表示，我们一般用 \\(x_1\\) 来代替 \\(x\\)，\\(x_2\\) 来代替 \\(y\\)，这样，直线就表示成了 \\(\\mathbf{w}^T\\mathbf{x}+b=0\\)，其中 \\(\\mathbf{w}=\\begin{bmatrix} a &amp; -1 \\end{bmatrix}^T\\)，\\(\\mathbf{x}=\\begin{bmatrix} x_1 &amp; x_2 \\end{bmatrix}^T\\)。 如果我们把目光投到三维：\\(ax_1+bx_2+cx_3+d=0\\)，那么原来的直线就变成了一个平面。如果继续将维度升高到四维、五维。。。这时，平面就变成了高维空间的超平面。机器学习中的问题基本都是在高维空间处理的。 不过，由于超平面没法用画图表示，因此本文会使用二维的例子来介绍。 如果你看了上面知乎那篇文章，你就会知道，SVM 正是借助这个超平面来划分数据的。 关于这个超平面，我们要知道三点： 超平面也是由一系列点组成的。在线性代数中通常将点称为向量。如果点 \\(\\mathbf{x}\\) 在超平面上，则满足 \\(\\mathbf{w}^T\\mathbf{x}+b=0\\)。由于这个超平面把数据分为两类，因此这些点又被称为支持向量。 假设超平面两侧各有一点 \\(\\mathbf{x_1}\\) 和 \\(\\mathbf{x_2}\\)，则满足 \\(\\mathbf{w}^T\\mathbf{x_1}+b&gt;0\\)，\\(\\mathbf{w}^T\\mathbf{x_2}+b&lt;0\\)。在二维中这一点很明显。 \\(\\mathbf{w}\\) 与超平面垂直。这一点是根据超平面的定义得来的，可以参看这个数学讲义。当然，如果实在无法理解，可以令 \\(b=0\\)，这样超平面就变成 \\(\\mathbf{w}^T\\mathbf{x}=0\\)，两向量内积为 0，证明 \\(\\mathbf{w} \\perp \\mathbf{x}\\)。在 SVM 中，正样本的标签通常记为 1，负样本记为 -1。因此，为了保持符号上的一致性，规定 \\(\\mathbf{w}\\) 的方向指向正样本的一侧。这样，如果 \\(\\mathbf{x_1}\\) 是一个正样本，那么 \\(\\mathbf{w}^T\\mathbf{x_1}+b&gt;0\\)，否则 \\(\\mathbf{w}^T\\mathbf{x_1}+b&lt;0\\)。 间距 margin 间距是 SVM 中另一个核心概念。间距指的就是点和超平面之间的距离。当样本点很多时，我们取样本点和超平面之间的最小距离作为间距。 SVM 的目标 SVM 的目的其实就是找一个区分数据最合适的超平面，超平面一侧是正样本，另一侧是负样本。我们应该能隐约感觉到：最合适的平面就在正负样本「最中间」的位置。换句话说，就是找一个间距最大的超平面。这样找到的超平面也将在正负样本的最中间，因为如果超平面离任何样本太近，间距都会变小，因此为了保证间距最大，就必须与所有正负样本都足够的远。 如何找到最合适的超平面 假设下图中的 A～G 表示样本点，橙色线是超平面，两条蓝线表示与超平面平行的面，它们划定了超平面的间距（注意右侧的蓝线穿过了离超平面最近的 A、B 两点）。 虽然这个超平面能把正负样本点分开，但显然不是最优的超平面，因为我们可以找到一个新的超平面，使间距更大（由 \\(\\frac{M_1}{2}\\) 扩大到 \\(\\frac{M_2}{2}\\)）。 到这里，我们可以发现，超平面是否合适，跟间距大小息息相关。因此，寻找最优的超平面，就等价于找到最大的间距。 寻找最大间距 下面就来讨论一下怎么找到最大的间距。 假设我们有一个样本集 \\(D=\\{(\\mathbf{x_i},y_i)\\ \\big |\\ \\mathbf{x_i} \\in R^p, y_i \\in \\{-1,1\\}\\}\\)，\\(y_i\\) 表示样本标签，正样本取 1，负样本取 -1。 为了计算超平面 \\(\\mathbf{w}^T\\mathbf{x}+b=0\\) 的间距，我们可以仿照上图中的蓝线，引入两个超平面： \\(\\mathbf{w}^T\\mathbf{x}+b=\\delta\\) 和 \\(\\mathbf{w}^T\\mathbf{x}+b=-\\delta\\)（\\(\\delta\\) 取正数）。注意，这两个超平面之间不能有任何数据点。因此，他们要满足一个限制条件：对于正样本（\\(y_i=1\\)）而言，\\(\\mathbf{w}^T\\mathbf{x}+b\\ge \\delta\\)，对于负样本（\\(y_i=-1\\)），\\(\\mathbf{w}^T\\mathbf{x}+b\\le -\\delta\\)。这样一来，求原超平面的间距就转换为求这两个超平面之间的距离。利用标签的正负号，我们可以把两个超平面的限制条件统一为： \\[ y_i(\\mathbf{w}^T\\mathbf{x_i}+b) \\ge \\delta \\tag{1} \\] 接下来要考虑如何计算这两个超平面之间的距离。 在高中阶段，我们就学过如何计算两条平行直线之间的距离，这里完全可以把二维的公式拓展到高维。不过，这里我们还是从向量的角度出发，看看如何计算两个超平面之间的距离。 假设这两个超平面分别为 \\(H_0: \\mathbf{w}^T\\mathbf{x}+b=-\\delta\\) 和 \\(H_1: \\mathbf{w}^T\\mathbf{x}+b=\\delta\\)，\\(x_0\\) 是 \\(H_0\\) 上一点，两个平面之间的距离是 \\(m\\)。 为了计算 \\(m\\)，需要找一个跟 \\(m\\) 相关的表达式。假设 \\(H_1\\) 上有一点 \\(x_1\\)，使得向量 \\(\\overline {x_0x_1} \\perp H_1\\)，则 \\(\\mathbf{x_1}=\\mathbf{x_0}+\\overline {x_0x_1}\\)。想要求 \\(\\overline {x_0x_1}\\)，我们需要确定它的方向和长度。在之前介绍超平面时，我们已经知道，\\(\\mathbf{w} \\perp H_1\\)，所以这个向量的方向应该和 \\(\\mathbf{w}\\) 相同，而它的长度就是我们要求的 \\(m\\)，所以 \\(\\overline {x_0x_1}=m\\frac{\\mathbf{w}}{||\\mathbf{w}||}\\)，既而 \\(\\mathbf{x_1}=\\mathbf{x_0}+m\\frac{\\mathbf{w}}{||\\mathbf{w}||}\\)。 现在把 \\(\\mathbf{x_1}\\) 代入 \\(H_1 (\\mathbf{w}^T\\mathbf{x}+b=\\delta)\\) 中： \\[ \\begin{align} \\mathbf{w}^T\\mathbf{x_1}+b=&amp;\\mathbf{w}^T(\\mathbf{x_0}+m\\frac{\\mathbf{w}}{||\\mathbf{w}||})+b \\notag \\\\ =&amp;\\mathbf{w}^T\\mathbf{x_0}+m\\frac{\\mathbf{w}^T\\mathbf{w}}{||\\mathbf{w}||}+b \\tag{2} \\\\ =&amp;\\delta \\notag \\end{align} \\] 由于 \\(\\mathbf{x_0}\\) 是 \\(H_0\\) 上一点，所以 \\(\\mathbf{w}^T\\mathbf{x_0}+b=-\\delta\\)，代入（2）式： \\[ \\begin{align} \\mathbf{w}^T\\mathbf{x_1}+b=&amp;-\\delta+m\\frac{\\mathbf{w}^T\\mathbf{w}}{||\\mathbf{w}||} \\notag \\\\ =&amp;-\\delta + m||\\mathbf{w}|| \\notag \\\\ =&amp;\\delta \\notag \\end{align} \\] 这样我们就得到 \\(m\\) 的表达式： \\[ m=\\frac{2\\delta}{||\\mathbf{w}||} \\tag{3} \\] 考虑到 \\(\\delta\\) 是一个正数，因此，要使 \\(m\\) 最大，就必须让 \\(||\\mathbf{w}||\\) 最小。再考虑到两个超平面的限制条件（1），我们就可以得到如下 SVM 的目标函数。 目标函数 综合考虑 (2) (3)，我们得到 SVM 最终的目标函数： \\[ \\underset{(\\mathbf{w},b)}{\\operatorname{min}} ||\\mathbf{w}|| \\ \\ \\ \\ \\operatorname{s.t.} \\ y_i(\\mathbf{w}^T\\mathbf{x_i}+b) \\ge \\delta, \\ \\ i=1,...,m \\] 找出使这个函数最小的 \\(\\mathbf{w}\\) 和 \\(b\\)，就找到了最合适的超平面。注意，由于我们有 \\(m\\) 个样本，所以总共有 \\(m\\) 个限制条件。 参考 支持向量机(SVM)是什么意思？ - 简之的回答 - 知乎 Part 2: How to compute the margin? Part 3: How to find the optimal hyperplane? Paul’s Online Math Notes","raw":null,"content":null,"categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://jermmy.github.io/categories/机器学习/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"https://jermmy.github.io/tags/机器学习/"},{"name":"优化理论","slug":"优化理论","permalink":"https://jermmy.github.io/tags/优化理论/"}]},{"title":"论文笔记：Image Smoothing via L0 Gradient Minimization","slug":"2017-12-22-paper-notes-image-smoothing-via-l0-gradient-minimization","date":"2017-12-22T02:05:17.000Z","updated":"2018-01-11T03:29:54.000Z","comments":true,"path":"2017/12/22/2017-12-22-paper-notes-image-smoothing-via-l0-gradient-minimization/","link":"","permalink":"https://jermmy.github.io/2017/12/22/2017-12-22-paper-notes-image-smoothing-via-l0-gradient-minimization/","excerpt":"今天要分享的这篇论文是我个人最喜欢的论文之一，它的思想简单、巧妙，而且效果还相当不错。这篇论文借助数学上的 \\(L_0\\) 范数工具对图像进行平滑，同时保留重要的边缘特征，可以实现类似水彩画的效果（见下图）。\n另外这篇论文的作者徐立也是一个相当高产的研究员。\n\n\n","text":"今天要分享的这篇论文是我个人最喜欢的论文之一，它的思想简单、巧妙，而且效果还相当不错。这篇论文借助数学上的 \\(L_0\\) 范数工具对图像进行平滑，同时保留重要的边缘特征，可以实现类似水彩画的效果（见下图）。 另外这篇论文的作者徐立也是一个相当高产的研究员。 论文的目的 所谓图像平滑，就是突出图像中的低频成分，抑制高频成分，减小突变的梯度。大部分情况下，这么做的目的是为了去除图像中的噪声，因为噪声一般就是一些孤立的像素点，是像素变化比较大的区域。在传统的图像处理中，大部分操作都是用一些具有平滑性质的卷积核对图像进行模糊处理，最常用的如：高斯模糊、均值滤波等等。这些方法都有一个缺陷，就是在模糊噪声的同时，也模糊了边缘。当然之后也有一些改进的方法，如：双边滤波等，这些方法都在边缘保持上进行了很多改进，但多少还是会损失边缘的信息。本文的方法完全不同于以往的这些算法，它从图像梯度的角度出发，在平滑掉大部分细小的噪声的同时，又能最大限度的保持重要的边缘信息。 论文主要思想 一维图像信号 在正式讲思想之前，我们先回忆一下图像平滑的目标是什么。 为了简单起见，我们先从一维出发，把图像当作一维的信号。那么，图像平滑就是要把那些比较小的类似「褶皱」的地方抹平，而把那些大的「褶皱」，或者更准确地说，变化很大的梯度（边缘）保留下来。上面这幅图展示的是不同算法的平滑效果。可以发现，之前的算法在平滑掉那些坑坑洼洼的「褶皱」的同时，也把可能把属于真正边缘的大的梯度给模糊了（细节放大图如下） 为了避免这种问题，论文提出一种基于 \\(L_0\\) 范数的能量最小化方法。 所谓 \\(L_0\\) 范数，指的就是向量中非 0 元素的个数。论文中借用这个概念，提出一个图像梯度数量的计算公式： \\[ c(f)=\\#\\{p\\ \\big |\\ |f_p-f_{p+1}|\\neq 0 \\} \\] 公式中，\\(f\\) 表示我们平滑后的图像，\\(\\#\\) 表示集合中元素 \\(p\\) 的个数，而 \\(p\\) 则表示像素位置，因此 \\(f_p\\) 其实就代表图像 \\(f\\) 在 \\(p\\) 这个位置的像素值。 因此这个公式其实就是在计算：满足 \\(|f_p-f_{p+1}| \\neq 0\\) 的像素数量，这个不等式正好就是 \\(L_0\\) 范数。它的现实意义就是计算图片信号中梯度的数量。 有了这个公式后，论文抛出它最核心的目标函数： \\[ \\underset{f} {\\operatorname {min}} \\sum_{p}(f_p-g_p)^2 \\ \\ \\ \\ \\operatorname{s.t.} \\ c(f)=k \\] 公式中的 \\(g\\) 表示原图像，\\(f\\) 表示平滑后的图像，\\(c(f)\\) 就是上面提到的计算梯度数量的公式，它表示 \\(f\\) 中的梯度数量应该为 \\(k\\) 个。 这个公式表示的是图像 \\(f\\) 中每个像素 \\(f_p\\) 和原图 \\(g\\) 中每个像素 \\(g_p\\) 之间的平方差之和。 最小化这个目标函数，其实就是要最小化 \\(f\\) 和 \\(g\\) 之间的像素差。如果没有 \\(c(f)\\) 这个限制，那么最终的优化结果就是 \\(f=g\\)。但加上 \\(c(f)\\) 限制后，这个目标函数在尽可能减少两个信号之间的能量差的同时，又要让 \\(f\\) 中的梯度数量满足 \\(k\\) 个。换句话说，它要尽可能让 \\(f\\) 和 \\(g\\) 相似，同时又抹平 \\(f\\) 中的梯度。因此，最后的优化结果只能是保留住 \\(f\\) 中那些梯度比较大的边缘，而平滑掉那些梯度比较小的「褶皱」。 \\(c(f)\\) 这个限制最大的作用就是防止 \\(f\\) 出现对边缘的模糊。如果仔细观察上面那张边缘模糊的细节图，你就会发现，造成模糊的原因是我们把原来很「抖」的梯度变「缓」了，而缓的梯度其实是由很多小梯度组成的。\\(c(f)\\) 的限制正是为了减少这种梯度的数量。因此，为了满足 \\(c(f)\\)，目标函数会让 \\(f\\) 中的梯度倾向于更「抖」。 在最小化能量差和减少梯度数量这两个约束的共同博弈下，最终得到了下面的这种很平滑、同时边缘很尖锐的结果： 这种相互制约的想法实在是简单而又精彩！ 不过，实际应用中存在一个问题，就是 \\(k\\) 的变化范围很大，是很难选择的。为了控制 \\(k\\) 的选择范围，论文把 \\(c(f)=k\\) 这个约束也加入到目标函数中： \\[ \\underset{f}{\\operatorname{min}} \\{ \\sum_p{(f_p-g_p)^2+\\lambda c(f)} \\} \\] 现在 \\(\\lambda\\) 代替 \\(k\\) 作为可以调节的参数。\\(\\lambda\\) 越大，目标函数对 \\(c(f)\\) 的抑制就越大，梯度数量就越少（即边缘越少），反之，梯度数量越大。 二维图像信号 一维信号的约束可以很容易引申到二维信号： \\[ C(S)=\\#\\{p\\ \\big | \\ |\\partial_x S_p|+|\\partial_y S_p| \\neq 0 \\} \\] \\(S\\) 类似上面提到的 \\(f\\)，在这里它表示处理后的二维图像，\\(\\partial_x S_p\\) 和 \\(\\partial_y S_p\\) 分别代表在 \\(x\\) 方向和 \\(y\\) 方向计算 \\(L_0\\) 范数。 然后，我们可以用同样的方法得到目标函数： \\[ \\underset{S}{\\operatorname{min}} \\{ \\sum_p{(S_p-I_p)^2+\\lambda C(S)} \\} \\] 公式中 \\(I\\) 表示原图，其他的类比一维信号的公式。 ###优化方法 希望有生之年看懂补上。。。 实验结果 上图（e）是论文的方法和其他图像平滑方法的对比，可以看出，这种基于 \\(L_0\\) 平滑的方法不仅平滑的效果好，而且几乎是原封不动地保留了色块之间的边缘信息。而其他方法在色块（尤其是红色）上的平滑力度不如 \\(L_0\\) 平滑，且边缘也模糊了不少。 另外，论文的平滑方法在边缘提取上效果也相当的好，下图中，左边（e）是直接对原图用 Canny 提取边缘的结果，右边是先用 \\(L_0\\) smoothing 进行平滑，再提取边缘的结果，可以看到，平滑后提取的边缘中抹去了很多不重要的细节，而主要的边缘信息都被提取出来。 除此之外，另一个很重要的用途是图像失量化。图像失量化要求图像中的色块数量尽可能少，而这一点正中 \\(L_0\\) smoothing 下怀。下图中的输入图像具有很多噪声细节，而这些细节用一般的平滑方向是很难根除的，但 \\(L_0\\) smoothing 由于限制了梯度数量，因此可以产生很「光滑」的色块区域，非常利于失量化操作。 参考 Image Smoothing via L0 Gradient Minimization L0范数图像平滑 机器学习中的范数规则化之（一）L0、L1与L2范数","raw":null,"content":null,"categories":[{"name":"图像处理","slug":"图像处理","permalink":"https://jermmy.github.io/categories/图像处理/"}],"tags":[{"name":"图像处理","slug":"图像处理","permalink":"https://jermmy.github.io/tags/图像处理/"},{"name":"NPR","slug":"NPR","permalink":"https://jermmy.github.io/tags/NPR/"},{"name":"论文","slug":"论文","permalink":"https://jermmy.github.io/tags/论文/"}]},{"title":"CNN的反向传播","slug":"2017-12-16-cnn-back-propagation","date":"2017-12-16T12:41:34.000Z","updated":"2018-01-10T05:00:33.000Z","comments":true,"path":"2017/12/16/2017-12-16-cnn-back-propagation/","link":"","permalink":"https://jermmy.github.io/2017/12/16/2017-12-16-cnn-back-propagation/","excerpt":"在一般的全联接神经网络中，我们通过反向传播算法计算参数的导数。BP 算法本质上可以认为是链式法则在矩阵求导上的运用。但 CNN 中的卷积操作则不再是全联接的形式，因此 CNN 的 BP 算法需要在原始的算法上稍作修改。这篇文章主要讲一下 BP 算法在卷积层和 pooling 层上的应用。","text":"在一般的全联接神经网络中，我们通过反向传播算法计算参数的导数。BP 算法本质上可以认为是链式法则在矩阵求导上的运用。但 CNN 中的卷积操作则不再是全联接的形式，因此 CNN 的 BP 算法需要在原始的算法上稍作修改。这篇文章主要讲一下 BP 算法在卷积层和 pooling 层上的应用。 原始的 BP 算法 首先，用两个例子回顾一下原始的 BP 算法。（不熟悉 BP 可以参考How the backpropagation algorithm works，不介意的话可以看我的读书笔记） 最简单的例子 先看一个最简单的例子（偷个懒，搬个手绘图～囧～）： 上图中，\\(a^l\\) 表示第 \\(l\\) 层的输出（\\(a^0\\) 就是网络最开始的输入），网络的激活函数假设都是 \\(\\sigma()\\)，\\(w^l\\) 和 \\(b^l\\) 表示第 \\(l\\) 层的参数，\\(C\\) 表示 \\(loss\\ function\\)，\\(\\delta^l\\) 表示第 \\(l\\) 层的误差，\\(z^l\\) 是第 \\(l\\) 层神经元的输入，即 \\(z^l=w^l a^{l-1}+b^l\\)，\\(a^l=\\sigma(z^l)\\)。 接下来要用 BP 算法求参数的导数 \\(\\frac{\\partial C}{\\partial w}\\) 和 \\(\\frac{\\partial C}{\\partial b}\\)。 \\[ \\delta^2=\\frac{\\partial C}{\\partial z^2}=\\frac{\\partial C}{\\partial a^2}\\frac{\\partial a^2}{\\partial z^2}=\\frac{\\partial C}{\\partial a^2}\\sigma&#39;(z^2) \\] \\[ \\delta^1=\\frac{\\partial C}{\\partial z^1}=\\delta^2\\frac{\\partial z^2}{\\partial a^1}\\frac{\\partial a^1}{\\partial z^1}=\\delta^2 w^2\\sigma&#39;(z^1) \\] 算出这两个误差项后，就可以直接求出导数了： \\[ \\frac{\\partial C}{\\partial b^2}=\\frac{\\partial C}{\\partial a^2}\\frac{\\partial a^2}{\\partial z^2}\\frac{\\partial z^2}{\\partial b^2}=\\delta^2 \\] \\[ \\frac{\\partial C}{\\partial w^2}=\\frac{\\partial C}{\\partial a^2}\\frac{\\partial a^2}{\\partial z^2}\\frac{\\partial z^2}{\\partial w^2}=\\delta^2 a^1 \\] \\(\\frac{\\partial C}{\\partial b^1}\\) 和 \\(\\frac{\\partial C}{\\partial w^1}\\) 的求法是一样的，这里不在赘述。 次简单的例子 接下来稍微把网络变复杂一点： 符号的标记和上一个例子是一样的。要注意的是，这里的 \\(W^l\\) 不再是一个数，而变成一个权重矩阵，\\(W_{kj}^l\\) 表示第 \\(l-1\\) 层的第 \\(j\\) 个神经元到第 \\(l\\) 层的第 \\(k\\) 个神经元的权值，如下图所示： 首先，还是要先求出网络的误差 \\(\\mathbf{\\delta}\\)。 \\[ \\delta_1^2=\\frac{\\partial C}{\\partial z_1^2}=\\frac{\\partial C}{\\partial a_1^2}\\sigma&#39;(z_1^2) \\] \\[ \\delta_2^2=\\frac{\\partial C}{\\partial z_2^2}=\\frac{\\partial C}{\\partial a_2^2}\\sigma&#39;(z_2^2) \\] 由此得到： \\[ \\delta^2=\\begin{bmatrix} \\delta_1^2 \\\\ \\delta_2^2 \\end{bmatrix}=\\begin{bmatrix} \\frac{\\partial C}{\\partial a_1^2} \\\\ \\frac{\\partial C}{\\partial a_2^2} \\end{bmatrix} \\odot \\begin{bmatrix} \\sigma&#39;(z_1^2) \\\\ \\sigma&#39;(z_2^2) \\end{bmatrix} \\] \\(\\odot\\) 表示 elementwise 运算。 接着要根据 \\(\\delta^2\\) 计算前一层的误差 \\(\\delta^1\\)。 \\[ \\begin{align} \\delta_1^1=\\frac{\\partial C}{\\partial z_1^1}=&amp;\\frac{\\partial C}{\\partial a_1^2}\\sigma&#39;(z_1^2)\\frac{\\partial z_1^2}{\\partial a_1^1}\\frac{\\partial a_1^1}{\\partial z_1^1}+ \\notag \\\\ &amp;\\frac{\\partial C}{\\partial a_2^2}\\sigma&#39;(z_2^2)\\frac{\\partial z_2^2}{\\partial a_1^1}\\frac{\\partial a_1^1}{\\partial z_1^1} \\notag \\\\ =&amp;\\frac{\\partial C}{\\partial a_1^2}\\sigma&#39;(z_1^2)W_{11}^2\\sigma&#39;(z_1^1)+\\tag{1} \\\\ &amp;\\frac{\\partial C}{\\partial a_2^2}\\sigma&#39;(z_2^2)W_{21}^2\\sigma&#39;(z_1^1) \\notag \\\\ =&amp;\\begin{bmatrix}\\frac{\\partial C}{\\partial a_1^2}\\sigma&#39;(z_1^2) &amp; \\frac{\\partial C}{\\partial a_2^2}\\sigma&#39;(z_2^2) \\end{bmatrix} \\begin{bmatrix} W_{11}^2 \\\\ W_{21}^2 \\end{bmatrix} \\odot \\begin{bmatrix} \\sigma&#39;(z_1^1) \\end{bmatrix} \\notag \\end{align} \\] 同理，\\(\\delta_2^1=\\begin{bmatrix}\\frac{\\partial C}{\\partial a_1^2}\\sigma&#39;(z_1^2) &amp; \\frac{\\partial C}{\\partial a_2^2}\\sigma&#39;(z_2^2) \\end{bmatrix} \\begin{bmatrix} W_{12}^2 \\\\ W_{22}^2 \\end{bmatrix} \\odot \\begin{bmatrix} \\sigma&#39;(z_2^1) \\end{bmatrix}\\)。 这样，我们就得到第 1 层的误差项： \\[ \\delta^1=\\begin{bmatrix} W_{11}^2 &amp; W_{21}^2 \\\\ W_{12}^2 &amp; W_{22}^2 \\end{bmatrix} \\begin{bmatrix} \\frac{\\partial C}{\\partial z_1^2} \\\\ \\frac{\\partial C}{\\partial z_2^2} \\end{bmatrix} \\odot \\begin{bmatrix} \\sigma&#39;(z_1^1) \\\\ \\sigma&#39;(z_2^1) \\end{bmatrix}={W^{2}}^T\\delta^2 \\odot \\sigma&#39;(z^1) \\tag{2} \\] 然后，根据误差项计算导数： \\[ \\frac{\\partial C}{\\partial b_j^2}=\\frac{\\partial C}{\\partial z_j^2}\\frac{\\partial z_j^2}{\\partial b_j^2}=\\delta_j^2 \\\\ \\frac{\\partial C}{\\partial w_{jk}^2}=\\frac{\\partial C}{\\partial z_j^2}\\frac{\\partial z_j^2}{\\partial w_{jk}^2}=a_k^{1}\\delta_j^2 \\\\ \\frac{\\partial C}{\\partial b_j^1}=\\frac{\\partial C}{\\partial z_j^1}\\frac{\\partial z_j^1}{\\partial b_j^1}=\\delta_j^1 \\\\ \\frac{\\partial C}{\\partial w_{jk}^1}=\\frac{\\partial C}{\\partial z_j^1}\\frac{\\partial z_j^1}{\\partial w_{jk}^1}=a_k^{0}\\delta_j^1 \\] BP 算法的套路 在 BP 算法中，我们计算的误差项 \\(\\delta^l\\) 其实就是 \\(loss\\ function\\) 对 \\(z^l\\) 的导数 \\(\\frac{\\partial C}{\\partial z^l}\\)，有了该导数后，根据链式法则就可以比较容易地求出 \\(\\frac{\\partial C}{\\partial W^l}\\) 和 \\(\\frac{\\partial C}{\\partial b^l}\\)。 CNN 中的 BP 算法 之所以要「啰嗦」地回顾普通的 BP 算法，主要是为了熟悉一下链式法则，因为这一点在理解 CNN 的 BP 算法时尤为重要。 下面就来考虑如何把之前的算法套路用在 CNN 网络中。 CNN 的难点在于卷积层和 pooling 层这两种很特殊的结构，因此下面重点分析这两种结构的 BP 算法如何执行。 卷积层 假设我们要处理如下卷积操作： \\[ \\left( \\begin{array}{ccc} a_{11}&amp;a_{12}&amp;a_{13} \\\\ a_{21}&amp;a_{22}&amp;a_{23}\\\\ a_{31}&amp;a_{32}&amp;a_{33} \\end{array} \\right) * \\left( \\begin{array}{ccc} w_{11}&amp;w_{12}\\\\ w_{21}&amp;w_{22} \\end{array} \\right) = \\left( \\begin{array}{ccc} z_{11}&amp;z_{12}\\\\ z_{21}&amp;z_{22} \\end{array} \\right) \\] 这个操作咋一看完全不同于全联接层的操作，这样，想套一下 BP 算法都不知从哪里入手。但是，如果把卷积操作表示成下面的等式，问题就清晰多了（卷积操作一般是要把卷积核旋转 180 度再相乘的，不过，由于 CNN 中的卷积参数本来就是学出来的，所以旋不旋转，关系其实不大，这里默认不旋转）： \\[ z_{11} = a_{11}w_{11} + a_{12}w_{12} + a_{21}w_{21} + a_{22}w_{22} \\\\ z_{12} = a_{12}w_{11} + a_{13}w_{12} + a_{22}w_{21} + a_{23}w_{22} \\\\ z_{21} = a_{21}w_{11} + a_{22}w_{12} + a_{31}w_{21} + a_{32}w_{22} \\\\ z_{22} = a_{22}w_{11} + a_{23}w_{12} + a_{32}w_{21} + a_{33}w_{22} \\] 更进一步，我们还可以把上面的等式表示成下图： 上图的网络结构中，左边青色的神经元表示 \\(a_{11}\\) 到 \\(a_{33}\\)，中间橙色的表示 \\(z_{11}\\) 到 \\(z_{22}\\)。需要注意的是，青色和橙色神经元之间的权值连接用了不同的颜色标出，紫色线表示 \\(w_{11}\\)，蓝色线表示 \\(w_{12}\\)，依此类推。这样一来，如果你熟悉 BP 链式法则的套路的话，你可能已经懂了卷积层的 BP 是怎么操作的了。因为卷积层其实就是一种特殊的连接层，它是部分连接的，而且参数也是共享的。 假设上图中，\\(z\\) 这一层神经元是第 \\(l\\) 层，即 \\(z=z^{l}\\)，\\(a=a^{l-1}\\)。同时假设其对应的误差项 \\(\\delta^{l}=\\frac{\\partial C}{\\partial z^{l}}\\) 我们已经算出来了。下面，按照 BP 的套路，我们要根据 \\(\\delta^{l}\\) 计算 \\(\\delta^{l-1}\\)、\\(\\frac{\\partial C}{\\partial w^l}\\) 和 \\(\\frac{\\partial C}{\\partial b^l}\\) 。 卷积层的误差项 \\(\\delta^{l-1}\\) 首先计算 \\(\\delta^{l-1}\\)。假设上图中的 \\(a^{l-1}\\) 是前一层经过某些操作（可能是激活函数，也可能是 pooling 层等，但不管是哪种操作，我们都可以用 \\(\\sigma()\\) 来表示）后得到的响应，即 \\(a^{l-1}=\\sigma(z^{l-1})\\)。那么，根据链式法则： \\[ \\delta^{l-1}=\\frac{\\partial C}{\\partial z^{l-1}}=\\frac{\\partial C}{\\partial z^{l}}\\frac{\\partial z^l}{\\partial a^{l-1}}\\frac{\\partial a^{l-1}}{\\partial z^{l-1}}=\\delta^l \\frac{\\partial z^l}{\\partial a^{l-1}} \\odot \\sigma&#39;(z^{l-1}) \\tag{3} \\] 对照上面的例子，\\(z^{l-1}\\) 应该是一个 9 维的向量，所以 \\(\\sigma&#39;(z^{l-1})\\) 也是一个向量，根据之前 BP 的套路，这里需要 \\(\\odot\\) 操作。 这里的重点是要计算 \\(\\frac{\\partial z^l}{\\partial a^{l-1}}\\)，这也是卷积层区别于全联接层的地方。根据前面展开的卷积操作的等式，这个导数其实比全联接层更容易求。以 \\(a_{11}^{l-1}\\) 和 \\(a_{12}^{l-1}\\) 为例（简洁起见，下面去掉右上角的层数符号 \\(l\\)）： \\[ \\begin{align} \\nabla a_{11} = &amp; \\frac{\\partial C}{\\partial z_{11}} \\frac{\\partial z_{11}}{\\partial a_{11}}+ \\frac{\\partial C}{\\partial z_{12}}\\frac{\\partial z_{12}}{\\partial a_{11}}+ \\frac{\\partial C}{\\partial z_{21}}\\frac{\\partial z_{21}}{\\partial a_{11}} + \\frac{\\partial C}{\\partial z_{22}}\\frac{\\partial z_{22}}{\\partial a_{11}} \\notag \\\\ =&amp; \\delta_{11}w_{11} \\notag \\end{align} \\] \\[ \\begin{align} \\nabla a_{12} =&amp; \\frac{\\partial C}{\\partial z_{11}}\\frac{\\partial z_{11}}{\\partial a_{12}} + \\frac{\\partial C}{\\partial z_{12}}\\frac{\\partial z_{12}}{\\partial a_{12}} + \\frac{\\partial C}{\\partial z_{21}}\\frac{\\partial z_{21}}{\\partial a_{12}} + \\frac{\\partial C}{\\partial z_{22}}\\frac{\\partial z_{22}}{\\partial a_{12}} \\notag \\\\ =&amp;\\delta_{11}w_{12} + \\delta_{12}w_{11} \\notag \\end{align} \\] （\\(\\nabla a_{ij}\\) 表示 \\(\\frac{\\partial C}{\\partial a_{ij}}\\)。如果这两个例子看不懂，证明对之前 BP 例子中的（1）式理解不够，请先复习普通的 BP 算法。） 其他 \\(\\nabla a_{ij}\\) 的计算，道理相同。 之后，如果你把所有式子都写出来，就会发现，我们可以用一个卷积运算来计算所有 \\(\\nabla a_{ij}^{l-1}\\)： \\[ \\left( \\begin{array}{ccc} 0&amp;0&amp;0&amp;0 \\\\ 0&amp;\\delta_{11}&amp; \\delta_{12}&amp;0 \\\\ 0&amp;\\delta_{21}&amp;\\delta_{22}&amp;0 \\\\ 0&amp;0&amp;0&amp;0 \\end{array} \\right) * \\left( \\begin{array}{ccc} w_{22}&amp;w_{21}\\\\ w_{12}&amp;w_{11} \\end{array} \\right) = \\left( \\begin{array}{ccc} \\nabla a_{11}&amp;\\nabla a_{12}&amp;\\nabla a_{13} \\\\ \\nabla a_{21}&amp;\\nabla a_{22}&amp;\\nabla a_{23}\\\\ \\nabla a_{31}&amp;\\nabla a_{32}&amp;\\nabla a_{33} \\end{array} \\right) \\] 这样一来，（3）式可以改写为： \\[ \\delta^{l-1}=\\frac{\\partial C}{\\partial z^{l-1}}=\\delta^l * rot180(W^l) \\odot \\sigma&#39;(z^{l-1}) \\tag{4} \\] （4）式就是 CNN 中误差项的计算方法。注意，跟原始的 BP 不同的是，这里需要将后一层的误差 \\(\\delta^l\\) 写成矩阵的形式，并用 0 填充到合适的维度。而且这里不再是跟矩阵 \\({W^l}^T\\) 相乘，而是先将 \\(W^l\\) 旋转 180 度后，再跟其做卷积运算。 卷积层的导数 \\(\\frac{\\partial C}{\\partial w^l}\\) 和 \\(\\frac{\\partial C}{\\partial b^l}\\) 这两项的计算也是类似的。假设已经知道当前层的误差项 \\(\\delta^l\\)，参考之前 \\(\\nabla a_{ij}\\) 的计算，可以得到： \\[ \\begin{align} \\nabla w_{11}=&amp;\\frac{\\partial C}{\\partial z_{11}} \\frac{\\partial z_{11}}{\\partial w_{11}}+ \\frac{\\partial C}{\\partial z_{12}}\\frac{\\partial z_{12}}{\\partial w_{11}}+ \\frac{\\partial C}{\\partial z_{21}}\\frac{\\partial z_{21}}{\\partial w_{11}} + \\frac{\\partial C}{\\partial z_{22}}\\frac{\\partial z_{22}}{\\partial w_{11}} \\notag \\\\ =&amp;\\delta_{11}a_{11}+\\delta_{12}a_{12}+\\delta_{21}a_{21}+\\delta_{22}a_{22} \\notag \\end{align} \\] \\[ \\begin{align} \\nabla w_{12}=&amp;\\frac{\\partial C}{\\partial z_{11}} \\frac{\\partial z_{11}}{\\partial w_{12}}+ \\frac{\\partial C}{\\partial z_{12}}\\frac{\\partial z_{12}}{\\partial w_{12}}+ \\frac{\\partial C}{\\partial z_{21}}\\frac{\\partial z_{21}}{\\partial w_{12}} + \\frac{\\partial C}{\\partial z_{22}}\\frac{\\partial z_{22}}{\\partial w_{12}} \\notag \\\\ =&amp;\\delta_{11}a_{12}+\\delta_{12}a_{13}+\\delta_{21}a_{22}+\\delta_{22}a_{23} \\notag \\end{align} \\] 其他 \\(\\nabla w_{ij}\\) 的计算同理。 跟 \\(\\nabla a_{ij}\\) 一样，我们可以用矩阵卷积的形式表示： \\[ \\left( \\begin{array}{ccc} a_{11}&amp;a_{12}&amp;a_{13}\\\\ a_{21}&amp;a_{22}&amp;a_{23}\\\\ a_{31}&amp;a_{32}&amp;a_{33} \\end{array} \\right) * \\left( \\begin{array}{ccc} \\delta_{11}&amp; \\delta_{12}\\\\ \\delta_{21}&amp;\\delta_{22}\\end{array} \\right) = \\left( \\begin{array}{ccc} \\nabla w_{11}&amp;\\nabla w_{12}\\\\ \\nabla w_{21}&amp;\\nabla w_{22} \\end{array} \\right) \\] 这样就得到了 \\(\\frac{\\partial C}{\\partial w^l}\\) 的公式： \\[ \\frac{\\partial C}{\\partial w^l}=a^{l-1}*\\delta^l \\tag{5} \\] 对于 \\(\\frac{\\partial C}{\\partial b^l}\\)，我参考了文末的链接，但对其做法仍然不太理解，我觉得在卷积层中，\\(\\frac{\\partial C}{\\partial b^l}\\) 和一般的全联接层是一样的，仍然可以用下面的式子得到： \\[ \\frac{\\partial C}{\\partial b^l}=\\delta^l \\tag{6} \\] 理解不一定对，所以这一点上大家还是参考一下其他资料。 pooling 层 跟卷积层一样，我们先把 pooling 层也放回网络连接的形式中： 红色神经元是前一层的响应结果，一般是卷积后再用激活函数处理。绿色的神经元表示 pooling 层。很明显，pooling 主要是起到降维的作用，而且，由于 pooling 时没有参数需要学习，因此，当得到 pooling 层的误差项 \\(\\delta^l\\) 后，我们只需要计算上一层的误差项 \\(\\delta^{l-1}\\) 即可。要注意的一点是，由于 pooling 一般会降维，因此传回去的误差矩阵要调整维度，即 \\(upsample\\)。这样，误差传播的公式原型大概是： \\(\\delta^{l-1}=upsample(\\delta^l) \\odot \\sigma&#39;(z^{l-1})\\)。 下面以最常用的 average pooling 和 max pooling 为例，讲讲 \\(upsample(\\delta^l)\\) 具体要怎么处理。 假设 pooling 层的区域大小为 \\(2 \\times 2\\)，pooling 这一层的误差项为： \\[ \\delta^l= \\left( \\begin{array}{ccc} 2 &amp; 8 \\\\ 4 &amp; 6 \\end{array} \\right) \\] 首先，我们先把维度还原到上一层的维度： \\[ \\left( \\begin{array}{ccc} 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 2 &amp; 8 &amp; 0 \\\\ 0 &amp; 4 &amp; 6 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 \\end{array} \\right) \\] 在 average pooling 中，我们是把一个范围内的响应值取平均后，作为一个 pooling unit 的结果。可以认为是经过一个 average() 函数，即 \\(average(x)=\\frac{1}{m}\\sum_{k=1}^m x_k\\)。在本例中，\\(m=4\\)。则对每个 \\(x_k\\) 的导数均为： \\[\\frac{\\partial average(x)}{\\partial x_k}=\\frac{1}{m}\\] 因此，对 average pooling 来说，其误差项为： \\[ \\begin{align} \\delta^{l-1}=&amp;\\delta^l \\frac{\\partial average}{\\partial x} \\odot \\sigma&#39;(z^{l-1}) \\notag \\\\ =&amp;upsample(\\delta^l) \\odot \\sigma&#39;(z^{l-1}) \\tag{7} \\\\ =&amp;\\left( \\begin{array}{ccc} 0.5&amp;0.5&amp;2&amp;2 \\\\ 0.5&amp;0.5&amp;2&amp;2 \\\\ 1&amp;1&amp;1.5&amp;1.5 \\\\ 1&amp;1&amp;1.5&amp;1.5 \\end{array} \\right)\\odot \\sigma&#39;(z^{l-1}) \\notag \\end{align} \\] 在 max pooling 中，则是经过一个 max() 函数，对应的导数为： \\[ \\frac{\\partial \\max(x)}{\\partial x_k}=\\begin{cases} 1 &amp; if\\ x_k=max(x) \\\\ 0 &amp; otherwise \\end{cases} \\] 假设前向传播时记录的最大值位置分别是左上、右下、右上、左下，则误差项为： \\[ \\delta^{l-1}=\\left( \\begin{array}{ccc} 2&amp;0&amp;0&amp;0 \\\\ 0&amp;0&amp; 0&amp;8 \\\\ 0&amp;4&amp;0&amp;0 \\\\ 0&amp;0&amp;6&amp;0 \\end{array} \\right) \\odot \\sigma&#39;(z^{l-1}) \\tag{8} \\] 参考 How the backpropagation algorithm works 卷积神经网络(CNN)反向传播算法 Convolutional Neural Networks backpropagation: from intuition to derivation 如何通俗易懂地解释卷积？ - 马同学的回答 - 知乎 https://www.slideshare.net/kuwajima/cnnbp","raw":null,"content":null,"categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://jermmy.github.io/categories/机器学习/"}],"tags":[{"name":"深度学习","slug":"深度学习","permalink":"https://jermmy.github.io/tags/深度学习/"}]},{"title":"PCA，到底在做什么","slug":"2017-12-15-PCA-another-view","date":"2017-12-14T13:57:25.000Z","updated":"2018-01-10T04:58:58.000Z","comments":true,"path":"2017/12/14/2017-12-15-PCA-another-view/","link":"","permalink":"https://jermmy.github.io/2017/12/14/2017-12-15-PCA-another-view/","excerpt":"很久以前写过一篇 PCA 的小白教程，不过由于当时对 PCA 的理解流于表面，所以只是介绍了一下 PCA 的算法流程。今天在数图课上偶然听到 PCA 在图像压缩上的应用，突然明白了一点实质性的东西，这里趁热记录一波。\n\n\n","text":"很久以前写过一篇 PCA 的小白教程，不过由于当时对 PCA 的理解流于表面，所以只是介绍了一下 PCA 的算法流程。今天在数图课上偶然听到 PCA 在图像压缩上的应用，突然明白了一点实质性的东西，这里趁热记录一波。 PCA 算法 首先还是简单回顾下 PCA 的算法流程。 我们把样本数据 \\(x\\) 归一化后，计算其协方差矩阵 \\(C_x\\)，然后计算 \\(C_x\\) 的特征向量，构造出一个特征向量矩阵 \\(A\\)，最后把 \\(x\\) 通过该矩阵映射到一个新的空间，得到的向量 \\(y\\) 就是能体现 \\(x\\) 主要成分的向量了。 PCA 在做什么 那么，这种空间映射有什么意义呢？问题要回到协方差矩阵 \\(C_x\\) 上。我们知道，协方差矩阵是一个对称矩阵，在线性代数中，对称矩阵的特征向量是相互正交的。而我们把 \\(x\\) 通过这个特征向量矩阵映射到 \\(y\\)，其实就是把原来的数据由最初的 \\([e_1, e_2, \\dots, e_n]\\) 的单位坐标系，调整到这些正交的特征向量组成的坐标系下，如下图所示： 这种坐标变换的意义又在哪呢？ 如果仔细分析，我们就会发现，这些新得到的向量 \\(y\\) 的均值为 \\(0\\)，而且它们的协方差矩阵为： \\[ C_y=AC_xA^T=\\begin{bmatrix} \\lambda_1 &amp; &amp; &amp; 0 \\\\ &amp; \\lambda_2 &amp; &amp; \\\\ &amp; &amp; \\ddots &amp; \\\\ 0 &amp; &amp; &amp; \\lambda_n \\end{bmatrix} \\] 这里，\\(A\\) 是由 \\(C_x\\) 的特征向量组成的矩阵，它的第一行表示最大特征值对应的特征向量，第二行表示第二大特征值对应的特征向量。\\(C_y\\) 对角线上的 \\(\\lambda_k\\) 代表 \\(C_x\\) 的特征值，而且是按照从大到小排序的（\\(\\lambda_1 &gt; \\lambda_2 &gt; \\dots &gt; \\lambda_n\\)）。 这个新的协方差矩阵有一个很重要的性质，除了对角线上的元素，其他元素通通是 0。要知道，协方差矩阵中，对角线上的元素表示方差，非对角线上的元素表示协方差。这说明，经过 PCA 处理后，我们把原本的数据 \\(x\\)，转变成各个分量之间没有任何关系（协方差为 0）的数据 \\(y\\)！我认为这正是 PCA 的精髓所在，也是我们使用 PCA 算法的根本目标。 另外，PCA 还经常用于降维处理，那么为什么 PCA 的降维效果会那么好？ 首先要明确一点，降维不是随便都能降的，最好的降维方法是要尽量保留重要的信息，而忽略次要的信息。在 PCA 中，我们一般是对协方差矩阵的特征值按从大到小排序，然后舍弃一些比较小的特征值（以及这些特征值对应的特征向量），这样重新计算得到 \\(y\\) 后，它的协方差矩阵可能是这个样子的： \\[ C_y=\\begin{bmatrix} \\lambda_1 &amp; &amp; &amp; 0 \\\\ &amp; \\lambda_2 &amp; &amp; \\\\ &amp; &amp; \\ddots &amp; \\\\ 0 &amp; &amp; &amp; \\lambda_k \\end{bmatrix} \\] （我们舍弃掉了 \\(n-k\\) 个特征向量，将数据由 \\(n\\) 维降到 \\(k\\) 维） 要知道，这些特征值（或者说方差）都是按照从大到小排序的，也就是说，我们在降维时，舍弃掉了那些特征值比较小的分量。这么做是符合常理的，因为数据的方差越大，证明分布越广，这样，我们还原这些数据的难度是越大的，而方差越小，证明数据分布越集中，还原它们的难度就越小（方差为 0 的话，用一个数就可以代表所有样本了）。所以，降维时，我们尽量保留那些方差大的数据，而忽略那些方差小的。本文开篇的图中给出一个形象的解释，我们把一个二维的数据映射到一维时，也是优先映射到方差大的那一维上，这样，原数据的分布规律可以最大限度的保留下来，信息的保留也是最完整的。","raw":null,"content":null,"categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://jermmy.github.io/categories/机器学习/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"https://jermmy.github.io/tags/机器学习/"}]},{"title":"论文笔记：Deep feature learning with relative distance comparison for person re-identification","slug":"2017-12-14-paper-note-deep-feature-learning-with-relative-distance-comparison","date":"2017-12-14T03:20:44.000Z","updated":"2018-10-03T09:10:52.000Z","comments":true,"path":"2017/12/14/2017-12-14-paper-note-deep-feature-learning-with-relative-distance-comparison/","link":"","permalink":"https://jermmy.github.io/2017/12/14/2017-12-14-paper-note-deep-feature-learning-with-relative-distance-comparison/","excerpt":"这篇论文是要解决 person re-identification 的问题。所谓 person re-identification，指的是在不同的场景下识别同一个人（如下图所示）。这里的难点是，由于不同场景下的角度、背景亮度等等因素的差异，同一个人的图像变化非常大，因而不能使用一般的图像分类的方法。论文采用了一种相似性度量的方法来促使神经网络学习出图像的特征，并根据特征向量的欧式距离来确定相似性。除此之外，论文通过对网络的训练过程进行分析，提出了一种计算效率更高的模型训练方法。\n\n\n","text":"这篇论文是要解决 person re-identification 的问题。所谓 person re-identification，指的是在不同的场景下识别同一个人（如下图所示）。这里的难点是，由于不同场景下的角度、背景亮度等等因素的差异，同一个人的图像变化非常大，因而不能使用一般的图像分类的方法。论文采用了一种相似性度量的方法来促使神经网络学习出图像的特征，并根据特征向量的欧式距离来确定相似性。除此之外，论文通过对网络的训练过程进行分析，提出了一种计算效率更高的模型训练方法。 论文方法 相似性模型 论文采用的度量相似性的方法基于一个简单的想法：相同类型图片（同一个人）的特征之间的距离要小于不同类型的特征。假设我们有一些训练样本，现在把它们组织成三元组的形式 \\({O_i}=&lt;O_i^1,O_i^2,O_i^3&gt;\\)，其中 \\(O_i^1\\) 和 \\(O_i^2\\) 表示属于同一类（匹配）的样本，\\(O_i^1\\) 和 \\(O_i^3\\) 表示不匹配的样本。设网络的参数为 \\(W=\\{W_j\\}\\)，\\(F_W(I)\\) 表示图像 \\(I\\) 的特征向量，则我们的目标是使下面的不等式成立： \\[ \\begin{align} ||F_W(O_i^1)-F_W(O_i^2)||^2&lt;||F_W(O_i^1)-F_W(O_i^3)||^2 \\tag{2} \\end{align} \\] 基于此，论文给出如下目标函数： \\[ \\begin{align} f(W,O)=\\sum_{i=1}^n{\\max{\\{||F_W(O_i^1)-F_W(O_i^2)||^2-||F_W(O_i^1)-F_W(O_i^3)||^2, C \\}}} \\tag{3} \\end{align} \\] 其中，\\(C\\) 被设为 -1。 网络结构 这三个子网络是共享参数的，目标函数是要让 \\(F_W(O_i^2)\\) 和 \\(F_W(O_i^1)\\) 靠近，而 \\(F_W(O_i^3)\\) 远离前两个特征向量。 训练算法 设 \\(d(W,O_i)=||F_W(O_i^1)-F_W(O_i^2)||^2-||F_W(O_i^1)-F_W(O_i^3)||^2\\)，基于 (3) 式，我们可以得到梯度下降的导数公式： \\[ \\begin{align} &amp; \\frac{\\partial f}{\\partial W_j}=\\sum_{O_i}{h(O_i)} \\tag{7} \\\\ &amp; h(O_i)=\\begin{cases} \\frac{\\partial d(W,O_i)}{\\partial W_j} &amp; if\\ d(W,O_i)&gt;C \\\\ 0 &amp; otherwise \\end{cases} \\tag{8} \\end{align} \\] \\[ \\begin{align} \\frac{\\partial d(W,O_i)}{\\partial W_j}=&amp;2(F_W(O_i^1)-F_W(O_i^2))\\frac{\\partial F_W(O_i^1)- \\partial F_W(O_i^2)}{\\partial W_j} \\notag \\\\ &amp;-2(F_W(O_i^1)-F_W(O_i^3))\\frac{\\partial F_W(O_i^1)- \\partial F_W(O_i^3)}{\\partial W_j} \\tag{9} \\end{align} \\] 由此可知，每次梯度下降时，我们只需要计算出每个 triplet 的 \\(F_W(O_i^1)\\)、\\(F_W(O_i^2)\\)、\\(F_W(O_i^3)\\) 和 \\(\\frac{\\partial F_W(O_i^1)}{\\partial W_j}\\)、\\(\\frac{\\partial F_W(O_i^2)}{\\partial W_j}\\)、\\(\\frac{\\partial F_W(O_i^3)}{\\partial W_j}\\)，就可以得到 \\(W_j\\) 的导数。这种导数计算方式是基于 triplet 的，每对样本需要计算三次前向和三次后向。由此可以得到论文中的算法 1: 然而，在实际训练时，一张图片可能在一个 batch 的多个 triplet 中出现，因此可以用一些技巧来减少一些重复计算的工作。重新审视导数的计算流程： \\[ \\begin{align} \\frac{\\partial f}{\\partial W}=\\sum_{O_i}(\\frac{\\partial f}{\\partial F_W(O_i^1)}\\frac{\\partial F_W(O_i^1)}{\\partial W_j} + \\frac{\\partial f}{\\partial F_W(O_i^2)}\\frac{\\partial F_W(O_i^2)}{\\partial W_j} + \\frac{\\partial f}{\\partial F_W(O_i^3)}\\frac{\\partial F_W(O_i^3)}{\\partial W_j}) \\notag \\end{align} \\] 可以发现，重复计算的地方在于 \\(\\frac{\\partial F_W(O_i^1)}{\\partial W_j}\\)、\\(\\frac{\\partial F_W(O_i^2)}{\\partial W_j}\\)、\\(\\frac{\\partial F_W(O_i^3)}{\\partial W_j}\\) 这些项，而且这些项也只跟对应的输入图像有关，所以，我们的想法是把这些可以重复使用的项提取出来。 假设一个训练 batch 中的图片集合为 \\(\\{I_k^{&#39;}\\}=\\{O_i^1\\} \\cup \\{O_i^2\\} \\cup \\{O_i^3\\}\\)，\\(m\\) 为图片数量，则针对一张图片的导数计算公式为： \\[ \\begin{align} \\sum_{O_i}\\frac{\\partial f}{\\partial F_W(I_i)}\\frac{\\partial F_W(I_i)}{W_j}=\\frac{\\partial F_W(I_i)}{W_j}\\sum_{O_i}\\frac{\\partial f}{\\partial F_W(I_i)}\\notag \\end{align} \\] 因此整个 batch 上的导数如下： \\[ \\begin{align} \\frac{\\partial f}{\\partial W}=\\sum_{i=1}^m\\{\\frac{\\partial F_W(I_i)}{W_j}\\sum_{O_i}\\frac{\\partial f}{\\partial F_W(I_i)}\\} \\tag{18} \\end{align} \\] \\(\\frac{\\partial F_W(I_i)}{W_j}\\) 只跟输入 \\(I_i\\) 有关，因此接下来要解决 \\(\\sum_{O_i}\\frac{\\partial f}{\\partial F_W(I_i)}\\) 的计算问题。后者的计算是跟 triplet 相关的： \\[ \\begin{align} \\frac{\\partial f}{\\partial F_W(I_K^{&#39;})}=\\sum_{i=1}^{n}\\frac{\\partial \\max\\{||F_W(O_i^1)-F_W(O_i^2)||^2-||F_W(O_i^1)-F_W(O_i^3)||^2,C\\}}{\\partial F_W(I_K^{&#39;})} \\tag{19} \\end{align} \\] 由此我们可以得出论文中的算法 3: 在此基础上，将 \\(\\frac{\\partial f}{\\partial F_W(I_K^{&#39;})}\\) 代入到 (18) 式，就得到了一个 batch 上的导数计算公式，即论文中的算法 2： 注意到，我们已经把原来基于 triplet 的计算方式转变为基于 image 的方式。后者可以大大减少计算量，我们只需要先计算出每张图片对应的 \\(F_W(I_K^{&#39;})\\) 和 \\(\\frac{\\partial F_W{I_K^{&#39;}}}{\\partial W_j}\\)，剩下的工作就是根据算法 2 计算出最终的导数。因此，这种计算方式使得整体的运算量只跟图片的数量有关。 最后要考虑的是样本生成的问题。最简单的想法是从所有可能的 triplet 组合中，随机挑选出若干的 triplet用于训练，但这种做法存在一个问题，考虑到数据集中的类别可能很大，因此所有 triplet 中包含的图片类别可能都是不同的，换句话说，网络在每次迭代时，处理的图片可能都是完全不同的，论文认为这种方式不利于参数的收敛。因此论文采用如下的 triplet 生成策略：在每轮迭代中，首先挑选出若干的类别（每个类别代表一个人），然后，对每个类别中的图片，从同类别的其他图片中随机选一张组成正样本对，从不同类别的图片中随机选一张组成负样本对。这种做法的优点在于，它的训练样本是梯度下降的时候动态生成的，假设显存中可以存放 300 张图片，那么对于最简单的 triplet 生成方法，可能只能放 100 对训练样本，但论文采用的生成策略，可以先从选定的几个类别中选出 300 张图片，然后进行 triplet 组合，等一次迭代训练完成后，再根据这 300 张图片随机生成另一种 triplet 组合。所以，这种方法不仅可以让网络更好地学习出样本对之间的距离约束关系，而且减少了频繁的 IO 操作。 下面给出完整的算法： 参考 Deep feature learning with relative distance comparison for person re-identification","raw":null,"content":null,"categories":[{"name":"计算机视觉","slug":"计算机视觉","permalink":"https://jermmy.github.io/categories/计算机视觉/"}],"tags":[{"name":"计算机视觉","slug":"计算机视觉","permalink":"https://jermmy.github.io/tags/计算机视觉/"},{"name":"深度学习","slug":"深度学习","permalink":"https://jermmy.github.io/tags/深度学习/"},{"name":"论文","slug":"论文","permalink":"https://jermmy.github.io/tags/论文/"}]},{"title":"论文笔记：Cross-Domain Visual Matching via Generalized Similarity Measure and Feature Learning","slug":"2017-12-12-paper-notes-cross-domain-visual-matching","date":"2017-12-12T15:34:00.000Z","updated":"2018-03-06T08:17:47.000Z","comments":true,"path":"2017/12/12/2017-12-12-paper-notes-cross-domain-visual-matching/","link":"","permalink":"https://jermmy.github.io/2017/12/12/2017-12-12-paper-notes-cross-domain-visual-matching/","excerpt":"Cross-Domain Visual Matching，即跨域视觉匹配。所谓跨域，指的是数据的分布不一样，简单点说，就是两种数据「看起来」不像。如下图中，（a）一般的正面照片和各种背景角度下拍摄的照片；（b）摄像头不同角度下拍到的照片；（c）年轻和年老时的人脸照；（d）证件照和草图风格的人脸照，这些图像都存在对应关系，但由于它们属于不同的域，因此必须针对不同的域采用不同的特征提取方法，之后再做特征匹配。这篇论文提出用一种通用的相似模型来匹配两个域之间的特征，并将其和特征提取流程融合在一起，统一成一个 end-to-end 的框架。\n\n\n","text":"Cross-Domain Visual Matching，即跨域视觉匹配。所谓跨域，指的是数据的分布不一样，简单点说，就是两种数据「看起来」不像。如下图中，（a）一般的正面照片和各种背景角度下拍摄的照片；（b）摄像头不同角度下拍到的照片；（c）年轻和年老时的人脸照；（d）证件照和草图风格的人脸照，这些图像都存在对应关系，但由于它们属于不同的域，因此必须针对不同的域采用不同的特征提取方法，之后再做特征匹配。这篇论文提出用一种通用的相似模型来匹配两个域之间的特征，并将其和特征提取流程融合在一起，统一成一个 end-to-end 的框架。 论文动机 针对这种跨域图片检索，常用的方法是：先用深度学习方法提取出不同域之间的特征，然后选择一种相似度模型来度量特征之间的相似性，并以此作为网络的代价函数来训练网络参数。如下图就是一种很常用的 Siamese 网络结构： 这里说的相似模型可以是欧氏距离、余弦相似度等等。但很多时候，我们并不知道哪种相似度模型最合适。这篇论文提出了一种通用的相似度模型——一种 Affine 变换。它表明，很多相似模型都可以用这个统一的模型来表示，而这个通用的模型也可以认为是多种相似模型的综合。因此，我们与其纠结选择哪种相似度模型，不如让网络自己学习出这个通用模型的参数，让网络自己选择最适合的模型。本文的另一个贡献在于将特征的提取与相似模型的学习融合进一个统一的框架内。 论文方法 通用的相似模型 论文认为，大多数相似模型都可以归纳为如下形式： \\[ \\begin{align} S(\\mathbf{x}, \\mathbf{y})=\\begin{bmatrix}\\mathbf{x}^T &amp; \\mathbf{y}^T &amp; 1 \\end{bmatrix} \\begin{bmatrix} \\mathbf{A} &amp; \\mathbf{C} &amp; \\mathbf{d} \\\\ \\mathbf{C}^T &amp; \\mathbf{B} &amp; \\mathbf{e} \\\\ \\mathbf{d}^T &amp; \\mathbf{e}^T &amp; f \\end{bmatrix} \\begin{bmatrix} \\mathbf{x} \\\\ \\mathbf{y} \\\\ 1 \\end{bmatrix} \\tag{1} \\end{align} \\] （\\(\\mathbf{A}\\)、\\(\\mathbf{B}\\)、\\(\\mathbf{C}\\) 是矩阵，\\(\\mathbf{d}\\) 和 \\(\\mathbf{e}\\) 是两个向量，\\(\\mathbf{x}\\) 和 \\(\\mathbf{y}\\) 分别表示两个待匹配的向量） 举个例子，如果 \\(\\mathbf{A}\\)=\\(\\mathbf{B} = \\mathbf{I}\\)，\\(\\mathbf{C}=-\\mathbf{I}\\)，\\(\\mathbf{d}=\\mathbf{e}=\\mathbf{0}\\)，\\(f=0\\)，则该模型退化为欧式距离：\\(S(x,y)=\\mathbf{x}^T\\mathbf{x}-2\\mathbf{x}\\mathbf{y}+\\mathbf{y}^T\\mathbf{y}=||\\mathbf{x}-\\mathbf{y}||^2\\)。 同时，论文还限制 \\(\\mathbf{A}\\)、\\(\\mathbf{B}\\) 为半正定矩阵，表示同域内的自相关性，\\(\\mathbf{C}\\) 为两个域之间的相关性矩阵。根据半正定矩阵的性质以及 \\(\\mathbf(C)\\) 的对称性，我们可以将 \\(\\mathbf{A}\\)、\\(\\mathbf{B}\\)、\\(\\mathbf{C}\\) 表示为： \\[ \\begin{align} \\mathbf{A}=\\mathbf{L_A}^T \\mathbf{L_A} \\notag \\\\ \\mathbf{B}=\\mathbf{L_B}^T \\mathbf{L_B} \\tag{2} \\\\ \\mathbf{C}=-\\mathbf{L_C^x}^T\\mathbf{L_C^y} \\notag \\end{align} \\] 这样，(1) 式可以展开为： \\[ \\begin{align} \\tilde{S}(\\mathbf{x},\\mathbf{y})&amp;=S(\\mathbf{f_1}(\\mathbf{x}), \\mathbf{f_2}(\\mathbf{y})) \\notag \\\\ =&amp; \\begin{bmatrix} \\mathbf{f_1}(\\mathbf{x})^T &amp; \\mathbf{f_2}(\\mathbf{y})^T &amp; 1 \\end{bmatrix} \\begin{bmatrix} \\mathbf{A} &amp; \\mathbf{C} &amp; \\mathbf{d} \\\\ \\mathbf{C}^T &amp; \\mathbf{B} &amp; \\mathbf{e} \\\\ \\mathbf{d}^T &amp; \\mathbf{e}^T &amp; f \\end{bmatrix} \\begin{bmatrix} \\mathbf{f_1}(\\mathbf{x}) \\\\ \\mathbf{f_2}(\\mathbf{y}) \\\\ 1 \\end{bmatrix} \\tag{3} \\\\ =&amp; \\mathbf{f_1}(\\mathbf{x})^T\\mathbf{A}\\mathbf{f_1}(\\mathbf{x}) + \\mathbf{f_1}(\\mathbf{x})^T\\mathbf{C}\\mathbf{f_2}(\\mathbf{y}) + \\mathbf{f_1}(\\mathbf{x})^T\\mathbf{d} + \\mathbf{f_2}(\\mathbf{y})^T\\mathbf{C}\\mathbf{f_1}(\\mathbf{x}) \\notag \\\\ &amp;+\\mathbf{f_2}(\\mathbf{y})^T\\mathbf{B}\\mathbf{f_2}(\\mathbf{y}) + \\mathbf{f_2}(\\mathbf{y})^T\\mathbf{e} + \\mathbf{d}^T\\mathbf{f_1}(\\mathbf{x}) + \\mathbf{e}^T\\mathbf{f_2}(\\mathbf{y}) + f \\notag \\\\ =&amp;||\\mathbf{L_A}\\mathbf{f_1}(\\mathbf{x})||^2+||\\mathbf{L_B}\\mathbf{f_2}(\\mathbf{y})||^2+2\\mathbf{d}^T\\mathbf{f_1}(\\mathbf{x}) \\notag \\\\ &amp; -2(\\mathbf{L_C^x}\\mathbf{f_1}(\\mathbf{x}))^T(\\mathbf{L_C^y}\\mathbf{f_2}(\\mathbf{y}))+2\\mathbf{e}^T\\mathbf{f_2}(\\mathbf{y})+f \\notag \\end{align} \\] (3) 式中的 \\(\\mathbf{f_1}(\\mathbf{x})\\) 和 \\(\\mathbf{f_2}(\\mathbf{y})\\) 分别代表 \\(\\mathbf{x}\\)、\\(\\mathbf{y}\\) 两个样本经过提取得到的向量。这样一来，我们就将半正定矩阵等限制引入到这个度量关系中。 接下来，我们考虑如何优化这个相似模型。 假设 \\(D=\\{(\\{\\mathbf{x_i}, \\mathbf{y_i}\\}, \\ell_i) \\}_{i=1}^N\\) 表示训练的数据集，\\(\\{\\mathbf{x_i}, \\mathbf{y_i}\\}\\) 表示来自两个不同域的样本，\\(\\ell_i\\) 标记两个样本是否属于同一类，如果是同一类则记为 -1，否则为 1： \\[ \\begin{align} \\ell_i=\\ell(\\mathbf{x_i}, \\mathbf{y_i})= \\begin{cases} -1 &amp; c(\\mathbf{x})=c(\\mathbf{y}) \\\\ 1 &amp; otherwise \\end{cases} \\tag{4} \\end{align} \\] 我们的目标是，当两个样本属于同一类时，相似度要小于 -1，否则大于 1： \\[ \\begin{align} \\tilde{S}(\\mathbf{x_i}, \\mathbf{y_i})=\\begin{cases} &lt; -1 &amp; if\\ \\ell_i=-1 \\\\ \\ge 1 &amp; otherwise \\end{cases} \\tag{5} \\end{align} \\] 基于此，论文提出如下目标函数： \\[ \\begin{align} G(\\mathbf{W}, \\mathbf{\\phi})=\\sum_{i=1}^N{(1-\\ell_i \\tilde{S}(\\mathbf{x_i}, \\mathbf{y_i}))_+}+\\Psi (\\mathbf{W}, \\mathbf{\\phi}) \\tag{6} \\end{align} \\] 其中，\\(\\mathbf{W}, \\mathbf{\\phi}\\) 分别表示网络参数和相似模型参数，\\(\\mathbf{\\phi}=(\\mathbf{L_A},\\mathbf{L_B},\\mathbf{L_C^x},\\mathbf{L_C^y},\\mathbf{d},\\mathbf{e},f)\\)，\\(\\Psi(\\mathbf{W}, \\mathbf{\\phi})\\) 表示正则化项。 特征学习与相似度融合 接下来要解决的是如何用一个框架将特征学习与相似模型融合在一起。论文采用如下网络结构： 首先，我们用两个子网络分支分别提取两个不同域的图片特征，然后用一个共享的网络将两种特征融合起来，最后再用两个子网络分支计算相似模型的参数。 接下来分析一下网络训练的流程。首先，我们将网络的输出结果记为： \\[ \\begin{align} \\tilde{x} \\triangleq \\begin{bmatrix} \\mathbf{L_A f_1(x)} &amp; \\mathbf{L_C^x f_1(x)} &amp; \\mathbf{d}^T \\mathbf{f_1(x)} \\end{bmatrix}^T \\notag \\\\ \\tilde{y} \\triangleq \\begin{bmatrix} \\mathbf{L_B f_2(y)} &amp; \\mathbf{L_C^y f_2(y)} &amp; \\mathbf{e}^T \\mathbf{f_2(y)} \\end{bmatrix}^T \\notag \\end{align} \\] 为了将 \\(\\tilde{x}\\) 和 \\(\\tilde{y}\\) 代入 (3) 式，我们再引入三个辅助矩阵： \\[ \\begin{align} &amp; \\mathbf{P_1}=\\begin{bmatrix} \\mathbf{I}^{r \\times r} &amp; \\mathbf{0}^{r \\times (r+1)} \\end{bmatrix} \\notag \\\\ &amp; \\mathbf{P_2}=\\begin{bmatrix} \\mathbf{0}^{r \\times r} &amp; \\mathbf{I}^{r \\times r} &amp; \\mathbf{0}^{r \\times 1} \\end{bmatrix} \\notag \\\\ &amp; \\mathbf{P_3}=\\begin{bmatrix} \\mathbf{0}^{1 \\times 2r} &amp; 1^{1 \\times 1} \\end{bmatrix}^T \\notag \\end{align} \\] \\(r\\) 表示网络输出的特征维度，即 \\(\\mathbf{f_1}(\\mathbf{x})\\) 和 \\(\\mathbf{f_2}(\\mathbf{y})\\) 的维度。 借助这三个矩阵，可以得到：\\(\\mathbf{L_A}\\mathbf{f_1}(\\mathbf{x})=\\mathbf{P_1}\\tilde{x}\\)，\\(\\mathbf{L_B}\\mathbf{f_2}(\\mathbf{y})=\\mathbf{P_1}\\tilde{y}\\) 等。 然后我们可以把 (3) 式表示为： \\[ \\begin{align} \\tilde{S}(x,y)=&amp;(\\mathbf{P_1} \\tilde{x})^T\\mathbf{P_1}\\tilde{x}+(\\mathbf{P_1} \\tilde{y})^T\\mathbf{P_1}\\tilde{y} \\notag \\\\ &amp;-2(\\mathbf{P_2}\\tilde{x}^T)\\mathbf{P_2}\\tilde{y}+2\\mathbf{P_3}^T\\tilde{x}+2\\mathbf{P_3}^T\\tilde{y}+f \\notag \\end{align} \\] 目标函数调整为： \\[ \\begin{align} G(\\mathbf{W}, \\mathbf{\\phi}; D) =&amp;\\sum_{i=1}^N{(1-\\ell_i \\tilde{S}(x_i,y_i))_+ + \\Psi{(\\mathbf{W}, \\phi)}} \\notag \\\\ =&amp;\\sum_{i=1}^N{\\{1-\\ell_i [(\\mathbf{P_1}\\tilde{x_i})^T\\mathbf{P_1}\\tilde{x_i} + (\\mathbf{P_1}\\tilde{y_i})^T\\mathbf{P_1}\\tilde{y_i}} - \\notag \\\\ &amp; {2(\\mathbf{P_2 \\tilde{x_i}})^T\\mathbf{P_2}\\tilde{y_i} + 2\\mathbf{P_3}^T\\tilde{x_i}+2\\mathbf{P_3}^T\\tilde{y_i}+f] \\}}_+ + \\Psi{(\\mathbf{W}, \\phi)} \\tag{13} \\end{align} \\] 公式中的 \\(D\\) 表示训练的样本对。 训练细节优化 值得注意的是，(13) 式是一种 sample-pair-based 的表示，每次计算目标函数的时候，我们需要生成很多的样本对，而且，由于同一张图片可能出现在多个 pair 中，这样就容易造成很多重复计算（例如，计算 \\(\\tilde{x_1}\\) 时需要一次前向传播，如果 \\(x_1\\) 这张图片又出现在同一个 batch 的其他 pair 中，那我们又需要计算一次前向传播，而这个计算结果其实是可以复用的）。所以，下一步是将它表示成 sample-based 的形式，以减少这种重复计算。 sample-based 的基本想法是，在每次 batch 训练时，我们不是用一个个 pair 计算，而是针对每张图片，把与该图片相关的操作都放在一起计算。 具体点说，假设一个 batch 中的数据为 \\(\\{\\{x_1,y_1\\},\\dots,\\{x_n,y_n\\}\\}\\)，在原始的 sample-pair-based 的算法中，我们会针对网络的两个输入计算两个导数。 对 \\(\\tilde{x_i}\\) 而言： \\[ \\begin{align} \\frac{\\partial G(\\Omega_x; D)}{\\partial \\Omega_x}=\\sum_{i=1}^n{-\\frac{\\partial (\\ell_i \\tilde{S}(x_i,y_i))}{\\partial \\tilde{x_i}}\\frac{\\partial \\tilde{x_i}}{\\partial \\Omega_x} + \\frac{\\partial \\Psi}{\\partial \\Omega_x} } \\notag \\end{align} \\] 对 \\(\\tilde{y_i}\\) 而言： \\[ \\begin{align} \\frac{\\partial G(\\Omega_y; D)}{\\partial \\Omega_y}=\\sum_{i=1}^n{-\\frac{\\partial (\\ell_i \\tilde{S}(x_i,y_i))}{\\partial \\tilde{y_i}}\\frac{\\partial \\tilde{y_i}}{\\partial \\Omega_y} + \\frac{\\partial \\Psi}{\\partial \\Omega_y} } \\notag \\end{align} \\] 在上面两个公式中，\\(\\Omega=\\{W,\\phi\\}\\)，\\(n\\)表示sample的数量。不难发现，\\(\\frac{\\partial \\tilde{x_i}}{\\partial \\Omega_x}\\) 和 \\(\\frac{\\partial \\tilde{y_i}}{\\partial \\Omega_y}\\) 都只跟各自的输入图片有关，跟样本对中的另一张图片无关，而这一项也是重复计算的根源，所以，sample-based 的计算过程是把这两项单独提取出来，重复利用。以 \\(\\tilde{x_i}\\) 为例： \\[ \\begin{align} \\frac{\\partial G(\\Omega_x; D)}{\\partial \\Omega_x}=\\sum_{i=1}^m{\\{ \\frac{\\partial \\tilde{x_i}}{\\partial \\Omega_x} \\sum_{j}(-\\frac{\\partial (\\ell_i \\tilde{S}(x_i,y_j))}{\\partial \\tilde{x_i}})\\} + \\frac{\\partial \\Psi}{\\partial \\Omega_x} } \\notag \\end{align} \\] 上式中的 \\(y_j\\) 表示与 \\(x_i\\) 组成一个 pair 的另一张图片。注意这里我们用\\(m\\)替换\\(n\\)，因为现在只需要针对\\(m\\)张不同的图片（\\(n\\)个样本对中可能只有\\(m\\)张\\(x_i\\)是不同的）计算导数平均值即可。 基于上述思想，论文给出了如下更加符号化的表述。对于 \\(D\\) 中的每一个样本对 \\(\\{\\mathbf{x_i}，\\mathbf{y_i}\\}\\)，我们假设 \\(\\mathbf{z}^{j_{i,1}}=\\mathbf{x_i}\\)，\\(\\mathbf{z}^{j_{i,2}}=\\mathbf{y_i}\\)，其中，\\(1 \\le j_{i,1} \\le M_x\\)，\\(M_x+1 \\le j_{i,2} \\le M_z(=M_x+M_y)\\)，\\(M_x\\) 是样本 \\(\\mathbf{x}\\) 的数量，\\(M_y\\) 是样本 \\(\\mathbf{y}\\) 的数量。同理，\\(\\tilde{\\mathbf{z}}^{j_{i,1}}=\\tilde{\\mathbf{x_i}}\\)，\\(\\tilde{\\mathbf{z}}^{j_{i,2}}=\\tilde{\\mathbf{y_i}}\\)。 然后，我们可以将 (13) 式改写成 sample-based 的形式： \\[ \\begin{align} G(\\Omega;Z)=&amp;L(\\Omega;Z)+\\Psi(\\Omega) \\notag \\\\ =&amp; \\sum_{i=1}^N{\\{1-\\ell_i [(\\mathbf{P_1} \\tilde{\\mathbf{z}}^{j_{i,1}})^T \\mathbf{P_1}\\tilde{\\mathbf{z}}^{j_{i,1}} + (\\mathbf{P_1} \\tilde{\\mathbf{z}}^{j_{i,2}})^T \\mathbf{P_1}\\tilde{\\mathbf{z}}^{j_{i,2}}} \\notag \\\\ &amp; {-2(\\mathbf{P_2}\\tilde{\\mathbf{z}}^{j_{i,1}})^T\\mathbf{P_2}\\tilde{\\mathbf{z}}^{j_{i,2}} + 2\\mathbf{P_3}^T\\tilde{\\mathbf{z}}^{j_{i,1}} + 2\\mathbf{P_3}^T\\tilde{\\mathbf{z}}^{j_{i,2}} + f] \\}}_+ \\notag \\\\ &amp; + \\Psi(\\Omega) \\tag{14} \\end{align} \\] 其中，\\(\\Omega=(\\mathbf{W}, \\phi)\\)。 梯度下降的导数为： \\[ \\Omega=\\Omega - \\alpha \\frac{\\partial G(\\Omega)}{\\partial \\Omega}=\\Omega - \\alpha ( \\frac{\\partial L(\\Omega)}{\\Omega} + \\frac{\\partial \\Psi}{\\partial \\Omega}) \\] 其中，\\(\\alpha\\) 是学习率。 这里的重点是要计算 \\(\\frac{\\partial L(\\Omega)}{\\partial \\Omega}\\)： \\[ \\frac{\\partial L(\\Omega)}{\\partial \\Omega}=\\sum_j{\\frac{\\partial L}{\\partial \\tilde{\\mathbf{z}}^j}\\frac{\\partial \\tilde{\\mathbf{z}}^j}{\\partial \\Omega}} \\] 由于存在两个子网络分支，上式中，\\(\\tilde{\\mathbf{z}}^j=\\tilde{\\mathbf{z}}^{j_{i,x}}\\) 或 \\(\\tilde{\\mathbf{z}}^j=\\tilde{\\mathbf{z}}^{j_{i,y}}\\)，\\(\\frac{\\partial \\tilde{\\mathbf{z}}^j}{\\partial \\Omega}\\) 分别对应两个网络分支的导数（论文中 \\(\\tilde{\\mathbf{z}}^{j_{i,x}}\\)、\\(\\tilde{\\mathbf{z}}^{j_{i,y}}\\) 和 (14) 式中的 \\(\\tilde{\\mathbf{z}}^{j_{i,1}}\\)、\\(\\tilde{\\mathbf{z}}^{j_{i,2}}\\) 等符号混用了）。在梯度下降中，我们需要针对两个网络分支计算两次导数。 对处理 \\(\\mathbf{x}\\) 的分支，计算得到： \\[ \\begin{align} \\frac{\\partial L}{\\partial \\tilde{\\mathbf{z}}^{j_{i,x}}}=-\\sum_{j_{i,y}}{2\\ell_{j_{i,x},j_{i,y}} (\\mathbf{P_1}^T\\mathbf{P_1}\\tilde{\\mathbf{z}}^{j_{i,x}}-\\mathbf{P_2}^T\\mathbf{P_2}\\tilde{\\mathbf{z}}^{j_{i,y}}+\\mathbf{P_3})} \\notag \\end{align} \\] 这里的 \\(\\ell_{j_{i,x},j_{i,y}}\\) 就是 (4) 式中定义的 \\(\\ell(\\mathbf{x_i}, \\mathbf{y_i})\\)。 另外，有些样本可能已经具备一定的区分度了，因此训练时要剔除这些样本。论文中引入一个标识函数 \\(\\mathbf{1}_{\\mathbf{z}^{j_{i,x}}}(\\mathbf{z}^{j_{i,y}})\\)，当 \\(\\ell_{j_{i,x},j_{i,y}}\\tilde{S}(\\mathbf{z}^{j_{i,x}},\\mathbf{z}^{j_{i,y}})&lt;1\\) 时「由 (5) 式可知，此时 \\((\\mathbf{z}^{j_{i,x}},\\mathbf{z}^{j_{i,y}})\\) 还达不到要求的区分度」，\\(\\mathbf{1}_{\\mathbf{z}^{j_{i,x}}}(\\mathbf{z}^{j_{i,y}})=1\\)，否则为 0。于是，得到最终的导数公式： \\[ \\frac{\\partial L}{\\partial \\tilde{\\mathbf{z}}^{j_{i,x}}}=-\\sum_{j_{i,y}}{2\\mathbf{1}_{\\mathbf{z}^{j_{i,x}}}(\\mathbf{z}^{j_{i,y}})\\ell_{j_{i,x},j_{i,y}} (\\mathbf{P_1}^T\\mathbf{P_1}\\tilde{\\mathbf{z}}^{j_{i,x}}-\\mathbf{P_2}^T\\mathbf{P_2}\\tilde{\\mathbf{z}}^{j_{i,y}}+\\mathbf{P_3})} \\tag{18} \\] 对于另一个分支，我们可以用同样的方法计算 \\(\\frac{\\partial L}{\\partial \\tilde{\\mathbf{z}}^{j_{i,y}}}\\)。 最后，把 (18) 式代回到 (17) 式，就可以得到 sample-based 的梯度公式。 论文给出了计算公式 (18) 的具体执行算法： 这个算法总结起来就是，对于每张输入图片 \\(\\mathbf{z}^j\\)，收集跟它组成 pair 的所有图片，按照公式 (18) 计算 \\(\\frac{\\partial L}{\\partial \\tilde{\\mathbf{z}}^j}\\)。 下面给出的是完整的梯度下降算法： 注意到，对于每次迭代训练的 batc h数据，算法中会将每张图片前向传播和后向传播的计算结果都保存下来，然后再针对之前提及的 sample-based 的方法计算梯度。因此，相比 sample-pair-based 的形式，我们实际上只需要对每张图片计算一次前向和后向即可，节省了大量重复计算。 样本生成 最后，在训练样本对的生成上，论文采用如下方法： 每次迭代时，先随机挑选 K 个目录（每个目录都包含两个不同的域），然后从每个目录的两个域中，分别随机挑选 \\(O_1\\) 和 \\(O_2\\) 张图片。接着，对第一个域中的每张图片 \\(A\\)，从第二个域中随机挑选若干图片和它组成样本对，注意，从第二个域中挑选的图片，有一半和 \\(A\\) 属于同一目录，另一半与 \\(A\\) 属于不同目录。 参考 Cross-Domain Visual Matching via Generalized Similarity Measure and Feature Learning","raw":null,"content":null,"categories":[{"name":"计算机视觉","slug":"计算机视觉","permalink":"https://jermmy.github.io/categories/计算机视觉/"}],"tags":[{"name":"计算机视觉","slug":"计算机视觉","permalink":"https://jermmy.github.io/tags/计算机视觉/"},{"name":"深度学习","slug":"深度学习","permalink":"https://jermmy.github.io/tags/深度学习/"},{"name":"论文","slug":"论文","permalink":"https://jermmy.github.io/tags/论文/"}]},{"title":"word2vec概述","slug":"2017-11-3-word2vec-introduction","date":"2017-11-03T08:28:37.000Z","updated":"2017-12-13T02:53:09.000Z","comments":true,"path":"2017/11/03/2017-11-3-word2vec-introduction/","link":"","permalink":"https://jermmy.github.io/2017/11/03/2017-11-3-word2vec-introduction/","excerpt":"既然是概述，那么我也只会在文中谈一点关于 Word2Vec 的思想和大概的方法。对于这个算法，如果一开始学习就深入到算法细节中，反而会陷入局部极值点，最后甚至不知道这个算法是干嘛的。在了解算法大概的思路后，如果有进一步研究的必要，再去深究算法细节，这时一切都是水到渠成的。\n先申明，由于我不是做 NLP 相关的，因此本文参考的主要是文末提供的博客，在算法理解上有很多不成熟的地方，还请见谅。","text":"既然是概述，那么我也只会在文中谈一点关于 Word2Vec 的思想和大概的方法。对于这个算法，如果一开始学习就深入到算法细节中，反而会陷入局部极值点，最后甚至不知道这个算法是干嘛的。在了解算法大概的思路后，如果有进一步研究的必要，再去深究算法细节，这时一切都是水到渠成的。 先申明，由于我不是做 NLP 相关的，因此本文参考的主要是文末提供的博客，在算法理解上有很多不成熟的地方，还请见谅。 什么是Word2Vec Word2Vec，顾名思义，就是把一个 word 变成一个 vector。其实，早在 Word2Vec 出来之前，就已经有很多人思考这个问题了。而用的最多的方法大概是 tf-idf 这类基于频率的方法。不过这类方法有什么问题呢？一个很明显的缺陷是，这些方法得到的向量是没有语义的，比如说，对于「苹果」、「香蕉」、「可乐」这三个词来说，「苹果」和「香蕉」表示的向量应该比「可乐」更加相似，这种相似有很多衡量方法（比如「欧式距离」或「余弦相似性」），但用频率的方法是很难体现这种相似性的。而 Word2Vec 就是为了解决这种问题诞生的。 Word2Vec 是 Google 的 Tomas Mikolov 及其团队于 2013 年创造的，正如之前提到的，这个模型是为了让 word 生成的向量能体现语义信息。它是一种基于预测的模型。 基本思想 在了解 Word2Vec 的基本思想之前，我们先思考一个问题：什么是词的语义？这是一个很难回答的问题，因此，我们降低一下难度：如何判断两个词的语义是相似的？好吧，这依然不好回答。那么，我们继续简单点想：两个语义相似的词会有什么特点呢？它们可能词性相同（比如都是名词或形容词），可能在句子中出现的位置很类似等等。 比如，我们看一个例子：「The quick brown fox jumps over the lazy dog.」。现在，我们聚焦到「fox」这个词，如果要把它换成其他词，要怎么做呢？一个最简单的方法是，我们去搜索其他句子，看其他句子中是否也有「brown __ jumps”」这样的格式出现，如果出现了，则下划线代表的词很可能是可以替换「fox」的。而根据常识，会出现在形容词之后和动词之前的词，一般来说是个名词，因此，我们这种替换的思路也是有一定道理的，起码一个名词和「fox」之间的相似性，相比动词或副词会更大一些。这里的替换其实就包含一点语义，只有两个词语义相似，才能相互替换。当然，这种相似显得稍微粗糙了一点（毕竟不同的名词或动词之间也是相差万别），但是，如果语料库足够丰富，我们还是可以进一步学习出不同名词之间的差别的。比如，对于这样的句子：「The man plays basketball.”」 ，如果要替换「man」这个词，我们发现，由于有「plays basketball」的限制，就不是什么名词都可以派上用场了。同样地，我们扫描其他句子，找出有类似结构「The __ plays basketball」的句子，然后将下划线的词和「man」做对比，这时，我们会发现，这样的词更多的会是一个表示人类的名词，而不在是「cat」等其他名词。因此，只要语料库够大，我们对相似度的判断粒度也会更细。 好了，以上这个例子，其实就是 Word2Vec 的基本思想了。 Word2Vec 主要就是利用上下文信息，或者更具体一点，与一个词前后相邻的若干个词，来提取出这个词的特征向量。 方法 为了利用这种上下文信息，Word2Vec 采用了两种具体的实现方法，分别是 CBOW 和 Skip-grams。这两种方法本质上是一样的，都是利用句子中相邻的词，训练一个神经网络。它们各有优劣，因此各自实现的 Word2Vec 的效果也各有千秋。 需要注意的是，由于语料库是没有任何标记的，因此这种方法是一种无监督学习的方法，而我们训练的这个网络，最终并不是想用它来输出特征，而是将网络的参数作为文本的特征（类似 auto-encoder）。 下面，我分别简单介绍一下 CBOW 和 Skip-grams。 CBOW(Continuous Bag of Words) CBOW 采用给定上下文信息来预测一个词的战术来训练神经网络。 用一个例子来说明 CBOW 的过程。假设现在我们的语料库只有一句话：“The quick brown fox jumps over the lazy dog”。算法中有一个参数 window，表示依赖的上下文数量，如果 window 设为 1，则只依赖左右一个单词，比如，对于「fox」这个词，对应的上下文就是「brown」和「jumps」，一般来说，这个值越大，准确性越高，计算量也会越大。现在，我们把 window 设为 2，这样，对于上面的句子，可以分解成下面这些训练的样本对： 当然，文本是没法直接用于训练的，通常，我们还需要把这些训练的文本向量化。这一步向量化可以采用多种方法，比如直接用 one-hot 向量来表示。这样一来，训练对就变成了向量对，我们输入网络的是左边的向量，输出的则是右边的向量。 在正式训练之前，有必要先讲一下 CBOW 的网络结构。 如上图所示，CBOW 的网络结构其实非常简单，Input layer 是训练对中左边的向量。Hidder layer 则是 Input layer 线性组合而成的，换句话说，中间的隐藏层没有激活函数。需要注意的是，Output layer 并不直接就是输出向量，而是经过 Softmax 函数后得到的向量，我们知道，Softmax 得到的向量表示一连串的概率分布，因此，网络要学习的任务，就是让概率最大的位置，对应到输出向量 one-hot 中值为 1 的位置。 再具体一点，假设我们语料库中的词的数目是 \\(V\\)，隐藏层的神经元数目是 \\(N\\)，那么，输入向量就是一个 \\([V \\times 1]\\) 的向量，输入层与隐藏层之间有一个 \\([N \\times V]\\) 的权重矩阵，隐藏层是一个 \\([N \\times 1]\\) 的向量，隐藏层和输出层之间有一个 \\([V \\times N]\\) 的向量，输出层是一个 \\([V \\times 1]\\) 的向量。 对于代价函数而言，由于我们最后的激活层是 Softmax，因此对应的一般用 \\(log-likelihood\\) 函数作为 cost function：\\(-\\log{(p())}\\)，其中 \\(p()\\) 指的就是 Softmax 函数，它的具体形式这里就不展开了。 好了，讲完 CBOW 的网络结构后，接下来要看看如何用那些 training samples 来训练网络了。 首先要强调一下，CBOW 的目的是要用 context 的内容来预测单词，如果只是简单地把训练样本对「喂」给上面所示的神经网络，那么我们用到的上下文信息只有一个相邻的单词而已。为了更好地利用 context 的内容，CBOW 实际上会把所有的邻居单词都输入到网络中。比如，对于例子中的第二个样本对 \\([(the, quick), (brown, quick), (fox, quick)]\\)，我们是这样输入到网络中的： 隐藏层那里，我们把三个输入向量前向后得到的隐藏层向量平均了一下，得到最终的隐藏层，之后的步骤和单个输入的情况是一致的。 这样不断训练优化参数后，最后得到的网络就可以根据输入的上下文信息，猜出对应的单词是哪个了。而我们要的 vector 向量其实就是隐藏层和输出层之间的权重矩阵 Hidden-Ouput Matrix。这是一个 \\(V \\times N\\) 的矩阵，矩阵的每一行代表的就是一个词的 vector，这个 vector 的维度是 \\(N\\)，刚好就是我们定义的隐藏层神经元的数目。你也可以把这个矩阵当作一个词典 根据输出层对应的 one-hot 编码，在词典对应的行中找到特征向量即可。比如，如果要找「fox」的特征，由于它的 one-hot 为：\\([0, 0, 0, 1, 0, 0, 0, 0, 0]\\)，因此，词典中第四行对应的向量就是「fox」的特征向量。 好了，我们已经把最简单版本的 CBOW 讲完了，它最主要的思想，其实就是把上下文的向量平均一下（上面例子中的 \\(Average\\ Activation\\)），再用这个向量去预测出输出向量（即在 \\(Hidden-Output\\ Matrix\\) 中 match 一个特定的行向量，使得到的概率值最大）。因此，我们可以认为，一个词最终得到的向量，其实是它所有上下文向量的平均效果。比如，对于「Apple」这个词来说，由于它既可以是一种水果，也可以是一个公司，因此，「Apple」这个词对应的向量可能会在水果和公司这些词向量的聚类集合之间。 Skip-grams 与 CBOW 相反，Skip-grams 采用给定一个词来预测上下文的战术来训练神经网络。 假如还是之前的例子 “The quick brown fox jumps over the lazy dog”。同样将 window 的值设为 2，这次我们会分解出下面这些训练的样本对： 和 CBOW 很类似，只不过训练样本中，输入和输出对调了位置。 Skip-grams 的网络结构和 CBOW 是一样的，只不过训练的时候，我们输入的是一个向量，而输出则对应多个向量： 我们再次用矩阵的形式看看这个过程（这里偷个懒，我就直接盗参考链接的图了）： skip-grams 这个图里只包括了隐藏层到输出层的部分，前面输出层到隐藏层的部分和一般的网络是一样的。 注意到，我们这里同样将隐藏层的神经元数目设为 4。 在 Output 的时候，根据输入单词对应的上下文单词数量，我们对应的要输出相同数量的向量，上图中，输出的向量个数是 2。当然，在一次前向中，这两个输出向量是相同的，不过，我们要的其实是用于反向传播的误差。比如，这两个输出向量对应的词是不同的，因此，对应的 Target 向量（也就是 one-hot）也是不同的，这样，用各自对应的 Target 向量减去 Softmax 得到的向量，我们可以得到两个不同的误差，然后，把这两个误差加起来作为总的误差，再反向传播回去。 这个过程和 CBOW 有相似之处，在 CBOW 中，我们平均的是上下文向量的隐藏层，而 Skip-grams 平均的是上下文向量的误差。所以，我们可以简单地想，Skip-grams 其实也是在平均上下文向量，并依此得到我们要的词向量。 对应的，Skip-grams 的词典是输入层和隐藏层之间的权重矩阵，和 CBOW 完全反着来。 总结 写到最后，我发现这篇文章其实主要是在讲 CBOW 和 Skip-grams 的思想，稍稍有点偏题了。 其实，重要的还是这两个方法中用到的基于上下文预测语义的思想，这一点和 Word2Vec 是一致的。当然啦，由于我并没有很深入地去了解这两个算法，所以也只能草草讲一下大概的思路，有一些具体的优化策略和实现方法都没有提及，比如，如何改进 one-hot 这种很耗内存的表示方法，如何减少网络参数等，这些内容也是算法的精髓。不过，如果已经知道算法的总体战略思想，对于这些具体的战术，多看几篇文章应该也不是什么问题。 废话到此为止～囧～ 参考 Word2Vec Tutorial - The Skip-Gram Model Word2Vec Tutorial Part 2 - Negative Sampling An Intuitive Understanding of Word Embeddings: From Count Vectors to Word2Vec Word2vec","raw":null,"content":null,"categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://jermmy.github.io/categories/机器学习/"}],"tags":[{"name":"深度学习","slug":"深度学习","permalink":"https://jermmy.github.io/tags/深度学习/"},{"name":"NLP","slug":"NLP","permalink":"https://jermmy.github.io/tags/NLP/"}]},{"title":"GMM与EM共舞","slug":"2017-10-29-GMM-and-EM","date":"2017-10-29T02:27:15.000Z","updated":"2018-02-18T04:30:42.000Z","comments":true,"path":"2017/10/29/2017-10-29-GMM-and-EM/","link":"","permalink":"https://jermmy.github.io/2017/10/29/2017-10-29-GMM-and-EM/","excerpt":"GMM，即高斯混合模型（Gaussian Mixture Model），简单地讲，就是将多个高斯模型混合起来，作为一个新的模型，这样就可以综合运用多模型的表达能力。EM，指的是均值最大化算法（expectation-maximization），它是一种估计模型参数的策略，在 GMM 这类算法中应用广泛，因此，有时候人们又喜欢把 GMM 这类可以用 EM 算法求解的模型称为 EM 算法家族。\n这篇文章会简单提一下 GMM 模型的内容，最主要的，还是讲一下 EM 算法如何应用到 GMM 模型的参数估计上。\n\n\n","text":"GMM，即高斯混合模型（Gaussian Mixture Model），简单地讲，就是将多个高斯模型混合起来，作为一个新的模型，这样就可以综合运用多模型的表达能力。EM，指的是均值最大化算法（expectation-maximization），它是一种估计模型参数的策略，在 GMM 这类算法中应用广泛，因此，有时候人们又喜欢把 GMM 这类可以用 EM 算法求解的模型称为 EM 算法家族。 这篇文章会简单提一下 GMM 模型的内容，最主要的，还是讲一下 EM 算法如何应用到 GMM 模型的参数估计上。 高斯混合模型 什么是 GMM GMM 可以认为是 K-means 算法的升级版。在 K-means 中，我们会先计算出几个聚类中心，然后根据数据点与聚类中心的距离，直接将数据点归类到最近的聚类中心。这种做法其实很“硬”，因为有很多边缘点属于两个聚类中心的概率可能相差不大，如果一股脑就直接将它归到某一个中心，实在是太粗暴了。而 GMM 不同于 K-means 的地方就在于，它除了给出聚类中心外，还能告诉你每个点归属于某个聚类中心的概率，因此，GMM 又被称作 soft assignment。 首先，还是给出 GMM 模型的公式： \\[ p( x)=\\sum_{k=1}^K{\\pi_k N( x|\\mu_k, \\Sigma_k)} \\] 其中，我们规定，\\(\\sum_{k=1}^K{\\pi_k}=1\\)。可以看出，GMM 就是将几个高斯模型线性组合起来，人们习惯上把这里面的各个高斯模型称为 Component。其中，\\(\\pi_k\\) 表示每个模型的占比，或者说数据属于模型 k 的概率，这个值越大，说明聚集在这个模型内的数据越多。 为什么要用这种模型组合的方式呢？我们知道，高斯模型一般成椭圆状（二维）或椭球状（三维），可以把这个椭圆或椭球认为是一种聚类的形状，而圆心或球心则是聚类中心（对应高斯函数的参数 \\(\\mu\\)）。但真实世界中，数据的分布并不一定都是按这样的形状分布的（如上面给出的图），因此，一个高斯模型可能没法很好的拟合这些数据，而如果能综合考虑几个高斯模型的表达能力，让这些模型发挥所长，不同的模型拟合不同的数据，这样一来，所有数据就可以很好地被这个「组合模型」拟合起来。 其实，这种组合模型的思路可以应用到很多模型上，比如：泊松模型。而由于高斯模型本身一些良好的性质，因此 GMM 这种模型被用的比较多。 前面说到，GMM 本质上是一种聚类算法，那么，如果已知一个 GMM 模型，现在给定一个点，我们要怎么知道这个点属于哪个聚类中心呢？更具体一点说，怎么知道这个点属于每个聚类中心的概率是多少？ 用数学的语言表达就是，已知一个 GMM 模型： \\(p( x)=\\sum_{k=1}^K{\\pi_k N( x|\\mu_k, \\Sigma_k)}\\)，它的 K 个聚类中心为 \\(C_k\\)，现在要求概率值 \\(p( x \\in C_k | x)\\)。 求解的方法很简单，根据贝叶斯公式：\\(p(a|b)=\\frac{p(b|a)p(a)}{p(b)}\\)，我们可以得出： \\[ p( x \\in C_k | x)=\\frac{p(C_k)p( x| x\\in C_k)}{p( x)} \\] 因此，对于每个聚类中心 \\(C_k\\)，由于分母 \\(p( x)\\) 都是相同的，我们只需要计算 \\(p(C_k)p( x| x\\in C_k)=\\pi_k N( x|\\mu_k, \\Sigma_k)\\) 即可。得到的值就是数据点 $ x$ 属于 \\(C_k\\) 的概率，至于具体要将 $ x$ 归类到哪个中心，可以根据具体情况决定，比如将概率最大的作为归属的聚类中心。这一点也是 GMM 优于 K-means 的地方，前者是通过概率的方式来决定归属，因此提供了更加丰富的信息。 参数估计 不过，GMM 模型最难的地方在于，如何根据一堆数据点估计出模型的参数？ GMM 需要确定的参数有三类： 高斯模型的个数 \\(K\\)，这个参数跟 K-means 的 \\(K\\) 一样，需要人工事先设定，\\(K\\) 越大，聚类的粒度也越细； \\(\\pi_k\\)， 每个 Component 的概率分量，或者说在总样本中的占比； \\(\\mu_k\\)、\\(\\Sigma_k\\)，各个 Component 的参数。 如果样本所属分类已知（即已知 \\(x\\) 属于哪个 \\(C_k\\)），那 GMM 的参数就很容易确定了。首先，参数 \\(K\\) 就一目了然得到了。然后，假设样本容量为 \\(N\\)，归属于聚类中心 \\(C_k\\) 的样本数量为 \\(N_k\\)，归属每个 \\(C_k\\) 的样本集合为 \\(S(k)\\)，可以用以下公式求出其他参数： \\[ \\pi_k=\\frac{N_k}{N} \\\\ \\mu_k=\\frac{1}{N_k}\\sum_{ x\\in S(k)}{ x} \\\\ \\Sigma_k=\\frac{1}{N_k}\\sum_{ x\\in S(k)}{( x-\\mu_k)( x-\\mu_k)^T} \\] 其实，这跟一个高斯模型的情况是一样的，只不过要依葫芦画瓢求出 \\(K\\) 个。 但如果样本的分类事先不知道，又该怎么办呢？首先，由于 \\(K\\) 这个值是需要人工确定的，所以这里暂时假设 \\(K\\) 已经知道了。现在，我们要预测 \\(K\\) 个高斯模型的概率分量 \\(\\pi_k\\) 以及每个模型各自的参数 \\(\\mu_k\\) 和 \\(\\Sigma_k\\)。 最简单也最容易想到的方法是极大似然估计。假设有 m 个样本，首先，写出 \\(p( x)=\\sum_{k=1}^K{\\pi_k N( x|\\mu_k, \\Sigma_k)}\\) 的似然函数： \\[ \\begin{eqnarray} \\ln{[\\prod_{i=1}^m p( x_i)]}&amp;=&amp;\\ln{[\\prod_{i=1}^m{\\sum_{k=1}^K{\\pi_k N( x|\\mu_k, \\Sigma_k)}}]} \\\\ &amp;=&amp;\\sum_{i=1}^m{\\ln{[\\sum_{k=1}^K{\\pi_k N( x|\\mu_k, \\Sigma_k)]}}} \\\\ \\end{eqnarray} \\] 不过，这个对数函数却出奇的复杂，直接求导数的方法很难解出 \\(\\pi_k\\)、\\(\\mu_k\\) 和 \\(\\Sigma_k\\)，因此，我们只能换用其他方式来求解。而这就是 EM 算法发挥作用的地方。 （其实求出偏导后，再用随机梯度下降的方法也是可以解出来的，比如 Mixture Density Network 就是这么处理的。不过，在传统的机器学习中，人们习惯用 EM 来求解） 均值最大化算法 EM K-means 的启示 在正式开讲 EM 之前，我们先回忆一下，K-means 是怎么求出聚类中心的。其实，总共分三步进行： 随机初始化 K 个聚类中心的位置； 将所有样本点，按照跟各个聚类中心的距离进行归类； 根据样本重新归类的结果，更新聚类中心的位置，重复步骤 2 直到收敛（即聚类中心重新调整的幅度小于某个阈值）。 既然 GMM 本身也属于一种聚类算法，那么，我们能不能用 K-means 的思路来求出 GMM 的参数呢？答案当然是可以的。 不过，在这之前，我们需要先知道 GMM 的几个参数（\\(\\pi_k\\)，\\(\\mu_k\\)，\\(\\Sigma_k\\)）要怎么计算。假设我们已经知道了后验概率 \\(P( x \\in C_k| x)\\)，则可以根据以下公式计算参数（其中，m 表示样本数量）： \\[ \\pi_k=\\frac{1}{n}\\sum_{i=1}^m{P( x_i\\in C_k|x_i)} \\tag{3} \\] 这个公式是把所有样本属于 \\(C_k\\) 的概率求平均后，作为 \\(C_k\\) 这个聚类中心（或者说这个高斯模型）的出现概率。 \\[ \\mu_k=\\frac{\\sum_{i=1}^m{ xP(x_i\\in C_k|x_i)}}{\\sum_{i=1}^m{P(x_i\\in C_k|x_i)}} \\tag{4} \\] 这个求均值的公式，跟单个高斯模型不同的地方在于，我们用的是加权平均。因为每个样本点都有一定的概率属于聚类中心 \\(C_k\\)，所以，每个样本对 \\(C_k\\) 对应的高斯模型的均值也会产生一定的作用，只是由于 \\(P(x_i\\in C_k|x_i)\\) 的值不同，因此这种作用也会有显著差别。 \\[ \\Sigma_k=\\frac{\\sum_{i=1}^m{P(x_i\\in C_k|x_i)(x_i-\\mu_k)(x_i-\\mu_k)^T}}{\\sum_{i=1}^m{P(x_i\\in C_k|x_i)}} \\tag{5} \\] 类似地，协方差也是用加权平均求出来的。 （公式 (3) (4) (5) 其实是从极大似然函数推出来的，在周志华老师的西瓜书和PRML书中都有详细推导，不过这里我只想给出感性的认识） 不过，以上公式都是基于 \\(P( x \\in C_k| x)=\\frac{p(C_k)p( x| x\\in C_k)}{p( x)}\\)计算出来的，而这个公式本身又需要知道 \\(P(C_k)\\)（即 \\(\\pi_k\\)）等参数，这就陷入一个鸡生蛋还是蛋生鸡的怪圈。 但是，借助 K-means 的思路，我们可以事先随机初始化这些参数，然后计算出 \\(P( x \\in C_k| x)\\)，再用它更新 GMM 参数，然后再用更新的模型计算 \\(P( x \\in C_k| x)\\)，如此迭代下去，总有收敛的时候，这样，我们不就可以像 K-means 一样计算出参数了吗？！ 下面，我们就仿照 K-means 的方法，给出迭代计算 GMM 参数的步骤： 随机初始化各个高斯模型的参数； 根据参数，计算 \\(P( x \\in C_k| x)\\)，这一步其实是计算出每一个样本归属于每一个聚类中心的概率； 根据第 2 步计算得到的 \\(P( x \\in C_k| x)\\)，按照公式 (3) (4) (5) 重新计算 GMM 参数，并重复步骤 2 直到收敛。 EM 算法 其实，上面仿照 K-means 算法的计算步骤，就是 EM 算法的核心部分了。 EM 算法主要分为 E 和 M 两步： E 指的是 Expectation，即计算均值的过程，对应的是上面的步骤 2，这一步主要是计算每个样本归属的聚类中心； M 指的是 Maximum，即对参数的最大似然估计，对应的是上面的步骤 3。我前面也说了，公式 (3) (4) (5) 计算参数的公式是用最大似然函数推出来的，所以，这一步其实是在根据步骤 2 的分类结果，重新用最大似然函数来估计参数。 下面这幅图是从西瓜书上截下来的，是 EM 算法求解 GMM 参数的完整过程。 图中有很多公式标记，可能要参考原书才看得懂，不过，它的流程和我之前给出的 3 个步骤是一致。另外，算法的停止条件可以是达到最大迭代次数，或者是似然函数（公式 (2)）的增长小于某个阈值。 好了，本文到这里就不再深入下去了。EM 算法博大精深，吴军老师在《数学之美》称它为上帝的算法，可见这个算法的强大之处。EM 算法可以应用的场合非常多，从本文给出的 GMM 例子来看，它其实很类似梯度下降算法，在给定的目标函数很复杂、难以求解时，EM 算法用一种迭代的策略来优化初始参数，并逐步收敛到最优解。 参考 漫谈 Clustering (3): Gaussian Mixture Model Mixture of Gaussian clustering EM及高斯混合模型 机器学习西瓜书 PRML","raw":null,"content":null,"categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://jermmy.github.io/categories/机器学习/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"https://jermmy.github.io/tags/机器学习/"}]},{"title":"多维高斯分布","slug":"2017-10-28-mutil-modal-gaussian","date":"2017-10-28T03:20:47.000Z","updated":"2019-03-19T14:04:46.000Z","comments":true,"path":"2017/10/28/2017-10-28-mutil-modal-gaussian/","link":"","permalink":"https://jermmy.github.io/2017/10/28/2017-10-28-mutil-modal-gaussian/","excerpt":"高中的时候我们便学过一维正态（高斯）分布的公式： \\[\nN(x|u,\\sigma^2)=\\frac{1}{\\sqrt{2\\pi \\sigma^2}}exp[-\\frac{1}{2\\sigma^2}(x-u)^2]\n\\] 拓展到高维时，就变成： \\[\nN(\\overline x | \\overline u, \\Sigma)=\\frac{1}{(2\\pi)^{D/2}}\\frac{1}{|\\Sigma|^{1/2}}exp[-\\frac{1}{2}(\\overline x-\\overline u)^T\\Sigma^{-1}(\\overline x-\\overline u)]\n\\] 其中，\\(\\overline x\\) 表示维度为 D 的向量，\\(\\overline u\\) 则是这些向量的平均值，\\(\\Sigma\\) 表示所有向量 \\(\\overline x\\) 的协方差矩阵。\n本文只是想简单探讨一下，上面这个高维的公式是怎么来的。","text":"高中的时候我们便学过一维正态（高斯）分布的公式： \\[ N(x|u,\\sigma^2)=\\frac{1}{\\sqrt{2\\pi \\sigma^2}}exp[-\\frac{1}{2\\sigma^2}(x-u)^2] \\] 拓展到高维时，就变成： \\[ N(\\overline x | \\overline u, \\Sigma)=\\frac{1}{(2\\pi)^{D/2}}\\frac{1}{|\\Sigma|^{1/2}}exp[-\\frac{1}{2}(\\overline x-\\overline u)^T\\Sigma^{-1}(\\overline x-\\overline u)] \\] 其中，\\(\\overline x\\) 表示维度为 D 的向量，\\(\\overline u\\) 则是这些向量的平均值，\\(\\Sigma\\) 表示所有向量 \\(\\overline x\\) 的协方差矩阵。 本文只是想简单探讨一下，上面这个高维的公式是怎么来的。 二维的情况 为了简单起见，本文假设所有变量都是相互独立的。即对于概率分布函数 \\(f(x_0,x_1,…,x_n)​\\) 而言，有 \\(f(x_0,x_1,…,x_n)=f(x_0)f(x_1)f(x_n)​\\) 成立。 现在，我们用一个二维的例子推出上面的公式。 假设有很多变量 \\(\\overline x=\\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix}​\\)，它们的均值为 \\(\\overline u=\\begin{bmatrix} u_1 \\\\ u_2 \\end{bmatrix}​\\)，方差为 \\(\\overline \\sigma=\\begin{bmatrix} \\sigma_1 \\\\ \\sigma_2 \\end{bmatrix}​\\)。 由于 \\(x_1\\)，\\(x_2\\) 是相互独立的，所以，\\(\\overline x\\) 的高斯分布函数可以表示为： \\[ \\begin{eqnarray} f(\\overline x) &amp;=&amp; f(x_1,x_2) \\\\ &amp;=&amp; f(x_1)f(x_2) \\\\ &amp;=&amp; \\frac{1}{\\sqrt{2\\pi \\sigma_1^2}}exp(-\\frac{1}{2}(\\frac{x_1-u_1}{\\sigma_1})^2) \\times \\frac{1}{\\sqrt{2\\pi \\sigma_2^2}}exp(-\\frac{1}{2}(\\frac{x_2-u_2}{\\sigma_2})^2) \\\\ &amp;=&amp;\\frac{1}{(2\\pi)^{2/2}(\\sigma_1^2 \\sigma_2^2)^{1/2}}exp(-\\frac{1}{2}[(\\frac{x_1-u_1}{\\sigma_1})^2+(\\frac{x_2-u_2}{\\sigma_2})^2]) \\end{eqnarray} \\] 接下来，为了推出文章开篇的高维公式，我们要想办法得到协方差矩阵 \\(\\Sigma\\)。 对于二维的向量 \\(\\overline x\\) 而言，其协方差矩阵为： \\[ \\begin{eqnarray} \\Sigma&amp;=&amp;\\begin{bmatrix} \\sigma_{11} &amp; \\sigma_{12} \\\\ \\sigma_{12} &amp; \\sigma_{22} \\end{bmatrix} \\\\ &amp;=&amp;\\begin{bmatrix} \\sigma_1^2 &amp; \\sigma_{12} \\\\ \\sigma_{21} &amp; \\sigma_{2}^2 \\end{bmatrix} \\\\ \\end{eqnarray} \\] (不熟悉协方差矩阵的请查找其他资料或翻看我之前的文章) 由于 \\(x_1\\)，\\(x_2\\) 是相互独立的，所以 \\(\\sigma_{12}=\\sigma_{21}=0\\)。这样，\\(\\Sigma\\) 退化成 \\(\\begin{bmatrix} \\sigma_1^2 &amp; 0 \\\\ 0 &amp; \\sigma_{2}^2 \\end{bmatrix}\\)。 则 \\(\\Sigma\\) 的行列式 \\(|\\Sigma|=\\sigma_1^2 \\sigma_2^2\\)，代入公式 (4) 就可以得到： \\[ f(\\overline x)=\\frac{1}{(2\\pi)^{2/2}|\\Sigma|^{1/2}}exp(-\\frac{1}{2}[(\\frac{x_1-u_1}{\\sigma_1})^2+(\\frac{x_2-u_2}{\\sigma_2})^2]) \\] 这样一来，我们已经推出了公式的左半部分，下面，开始处理右面的 \\(exp\\) 函数。 原始的高维高斯函数的 \\(exp\\) 函数为：\\(exp[-\\frac{1}{2}(\\overline x-\\overline u)^T\\Sigma^{-1}(\\overline x-\\overline u)]\\)，根据前面算出来的 \\(\\Sigma\\)，我们可以求出它的逆：\\(\\Sigma^{-1}=\\frac{1}{\\sigma_1^2 \\sigma_2^2} \\begin{bmatrix} \\sigma_2^2 &amp; 0 \\\\ 0 &amp; \\sigma_1^2 \\end{bmatrix}\\)。 接下来根据这个二维的例子，将原始的 \\(exp()\\) 展开： \\[ \\begin{eqnarray} exp[-\\frac{1}{2}(\\overline x-\\overline u)^T\\Sigma^{-1}(\\overline x-\\overline u)] &amp;=&amp; exp[-\\frac{1}{2} \\begin{bmatrix} x_1-u_1 \\ \\ \\ x_2-u_2 \\end{bmatrix} \\frac{1}{\\sigma_1^2 \\sigma_2^2} \\begin{bmatrix} \\sigma_2^2 &amp; 0 \\\\ 0 &amp; \\sigma_1^2 \\end{bmatrix} \\begin{bmatrix} x_1-u_1 \\\\ x_2-u_2 \\end{bmatrix}] \\\\ &amp;=&amp;exp[-\\frac{1}{2} \\begin{bmatrix} x_1-u_1 \\ \\ \\ x_2-u_2 \\end{bmatrix} \\frac{1}{\\sigma_1^2 \\sigma_2^2} \\begin{bmatrix} \\sigma_2^2(x_1-u_1) \\\\ \\sigma_1^2(x_2-u_2) \\end{bmatrix}] \\\\ &amp;=&amp;exp[-\\frac{1}{2\\sigma_1^2 \\sigma_2^2}[\\sigma_2^2(x_1-u_1)^2+\\sigma_1^2(x_2-u_2)^2]] \\\\ &amp;=&amp;exp[-\\frac{1}{2}[\\frac{(x_1-u_1)^2}{\\sigma_1^2}+\\frac{(x_2-u_2)^2}{\\sigma_2^2}]] \\end{eqnarray} \\] 展开到最后，发现推出了公式 (4)。说明原公式 \\(N(\\overline x | \\overline u, \\Sigma)=\\frac{1}{(2\\pi)^{D/2}}\\frac{1}{|\\Sigma|^{1/2}}exp[-\\frac{1}{2}(\\overline x-\\overline u)^T\\Sigma^{-1}(\\overline x-\\overline u)]\\) 是成立的。你也可以将上面展开的过程逆着推回去，一样可以从例子中的公式 (4) 推出多维高斯公式。 函数图像 知道多维的公式后，下面再简单比较一下一维和二维的图像区别。 上图展示的是 4 个一维高斯函数的图像。高斯函数是一个对称的山峰状，山峰的中心是均值 \\(u\\)，山峰的「胖瘦」由标准差 \\(\\sigma\\) 决定，如果 \\(\\sigma\\) 越大，证明数据分布越广，那么山峰越「矮胖」，反之，则数据分布比较集中，因此很大比例的数据集中在均值附近，山峰越「瘦高」。在偏离均值 \\(u\\) 三个 \\(\\sigma\\) 的范围外，数据出现的概率几乎接近 0，因此这一部分的函数图像几乎与 x 轴重合。 下面看二维的例子： 有了一维图像的例子，二维图像就可以类比出来了。如果说，一维只是山峰的一个横截面，那么二维则是一个完整的有立体感的山峰。山峰的「中心」和「胖瘦」和一维的情况是一致的，而且，对于偏离中心较远的位置，数据出现的概率几乎为 0，因此，函数图像在这些地方就逐渐退化成「平原」了。 参数估计 另外，如果给定了很多数据点，并且知道它们服从某个高斯分布，我们要如何求出高斯分布的参数（\\(\\mu\\) 和 \\(\\Sigma\\)）呢？ 当然，估计模型参数的方法有很多，最常用的就是极大似然估计。 简单起见，拿一维的高斯模型举例。假设我们有很多数据点：\\((x_1, x_2, x_3, \\dots, x_m)\\)，它们的均值是\\(\\tilde u\\)。一维高斯函数是：\\(p(x|\\mu, \\sigma^2)=\\frac{1}{\\sqrt{2\\pi}\\sigma}exp(-\\frac{(x-\\mu)^2}{2\\sigma^2})\\) 首先，我们先写出似然函数： \\[ \\begin{eqnarray} f(x_1, x_2, \\dots, x_m)&amp;=&amp;\\prod_{i=1}^{m}\\frac{1}{\\sqrt{2\\pi}\\sigma}exp(-\\frac{(x_i-\\tilde \\mu)^2}{2\\sigma^2}) \\\\ &amp;=&amp;(2\\pi \\sigma^2)^{-\\frac{m}{2}}exp(-\\frac{\\sum_{i=1}^n{(x_i-\\tilde \\mu)^2}}{2\\sigma^2}) \\end{eqnarray} \\] 然后取对数： \\[ \\ln{f(x_1, x_2, \\dots, x_m)}=-\\frac{m}{2}\\ln{(2\\pi \\sigma^2)}-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n{(x_i-\\tilde \\mu)^2} \\] 求出导数，令导数为 0 得到似然方程： \\[ \\frac{\\partial \\ln f}{\\partial \\overline \\mu}=\\frac{1}{\\sigma^2}\\sum_{i=1}^{n}{(x_i-\\tilde \\mu)}=0 \\] \\[ \\frac{\\partial \\ln{f}}{\\partial \\sigma}=-\\frac{m}{\\sigma}+\\frac{1}{\\sigma^3}\\sum_{i=1}^n{(x_i-\\tilde \\mu)}=0 \\] 我们可以求出：\\(\\mu=\\frac{1}{m}\\sum_{i=1}^m{(x_i-\\tilde \\mu)}\\)，\\(\\sigma=\\sqrt{\\frac{1}{m}\\sum_{i=1}^m{(x_i-\\tilde \\mu)^2}}\\)，可以看到，这其实就是高斯函数中平均值和标准差的定义。 对于高维的情况，平均值和协方差矩阵也可以用类似的方法计算出来。 总结 本文只是从一个简单的二维例子出发，来说明多维高斯公式的来源。在 PRML 的书中，推导的过程更加全面，也复杂了许多，想深入学习多维高斯模型的还是参考教材为准。 重新对比一维和多维的公式： \\[ N(x|u,\\sigma^2)=\\frac{1}{\\sqrt{2\\pi \\sigma^2}}exp[-\\frac{1}{2\\sigma^2}(x-u)^2] \\] \\[ N(\\overline x | \\overline u, \\Sigma)=\\frac{1}{(2\\pi)^{D/2}}\\frac{1}{|\\Sigma|^{1/2}}exp[-\\frac{1}{2}(\\overline x-\\overline u)^T\\Sigma^{-1}(\\overline x-\\overline u)] \\] 其实二者是等价的。一维中，我们针对的是一个数，多维时，则是针对一个个向量求分布。如果向量退化成一维，则多维公式中的 \\(D=1\\)，\\(\\Sigma=\\sigma^2\\)，\\(\\Sigma^{-1}=\\frac{1}{\\sigma^2}\\)，这时多维公式就退化成一维的公式。所以，在多维的公式中，我们可以把 \\(\\Sigma\\) 当作是样本向量的标准差。 参考 协方差矩阵 Gaussian function","raw":null,"content":null,"categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://jermmy.github.io/categories/机器学习/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"https://jermmy.github.io/tags/机器学习/"},{"name":"线性代数","slug":"线性代数","permalink":"https://jermmy.github.io/tags/线性代数/"}]},{"title":"图像中的傅立叶变换（三）","slug":"2017-10-15-fourier-transform-in-image-3","date":"2017-10-15T08:26:16.000Z","updated":"2017-11-25T13:31:38.000Z","comments":true,"path":"2017/10/15/2017-10-15-fourier-transform-in-image-3/","link":"","permalink":"https://jermmy.github.io/2017/10/15/2017-10-15-fourier-transform-in-image-3/","excerpt":"在之前的文章中，我们介绍了傅立叶变换的本质和很多基本性质，现在，该聊聊代码实现的问题了。\n为了方便起见，本文采用的编程语言是 Python3，矩阵处理用 numpy，图像处理则使用 OpenCV3。","text":"在之前的文章中，我们介绍了傅立叶变换的本质和很多基本性质，现在，该聊聊代码实现的问题了。 为了方便起见，本文采用的编程语言是 Python3，矩阵处理用 numpy，图像处理则使用 OpenCV3。 离散傅立叶变换 首先，回忆一下离散傅立叶变换的公式： \\[ \\begin{eqnarray} F(u, v)&amp;=&amp;\\frac{1}{MN}\\sum_{x=0}^{M-1}\\sum_{y=0}^{N-1}f(x, y)e^{-j2\\pi ux/M}e^{-j2\\pi vy/N} \\\\ &amp;=&amp;\\frac{1}{MN}\\sum_{y=0}^{N-1}\\lbrace \\sum_{x=0}^{M-1}f(x, y)e^{-j2\\pi ux/M}\\rbrace e^{-j2\\pi vy/N} \\end{eqnarray} \\] 从上式可以得到一个很有用的性质：可分性。即我们可以先计算 \\(\\sum_{x=0}^{M-1}f(x, y)e^{-j2\\pi ux/M}\\)，得到 \\(F(u,y)\\)，再计算 \\(\\frac{1}{MN}\\sum_{y=0}^{N-1} F(u,y) e^{-j2\\pi vy/N}\\) 得到 \\(F(u,v)\\)。 根据这种可分性，我们可以将二维的计算分为两个一维进行。 现在，考虑如何计算 \\(F(u,y)=\\sum_{x=0}^{M-1}f(x, y)e^{-j2\\pi ux/M}\\)，这个式子中的 \\(y\\) 可以当作是常数，所以这其实是关于 \\(x\\) 的一维运算。 根据这个式子，可以得到： \\[ F(0,y)=\\sum_{x=0}^{M-1}f(x,y)e^{-j2\\pi 0 x} \\\\ F(1,y)=\\sum_{x=0}^{M-1}f(x,y)e^{-j2\\pi x/M} \\\\ \\dots \\\\ F(M-1,y)=\\sum_{x=0}^{M-1}f(x,y)e^{-j2\\pi (M-1)x/M} \\] 我们完全可以用矩阵相乘的形式来表示这些式子： \\[ \\begin{bmatrix} F(0,y) \\\\ F(1,y) \\\\ \\dots \\\\ F(M-1,y) \\end{bmatrix} = \\begin{bmatrix} 1 &amp; 1 &amp; \\dots &amp; 1 \\\\ 1 &amp; W_M^{1} &amp; \\dots &amp; W_M^{M-1} \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots \\\\ 1&amp; W_{M}^{M-1} &amp; \\dots &amp; W_M^{(M-1)(M-1)} \\end{bmatrix} \\times \\begin{bmatrix} f(0,y) \\\\ f(1,y) \\\\ \\dots \\\\ f(M-1, y) \\end{bmatrix} \\] （式子中的 \\(W_M\\) 表示 \\(e^{-j2\\pi /M}\\)） 当然，由于图片是二维的，所以 \\(f(x,y)\\) 对应的向量实际上应该是： \\[ \\begin{bmatrix} f(0,y_1) &amp; f(0,y_2) &amp; \\dots &amp; f(0,y_N) \\\\ f(1,y_1) &amp; f(1,y_2) &amp; \\dots &amp; f(1,y_N) \\\\ \\dots \\\\ f(M-1, y_1) &amp; f(M-1,y_2) &amp; \\dots &amp; f(M-1,y_N) \\end{bmatrix} \\] 同理，得到的 \\(F(u,y)\\) 也是一个二维矩阵。 现在，我们还是先考虑怎么实现这个一维的计算。 首先，需要先把 \\(W_M\\) 这个矩阵表示出来。注意到，这个矩阵实际上可以由 \\(\\begin{bmatrix} W_M^0 \\\\ W_M^1 \\\\ \\vdots \\\\ W_M^{M-1} \\end{bmatrix}\\) \\(\\times\\) \\(\\begin{bmatrix} W_M^0 &amp; W_M^1 &amp; \\dots &amp; W_M^{M-1} \\end{bmatrix}\\) 得到。借助 numpy 强大的矩阵处理能力，可以很方便的计算出这个矩阵。示例如下： 123def dftmtx(M): n = np.asmatrix(np.arange(M)) return np.exp((-2j * np.pi / M) * n.transpose() * n) np.asmatrix 是把 M 维的向量变成 1 \\(\\times\\) M 的矩阵的格式，因为只有矩阵才有 transpose() 操作。np.exp 会把 \\(exp\\) 函数作用到矩阵的每个元素中。 得到这个矩阵后，最关键的一步其实就做完了，我们可以用这个矩阵计算出 \\(F(u,y)\\)： 123# input表示输入图像，M是图像的高M = input.shape[0]F = dftmtx(M) * input 得到 \\(F(u,y)\\) 后，剩下的是要对 y 这一维进行同样的操作：\\(F(u,v)=\\frac{1}{MN}\\sum_{y=0}^{N-1}F(u,y) e^{-j2\\pi vy/N}\\)。同样地，我们需要计算一个 \\(W_N\\) 的矩阵。幸运的是，这个矩阵的计算方法和之前的 \\(W_M\\) 一模一样，这样一来，我们已经可以得到完整的计算方法了： 1234# 傅立叶变换函数def dft2d(input): M, N = input.shape[0], input.shape[1] return dftmtx(M) * input * dftmtx(N) / (M * N) 接下来我们把频谱图打印出来。傅立叶频谱图是实部和虚部的平方和，需要注意的是，由于数值显示的问题，我们需要将频谱图用 log 函数增强后，再标定到 [0, 255] 之间才能看清。代码如下： 123456789101112131415161718# 将像素值标定到[0，255]之间def scale_intensity(image): min = image.min() max = image.max() image = (image - min) / (max - min) * 255.0 return image# 计算频谱图def spectrogram(image): dft = dft2d(image) spec = np.sqrt(np.power(np.real(dft), 2) + np.power(np.imag(dft), 2)) spec = np.log(0.5 + spec) * 10 spec = scale_intensity(spec) return spec image = cv2.imread(\"your_file.png\", cv2.IMREAD_GRAYSCALE)spec = spectrogram(image)cv2.imwrite(\"spec.png\", spec) 结果展示： 上一幅图是原图，下面的图是频谱图。如果仔细看的话，可以发现频谱图四个角有一些白色的点。这是因为图片中低频成分居多，而频谱图四个角代表的就是低频分量（至于为什么四个角是低频，我也没搞懂）。 实践中，人们习惯于把低频都聚集到图片中心，这样方便后续的操作。根据平移性质： \\[ F(u-\\frac{M}{2},v-\\frac{N}{2}) =f(x,y)(-1)^{x+y} \\] 要把频谱图的低频部分平移到中心，需要将整个频谱图平移 \\((M/2, N/2)\\) 个单位，也就是需要对原图乘以 \\((-1)^{x+y}\\)。代码如下： 123456789101112def shift_image(image): M, N = image.shape[0], image.shape[1] for x in range(M): for y in range(N): image[x, y] *= np.power(-1, x + y) return image image = cv2.imread(\"your_file.png\", cv2.IMREAD_GRAYSCALE)shift_image(image)spec = spectrogram(image)cv2.imwrite(\"shift_spec.png\", spec) 结果展示： 离散傅立叶反变换 讲完傅立叶变换后，反变换基本也得到了，唯一的区别是，这一次我们需要计算一个傅立叶反变换的矩阵。这个矩阵和之前计算的矩阵 \\(W_M\\) 的区别只在于符号，这里就直接给出代码了： 1234def idftmtx(M): n = np.asmatrix(np.arange(M)) # 下面的符号是正的 return np.exp((2j * np.pi / M) * n.transpose() * n) 反变换的代码如下： 123def idft2d(input): M, N = input.shape[0], input.shape[1] return idftmtx(M) * input * idftmtx(N) 把之前得到的傅立叶变换的结果，输入 idft2d 函数后，再取实部既可以得到原图： 1234image = cv2.imread(\"your_file.png\", cv2.IMREAD_GRAYSCALE)dft = dft2d(image)idft = idft2d(dft)cv2.imwrite(\"idft.png\", np.real(idft)) 结果如下： 这个反变换的结果和原图是略有差别的，因为傅立叶变换时舍弃了很多高频成分。不过，由于图片中高频成分本身就比较少，所以这点差别可以忽略不计。","raw":null,"content":null,"categories":[{"name":"图像处理","slug":"图像处理","permalink":"https://jermmy.github.io/categories/图像处理/"}],"tags":[{"name":"图像处理","slug":"图像处理","permalink":"https://jermmy.github.io/tags/图像处理/"}]},{"title":"图像中的傅立叶变换（二）","slug":"2017-10-10-fourier-transform-in-image-2","date":"2017-10-10T08:37:10.000Z","updated":"2017-10-28T03:10:33.000Z","comments":true,"path":"2017/10/10/2017-10-10-fourier-transform-in-image-2/","link":"","permalink":"https://jermmy.github.io/2017/10/10/2017-10-10-fourier-transform-in-image-2/","excerpt":"上一篇文章讲了傅立叶变换的本质。这篇文章会总结一下傅立叶变换的常用性质，公式巨多，慎入！慎入！\n\n\n","text":"上一篇文章讲了傅立叶变换的本质。这篇文章会总结一下傅立叶变换的常用性质，公式巨多，慎入！慎入！ 相关概念 首先，回顾一下傅立叶变换的公式： \\[ F(u)=\\frac{1}{M}\\sum_{x=0}^{M-1}f(x)e^{-2j\\pi (ux/M)} \\] 频谱(spectrum) 由上面的公式可以看出，傅立叶变换得到的系数 \\(F(u)\\) 是一个复数，因此可以表示为：\\(F(u)=R(u)+jI(u)\\)，其中，\\(R(u)\\) 是实部，\\(I(u)\\) 是虚部。傅立叶变换的频谱被定义为： \\[ |F(u)|=\\sqrt{R^2(u)+I^2(u)} \\] 相位谱(phase) 根据欧拉公式，我们知道 \\(R(u)\\) 代表的是一个余弦值，而 \\(I(u)\\) 则是正弦值。如果把 \\(F(u)\\) 看作一个向量 \\((R(u), I(u))\\)，则这个向量的夹角为 \\(\\phi(u)=\\arctan{[\\frac{I(u)}{R(u)}]}\\)。这个夹角也被称为相位谱。 能量谱(power) 能量谱其实就是频谱的平方：\\(P(u)=|F(u)|^2=R^2(u)+I^2(u)\\)。 常用性质 周期性 所谓周期性，即： \\[ F(u,v)=F(u+M,v)=F(u,v+N)=F(u+M,v+N) \\] 证明如下： \\[ \\begin{eqnarray} F(u+M,v+N)&amp;=&amp;\\frac{1}{MN}\\sum_{x=0}^{M-1}\\sum_{y=0}^{N-1}f(x,y)e^{-j2\\pi [(u+M)x/M+(v+N)/N]} \\\\ &amp;=&amp;\\frac{1}{MN}\\sum_{x=0}^{M-1}\\sum_{y=0}^{N-1}f(x,y)e^{-j2\\pi [(u+M)x/M+(v+N)y/N]} \\\\ &amp;=&amp;\\frac{1}{MN}\\sum_{x=0}^{M-1}\\sum_{y=0}^{N-1}f(x,y)e^{-j2\\pi (ux/M+vy/N)}e^{-j2\\pi (x+y)} \\end{eqnarray} \\] 注意，\\(e^{-j2\\pi (x+y)}={(e^{-j2\\pi})}^{x+y}=1^{(x+y)}=1\\)，所以 \\[ F(u+M,v+N)=\\frac{1}{MN}\\sum_{x=0}^{M-1}\\sum_{y=0}^{N-1}f(x,y)e^{-j2\\pi (ux/M+vy/N)}=F(u,v) \\] 类似地，可以推出 \\[ f(x,y)=f(x+M,y)=f(x,y+N)=f(x+M,y+N) \\] 共轭对称性 回忆一下，在复数域中，共轭指的是虚部取反。即 \\(z=x+jy\\) 的共轭是 \\(z*=x-jy\\)。 在傅立叶变换中，存在以下共轭对称性： \\[ F(u,v)=F*(-u,-v) \\] 证明如下： \\[ \\begin{eqnarray} F*(-u,-v)&amp;=&amp;\\frac{1}{MN}\\sum_{x=0}^{M-1}\\sum_{y=0}^{N-1}f(x,y)e^{ux/M+vy/N} \\\\ &amp;=&amp;\\frac{1}{MN}\\sum_{x=0}^{M-1}\\sum_{y=0}^{N-1}f(x,y)[\\cos{[2\\pi (ux/M+vy/N)]}-j\\sin{[2\\pi(ux/M+vy/N)]}] \\text{（注意共轭）} \\\\ &amp;=&amp;F(u,v) \\end{eqnarray} \\] 那么这个性质有什么用呢？注意，\\(|F*(-u,-v)|=|F(-u,-v)|\\)，换句话说，\\(|F(u,v)|=|F(-u,-v)|\\)。 要知道，\\(|F(u,v)|\\) 表示的是傅立叶频谱图，所以，共轭对称性表明，傅立叶的频谱图是中心对称的。 具体地，下图所示的傅立叶频谱图，四个对角上的能量是沿图片中心对称的。 平移性 平移性指的是： \\[ f(x-x_0,y-y_0) \\Leftrightarrow F(u,v)e^{-j2\\pi (ux_0/M+vy_0/N)} \\tag{1} \\] 这个等价关系的意思是说，如果原图 \\(f(x,y)\\) 平移了 \\((x_0,y_0)\\) 个单位，那么平移后的图像对应的傅立叶变换为 \\(F(u,v)e^{-j2\\pi (ux_0/M+vy_0/N)}\\)，即在原来 \\(F(u,v)\\) 的基础上乘上 \\(e^{-j2\\pi (ux_0/M+vy_0/N)}\\)。 这个公式的证明很简单。平移前的公式为： \\[ f(x,y)=\\sum_{u=0}^{M-1}\\sum_{v=0}^{N-1}F(u,v)e^{j2\\pi (ux/M+vy/N)} \\] 现在，原图的像素由 \\((x,y)\\) 平移到 \\((x-x_0,y-y_0)\\)，因此，我们只需要将 \\((x-x_0,y-y_0)\\) 代入上式即可： \\[ \\begin{eqnarray} f(x-x_0,y-y_0)&amp;=&amp;\\sum_{u=0}^{M-1}\\sum_{v=0}^{N-1}F(u,v)e^{j2\\pi [(x-x_0)u/M+(y-y_0)v/N]} \\\\ &amp;=&amp;\\sum_{u=0}^{M-1}\\sum_{v=0}^{N-1}F(u,v)e^{-j2\\pi(ux_0/M+vy_0/N)}e^{j2\\pi (ux/M+vy/N)} \\end{eqnarray} \\] 在保持原来的基底向量不变的情况下，我们只需要将傅立叶系数变成 \\(F(u,v)e^{-j2\\pi(ux_0/M+vy_0/N)}\\) 即可。 同样的，如果频谱图发生平移，有如下关系成立： \\[ F(u-u_0,v-v_0)\\Leftrightarrow f(x,y)e^{j2\\pi (u_0 x/M+v_0 y/N)} \\tag{2} \\] 证明的方法是类似的。 从平移关系中，我们可以得到一个很好的性质。注意，在复数中，有这样两个等式成立 \\(|e^{aj}e^{bj}|=|e^{aj}||e^{bj}|\\)、\\(|e^{jx}|=1\\)（不懂的请复习复数相关的内容）。应用到上面的结论，即 \\(|F(u,v)e^{-j2\\pi (ux_0/M+vy_0/N)}|=|F(u,v)|\\)。换句话说，原图平移后，傅立叶频谱图不变。 例如，对于下面两幅图（为了保持图片大小不变，我们在图片外围补了一层 ‘0’ 边界）： 它们对应的傅立叶频谱图都是这个样子的： 注意，四个角上的白点代表低频信号的分量。 另外，我们平时经常用的中心化操作也依赖于平移性和周期性。 所谓中心化，就是将频谱图平移 \\((M/2, N/2)\\) 个单位。由平移性的公式 (2)，可以得到： \\[ \\begin{eqnarray} F(u-\\frac{M}{2},v-\\frac{N}{2}) &amp;\\Leftrightarrow&amp; f(x,y)e^{j2\\pi (\\frac{1}{2}x+\\frac{1}{2}y)} \\\\ &amp;=&amp;f(x,y)e^{j\\pi(x+y)} \\\\ &amp;=&amp;f(x,y){e^{j\\pi}}^{(x+y)} \\\\ &amp;=&amp;f(x,y)(-1)^{x+y} \\end{eqnarray} \\] 所以，我们只要对原图的每个像素乘以 \\((-1)^{x+y}\\)，然后进行傅立叶变换，这样得到的频谱图便是中心化后的频谱图了。 如果对上一幅频谱图中心化，则可以得到： 这么做的目的是为了方便肉眼观察。中心化后，频谱图中心对应的便是低频分量，远离中心的，则是高频分量。 卷积定理 卷积定理表述为： \\[ f(x,y)*h(x,y) \\Leftrightarrow F(u,v)H(u,v) \\\\ F(u,v)*H(u,v) \\Leftrightarrow f(x,y)h(x,y) \\] （注意，右边式子表示的是矩阵的点乘运算，而不是矩阵乘法） 它的意思是说，在空间域内进行卷积运算，跟把它们转换到频率域再进行点乘运算，效果是等价的。 要证明这个定理，首先要知道卷积的定义（关于卷积的定义，可以参考这篇知乎的回答）： \\[ f(x,y)*h(x,y)=\\sum_{m=0}^{M-1}\\sum_{n=0}^{N-1}f(m,n)h(x-m,y-n) \\] 然后，我们对等式两边同时进行傅立叶变换（注意，傅立叶变换是针对x、y进行的，m、n相关的式子可以看作常数）： \\[ \\begin{eqnarray} F[f(x,y)*h(x,y)]&amp;=&amp;F[\\sum_{m=0}^{M-1}\\sum_{n=0}^{N-1}f(m,n)h(x-m,y-n)] \\\\ &amp;=&amp;\\sum_{m=0}^{M-1}\\sum_{n=0}^{N-1}f(m,n)F[h(x-m,y-n)] \\\\ \\end{eqnarray} \\] 由之前的平移性，我们知道：\\(h(x-m,y-n)\\Leftrightarrow H(u,v)e^{-j2\\pi (um/M+vn/N)}\\) 。所以上式中的 \\(F[h(x-m,y-n)]=H(u,v)e^{-j2\\pi (um/M+vn/N)}\\)，这样，我们便得到： \\[ \\begin{eqnarray} F[f(x,y)*h(x,y)]&amp;=&amp;\\sum_{m=0}^{M-1}\\sum_{n=0}^{N-1}f(m,n)F[h(x-m,y-n)] \\\\ &amp;=&amp;\\sum_{m=0}^{M-1}\\sum_{n=0}^{N-1}f(m,n)H(u,v)e^{-j2\\pi (um/M+vn/N)} \\\\ &amp;=&amp;\\sum_{m=0}^{M-1}\\sum_{n=0}^{N-1}f(m,n)e^{-j2\\pi (um/M+vn/N)} H(u,v) \\\\ &amp;=&amp;F(u,v)H(u,v) \\end{eqnarray} \\] 上面的 \\(\\sum_{m=0}^{M-1} \\sum_{n=0}^{N-1} f(m,n)e^{-j2\\pi (um/M+vn/N)}\\) 刚好凑成一个傅立叶变换 \\(F(u,v)\\)。所以我们最终证明：\\(f(x,y)*h(x,y) \\Leftrightarrow F(u,v)H(u,v)\\)。 另一个式子 \\(F(u,v)*H(u,v) \\Leftrightarrow f(x,y)h(x,y)\\) 的证明是类似的。 参考 如何通俗易懂地解释卷积？ 图像处理中的数学原理详解17——卷积定理及其证明","raw":null,"content":null,"categories":[{"name":"图像处理","slug":"图像处理","permalink":"https://jermmy.github.io/categories/图像处理/"}],"tags":[{"name":"图像处理","slug":"图像处理","permalink":"https://jermmy.github.io/tags/图像处理/"}]},{"title":"图像中的傅立叶变换（一）","slug":"2017-10-5-fourier-transform-in-image-1","date":"2017-10-05T12:57:22.000Z","updated":"2017-10-25T16:27:46.000Z","comments":true,"path":"2017/10/05/2017-10-5-fourier-transform-in-image-1/","link":"","permalink":"https://jermmy.github.io/2017/10/05/2017-10-5-fourier-transform-in-image-1/","excerpt":"关于傅立叶变换，知乎上已经有一篇很好的教程，因此，这篇文章不打算细讲傅立叶的物理含义，只是想从图像的角度谈一谈傅立叶变换的本质和作用。\n本文假设读者已经熟知欧拉公式： \\[\ne^{j\\pi x}=\\cos{\\pi x}+j\\sin{\\pi x}\n\\] 并且知道高数课本中给出的傅立叶变换公式： \\[\nf(x) ～ \\frac{a_0}{2}+\\sum_{n=1}^{\\infty}{[a_n \\cos{nx}+b_n\\sin{nx}]}\n\\] 其中 \\(a_n=\\frac{1}{\\pi}\\int_{-\\pi}^{\\pi}{f(x)\\cos{nx}}dx\\)，\\(b_n=\\frac{1}{\\pi}\\int_{-\\pi}^{\\pi}{f(x)\\sin{nx}}dx\\)。\n当然，线性代数也还是要懂一些的。","text":"关于傅立叶变换，知乎上已经有一篇很好的教程，因此，这篇文章不打算细讲傅立叶的物理含义，只是想从图像的角度谈一谈傅立叶变换的本质和作用。 本文假设读者已经熟知欧拉公式： \\[ e^{j\\pi x}=\\cos{\\pi x}+j\\sin{\\pi x} \\] 并且知道高数课本中给出的傅立叶变换公式： \\[ f(x) ～ \\frac{a_0}{2}+\\sum_{n=1}^{\\infty}{[a_n \\cos{nx}+b_n\\sin{nx}]} \\] 其中 \\(a_n=\\frac{1}{\\pi}\\int_{-\\pi}^{\\pi}{f(x)\\cos{nx}}dx\\)，\\(b_n=\\frac{1}{\\pi}\\int_{-\\pi}^{\\pi}{f(x)\\sin{nx}}dx\\)。 当然，线性代数也还是要懂一些的。 图像的表示 傅立叶变换本质上是把信号从时空域变换到频率域。但在图像里面，这个本质又说明什么呢？ 为了搞清楚这一点，我们先回顾一下，什么是图像。 通常来说，我们看到的计算机里的图像是一个二维矩阵： 比如，上面这个只有四个像素点的图片，就是一个这样的矩阵（不要在意数值大小，你可以把它们归一化到常用的 0～255 区间，但本质上它们表达的信息是一样的）： \\[ \\begin{bmatrix} 0.4 &amp; 0.6 \\\\ 0.8 &amp; 0.2 \\end{bmatrix} \\] 假设图像是 \\(f(x)\\)，这个 \\(f(x)\\) 就是我们常说的信号。这个信号表面上看是一个矩阵，其实它是由几个最基本的向量的线性组合产生的： \\[ f(x)=0.4 \\times \\begin{bmatrix}1 &amp; 0 \\\\ 0 &amp; 0 \\end{bmatrix}+0.6 \\times \\begin{bmatrix}0 &amp; 1 \\\\ 0 &amp; 0 \\end{bmatrix}+0.8 \\times \\begin{bmatrix}0 &amp; 0 \\\\ 1 &amp; 0 \\end{bmatrix}+0.2 \\times \\begin{bmatrix}0 &amp; 0 \\\\ 0 &amp; 1 \\end{bmatrix} \\] 如果看到这里你有一种恍然大悟的感觉，那你差不多快摸清傅立叶的套路了。 其实，从线性代数的角度出发去思考问题，你会发现，图像这种信息是由一些最基本的元素组合而成的。这些元素在线性代数中被称为基向量，它们构成的集合称为基底。选择不同的基底，信息就可以有多种不同的表示。例如上图中，我们选择的是最常见的基底： \\[ \\{\\begin{bmatrix}1 &amp; 0 \\\\ 0 &amp; 0 \\end{bmatrix} , \\begin{bmatrix}0 &amp; 1 \\\\ 0 &amp; 0 \\end{bmatrix} , \\begin{bmatrix}0 &amp; 0 \\\\ 1 &amp; 0 \\end{bmatrix}, \\begin{bmatrix}0 &amp; 0 \\\\ 0 &amp; 1 \\end{bmatrix}\\} \\] 在这组基底下，图片就表示为：\\(\\begin{bmatrix} 0.4 &amp; 0.6 \\\\ 0.8 &amp; 0.2 \\end{bmatrix}\\)。 如果换成另外一组基底，就会得到另一种表示。甚至，在计算机视觉中，我们常常会提取图像的语义信息，把图像转换到其他一些高维空间。但不管怎样，它们本质上都是从某一个特殊的角度来表示图像这一信息，只是不同的基底下，表示出来的特征有所不同，有些适合肉眼观看，有些适合计算机识别。 那傅立叶变换是想干嘛？其实就是换了另一种基底（正/余弦函数）来表示图像。傅立叶变换之所以重要，是因为它所采用的基底具有一些非常好的性质，可以方便我们对图像进行处理。 傅立叶变换 这一节中，我们就来看看，如何把图像用傅立叶的基底表示出来，也就是我们常说的图像的傅立叶变换。 首先，要明确，在一组基底下，信息的表示是什么。回到上一节的例子，\\(f(x)=\\begin{bmatrix} 0.4 &amp; 0.6 \\\\ 0.8 &amp; 0.2 \\end{bmatrix}\\)，有没有发现，在特定基底下，信息是用这些基底线性组合的权重来表示的。只要我们有了这些权重信息，就可以用基底向量的线性组合把信息恢复出来。 因此，对于傅立叶变换而言，我们要求的其实就是基底的权重。 那傅立叶变换的基底是什么呢？如果你看过前面说的那篇教程，你会发现，傅立叶是用无穷多个三角函数的叠加来逼近原来的信号。因此，对于傅立叶变换而言，它的基底其实就是这些三角函数，而我们要求的则是这些函数的线性组合参数。 回到最开始的傅立叶公式： \\[ f(x) ～ \\frac{a_0}{2}+\\sum_{n=1}^{\\infty}{[a_n \\cos{nx}+b_n\\sin{nx}]} \\] 有没有看到，这个公式已经揭示了这些三角函数的系数，也就是我们要求的线性组合参数。公式前面的 \\(a_0\\) 是可以统一进去的。下面，我们就从它出发，看看如何推导出统一的参数表示。 （⚠️以下是公式重灾区，恐惧者可直接跳到结论部分） 首先，我们考虑更一般的情况，即函数 \\(f(x)\\) 的周期是 \\(T\\)（上面这个公式的周期是 \\(2\\pi\\)），然后将 \\(f(x)\\) 表示成另一种形式： \\[ f(x)=a_0+\\sum_{n=1}^\\infty {[a_n\\cos{\\frac{2n\\pi x}{T}}+b_n \\sin{\\frac{2n\\pi x}{T}}]} \\] 其中，\\(a_0=\\frac{1}{T}\\int_0^T{f(x)dx}\\)，\\(a_n=\\frac{2}{T}\\int_{0}^T{f(x)\\cos{\\frac{2n\\pi x}{T}dx}}\\)，\\(b_n=\\frac{2}{T}\\int_0^T{f(x)\\sin{\\frac{2n\\pi x}{T}dx}}\\)。 注意，这个表示和之前的公式没有本质区别。 接下来，对 \\(f(x)\\) 进行一系列操作： \\[ \\begin{eqnarray} f(x)&amp;=&amp;a_0+\\sum_{n=1}^\\infty {[a_n\\cos{\\frac{2n\\pi x}{T}}+b_n \\sin{\\frac{2n\\pi x}{T}}]} \\\\ &amp;=&amp;a_0+\\sum_{n=1}^{\\infty}{[a_n\\cos{\\omega_nx+b_n\\sin{\\omega_nx}}]} \\\\ &amp;=&amp;a_0+\\sum_{n=1}^\\infty{[a_n(\\frac{e^{j\\omega_n x}+e^{-j\\omega_n x}}{2})+b_n(\\frac{e^{j\\omega_n x}-e^{-j\\omega_n x}}{2j})]} \\\\ &amp;=&amp;a_0+\\sum_{n=1}^\\infty{(\\frac{a_n-jb_n}{2})e^{j\\omega_n x}+\\sum_{n=1}^\\infty{(\\frac{a_n+jb_n}{2})e^{-j\\omega_nx}}} \\end{eqnarray} \\] 其中，\\(\\omega_n=\\frac{2n\\pi}{T}\\)。 下一步，继续化简括号里的东西： \\[ \\begin{eqnarray} \\frac{a_n-jb_n}{2}&amp;=&amp;\\frac{1}{2}[\\frac{2}{T}\\int_{-\\frac{T}{2}}^{\\frac{T}{2}}f(x)\\cos{\\omega_n x\\ dx-j\\frac{2}{T}\\int_{-\\frac{T}{2}}^{\\frac{T}{2}}f(x)\\sin{\\omega_nx\\ dx}}] \\\\ &amp;=&amp;\\frac{1}{T}\\int_{-\\frac{T}{2}}^{\\frac{T}{2}}f(x)[\\cos{\\omega_n x-j\\sin{\\omega_n x}}]\\ dx \\\\ &amp;=&amp;\\frac{1}{T}\\int_{-\\frac{T}{2}}^{\\frac{T}{2}}{f(x)e^{-j\\omega_n x}}\\ dx \\\\ &amp;=&amp;c_n \\end{eqnarray} \\] 其中，\\(c_n=\\frac{1}{T}\\int_{-\\frac{T}{2}}^{\\frac{T}{2}}{f(x)e^{-j\\omega_n x}}\\ dx\\)。 用上面的结果化简公式右边的内容： \\[ \\begin{eqnarray} \\sum_{n=1}^\\infty{(\\frac{a_n+jb_n}{2})e^{-j\\omega_n x}}&amp;=&amp;\\sum_{n=-\\infty}^{-1}{(\\frac{a_{-n}+jb_{-n}}{2})e^{-j\\omega_{-n}x}} \\\\ &amp;=&amp;\\sum_{n=-\\infty}^{-1}{(\\frac{a_n-jb_n}{2})e^{j\\omega_n x}} \\\\ &amp;=&amp;\\sum_{n=-\\infty}^{-1}{(\\frac{a_n-jb_n}{2})e^{j\\omega_n x}} \\\\ &amp;=&amp;\\sum_{n=-\\infty}^{-1}{c_n e^{j\\omega_n x}} \\end{eqnarray} \\] 这样一来，我们就得到一个统一的表达式： \\[ \\begin{eqnarray} f(x)&amp;=&amp;a_0+\\sum_{n-1}^{\\infty}{(\\frac{a_n-jb_n}{2})e^{j\\omega_n x}+\\sum_{n=1}^{\\infty}{(\\frac{a_n+jb_n}{2})e^{-j\\omega_n x}}} \\\\ &amp;=&amp;c_0+\\sum_{n=1}^{\\infty}{c_ne^{j\\omega_n x}+\\sum_{n=-\\infty}^{-1}c_n e^{j\\omega_n x}} \\\\ &amp;=&amp;\\sum_{n=-\\infty}^{\\infty}{c_ne^{j\\omega_n x}} \\end{eqnarray} \\] 码了这么多公式后，我们终于得到了一个关于三角函数的线性组合的形式。再经过傅立叶变换后，函数 \\(f(x)\\) 就变成了一个向量的形式：\\({[\\dots, c_{-n}, \\dots, c_0, c_1, c_2, \\dots, c_n, \\dots]}\\)。这里的 \\(c_n\\) 就是我们通常说的傅立叶变换，而公式中的 \\(e^{j\\omega_nx}\\) 就是傅立叶变换的基底。 接下来，我们想知道，对于一张图片而言，这个 \\(c_n\\) 该怎么求。 我们先考虑一维的情况（你可以想象成一个宽度为 \\(N\\)，高度为 1 的图片）。在公式中，\\(c_n=\\frac{1}{T}\\int_{-\\frac{T}{2}}^{\\frac{T}{2}}{f(x)e^{-j\\omega_n x}}\\ dx\\)，这是对连续函数而言的，但计算机中的图像信号是离散的，因此，我们要把积分符号换成求和符号。另外，周期用图像的宽度代替。这样，我们就得到图像中的傅立叶变换：\\(c_n=\\frac{1}{N}\\sum_{x=0}^{N-1}f(x)e^{-j\\omega_n x}\\)。 然后，有同学会问，这样的 \\(c_n\\) 有多少个呢？在我们前面推导的公式中，\\(c_n\\) 有无穷多个，这时符合傅立叶的前提假设的：无穷多个三角函数的叠加才能构成原来的信号。但是，计算机只能表示有穷的信息，因此，我们需要进行采样。好在，实际情况中，当 \\(n\\) 的值越来越大时，\\(c_n\\) 的值会渐渐趋于 0，因为图片中的高频分量往往比较小，所以这一部分高频的三角函数的系数 \\(c_n\\) 的值也会很小（公式中，频率 \\(\\omega_n=\\frac{2n\\pi}{N}\\)，\\(n\\) 越大也就表明频率越大）。实际操作中，我们会取 \\(N\\) 个 \\(c_n\\)，其中 \\(N\\) 就是图像信号的周期。因此，我们最终得到图像中的傅立叶变换： \\[ F(u)=\\frac{1}{N}\\sum_{x=0}^{N-1}f(x)e^{-j2\\pi ux/N} , \\ \\ u=0,1,...,N-1 \\] 公式中的 \\(f(x)\\) 指的就是图片中 x 位置的像素值，这也说明，傅立叶变换是综合计算图像像素的整体灰度变化后得到的。如果图像中高频信息比较大（即图像灰度变化很剧烈），则对于频率高的基底向量（\\(u\\) 的值比较大），其对应的权重 \\(F(u)\\) 的值也会比较大，反之亦然。 傅立叶逆变换 所谓傅立叶逆变换，就是在得到一堆傅立叶变换的参数 \\(F(u)\\) 后，如何反推回去，得到空间域的图像。 其实非常简单，前面说了，傅立叶变换就是把时空域下的图像信号 \\(f(x)\\) 重新用频率域的基底来表示。现在，频率域基底向量的系数 \\(F(u,v)\\) 已经有了，把它们组合起来就可以得到原来的信号：\\(f(x)=\\sum_{n=-\\infty}^{\\infty}{c_ne^{j\\omega_n x}}\\)。因此，傅立叶逆变换就是把傅立叶基底向量的系数再累加起来： \\[ f(x)=\\sum_{u=0}^{N-1}F(u)e^{j2\\pi ux/N}, \\ \\ x=0,1,...,N-1 \\] 二维傅立叶 二维傅立叶其实就是在一维的基础上，继续向另一个方向扩展而已，相当于一个二重积分： \\[ \\begin{eqnarray} F(u, v)&amp;=&amp;\\frac{1}{MN}\\sum_{x=0}^{M-1}\\sum_{y=0}^{N-1}f(x, y)e^{-j2\\pi ux/M}e^{-j2\\pi vy/N} \\\\ \\end{eqnarray} \\] 傅立叶逆变换： \\[ f(x,y)=\\sum_{u=0}^{M-1}\\sum_{v=0}^{N-1}F(u,v)e^{j2\\pi ux/M}e^{j2\\pi vy/N} \\] 参考 傅里叶分析之掐死教程","raw":null,"content":null,"categories":[{"name":"图像处理","slug":"图像处理","permalink":"https://jermmy.github.io/categories/图像处理/"}],"tags":[{"name":"图像处理","slug":"图像处理","permalink":"https://jermmy.github.io/tags/图像处理/"}]},{"title":"最大似然估计","slug":"2017-9-30-maximum-likelihood-estimation","date":"2017-09-30T05:54:29.000Z","updated":"2017-10-05T07:42:13.000Z","comments":true,"path":"2017/09/30/2017-9-30-maximum-likelihood-estimation/","link":"","permalink":"https://jermmy.github.io/2017/09/30/2017-9-30-maximum-likelihood-estimation/","excerpt":"在讨论最大似然估计之前，我们先来解决这样一个问题：有一枚不规则的硬币，要计算出它正面朝上的概率。为此，我们做了 10 次实验，得到这样的结果：[1, 0, 1, 0, 0, 0, 0, 0, 0, 1]（1 代表正面朝上，0 代表反面朝上）。现在，要根据实验得到的结果来估计正面朝上的概率，即模型的参数 \\(p\\)（\\(0 \\le p \\le 1\\)）。\n当然，对于投硬币这种问题，由于模型很简单，我们可以用大量实验来近似最终结果，不过，如果事件模型复杂一些，做大量的实验就显得不太现实。这个时候，用最大似然估计的思想，则可以通过较少的实验得出一个相对好的结果。本文就从这个简单的例子出发，对最大似然估计做一次简单的描述。","text":"在讨论最大似然估计之前，我们先来解决这样一个问题：有一枚不规则的硬币，要计算出它正面朝上的概率。为此，我们做了 10 次实验，得到这样的结果：[1, 0, 1, 0, 0, 0, 0, 0, 0, 1]（1 代表正面朝上，0 代表反面朝上）。现在，要根据实验得到的结果来估计正面朝上的概率，即模型的参数 \\(p\\)（\\(0 \\le p \\le 1\\)）。 当然，对于投硬币这种问题，由于模型很简单，我们可以用大量实验来近似最终结果，不过，如果事件模型复杂一些，做大量的实验就显得不太现实。这个时候，用最大似然估计的思想，则可以通过较少的实验得出一个相对好的结果。本文就从这个简单的例子出发，对最大似然估计做一次简单的描述。 基本思想 似然（likelihood），就是可能性的意思。所谓最大似然估计，顾名思义，就是根据最大的可能性对参数进行估计。那么什么是最大的可能性呢？对于上面那个投硬币的例子，扔 10 次硬币最可能出现的结果会是什么？最大似然估计认为，最可能出现的结果就是：[1, 0, 1, 0, 0, 0, 0, 0, 0, 1]。有人可能会纳闷，这不就是我们实验的结果吗？不错，最大似然估计有点类似于人类「先入为主」的思维。投 10 次硬币可能出现的情况有那么多，为什么偏偏我们的实验结果就是这样的呢？这是否意味着，这个结果出现的概率是最大的？ 再举个例子（该例子改编自文末链接）：两位猎人 A 和 B 一起外出打猎，一只野兔从两人面前窜过，两人同时开枪，结果 A 猎人射杀了野兔。如果要推测谁的枪法准，你是不是会「先入为主」地认为 A 猎人的枪法好？因为射杀兔子的可能情况有那么多种（可能是 B 射杀，也可能是 A、B 同时射杀），但偏偏发生的却是 A 射杀了兔子，那我们当然会倾向于认为 A 的枪法好一些。这种「先入为主」的思想，其实就是最大似然法的思想。简单地说，就是按照最可能的情况来评估事件。当然，这种思想多少存在误判的情况（比如，A 这次能射杀兔子纯属偶然），但随着实验次数增多，结果也会更加准确（如果两人多次狩猎，B 偶尔得手，但 A 频频得手，那 A 枪法好的可能性就更大了）。 回到硬币那个例子，同样的道理，我们认为，出现结果 [1, 0, 1, 0, 0, 0, 0, 0, 0, 1] 的可能性比其他结果要大。 最大似然估计的求解方法 我们先把实验的结果用数学式子表达出来：\\(f(x_1, x_2, \\cdots , x_{10}; p)=p(1-p)p\\cdots p=p^3(1-p)^7\\)。 现在要用最大似然估计的思想求出这里的 \\(p\\)。前面说过了，\\(f(x_1, x_2, \\cdots , x_{10}; p)\\) 出现的可能性是最大的，也就是说，\\(p^3(1-p)^7\\) 的值要满足最大。这样一来，问题就简单多了，只要根据函数 \\(h(p)=p^3(1-p)^7\\) 的单调性，找出使得 \\(h(p)\\) 的值最大的 \\(p\\) 即可。为了计算的方便，我们往往会引入对数，即 \\(\\ln {h(p)}=\\ln{p^3(1-p)^7}\\)，这个函数单调性和 \\(h(p)\\) 是一致的，因此只需要求出 \\(\\ln{h(p)}\\) 的最大值即可。最大值一般来说出现在导数为 0 的时候，因此，令 \\(\\frac{d \\ln{h(p)}}{dp}=\\frac{3}{p}-\\frac{7}{1-p}=0\\)，解得 \\(p=\\frac{3}{10}\\)。 换句话说，当 \\(p=\\frac{3}{10}\\) 时，\\(f(x_1, x_2, \\cdots , x_{10}; p)\\) 出现的可能性最大。因此，我们估计出来的模型参数就是 \\(p=\\frac{3}{10}\\)。这个结果也符合我们的预期（10 次实验中有 3 次正面朝上）。事实上，投硬币这个简单的模型并没法完全体现出最大似然估计的威力，而且，可以证明，在这个例子中，用最大似然估计得出来的结果永远都是 \\(\\frac{x}{n}\\) （其中，n 是实验次数，x 是正面朝上的次数）。不过，在其他一些更复杂的模型中，用最大似然法来估计参数，往往是最方便有效的。 下面，我们总结一下最大似然估计的一般步骤（改自文末链接）： 写出似然函数；（即上文中的 \\(f(x_1, x_2, \\cdots , x_{10}; p)\\)） 对似然函数取对数；（因为似然函数往往是众多概率相乘的形式，对数可以方便运算） 求导数，令导数为 0，得到似然方程； 解方程，得到参数。 总结 最大似然法是在已知实验结果的基础上，估计模型参数的方法。它的基本思想是，假设实验结果出现的可能性最大，并依此反推出参数。 表述成数学语言如下： 假设我们观察到一些实验结果：\\(x_1, x_2, \\dots, x_n\\)，要估计出模型参数 \\(\\theta_1, \\theta_2, \\dots, \\theta_m\\)。根据最大似然法，要让似然函数 \\(f(x_1, x_2, \\dots, x_n; \\theta_1, \\theta_2, \\dots, \\theta_m)\\) 满足： \\[ f(x_1, x_2, \\dots, x_n; \\hat \\theta_1, \\hat \\theta_2, \\dots, \\hat \\theta_m)\\ge f(x_1, x_2, \\dots, x_n; \\theta_1, \\theta_2, \\dots, \\theta_m) \\] 这里的 \\(\\hat \\theta_1, \\hat \\theta_2, \\dots, \\hat \\theta_m\\) 就是使得实验结果出现的可能性最大的参数，也是最大似然法估计出来的参数。 参考 从最大似然到EM算法浅解 概率与统计","raw":null,"content":null,"categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://jermmy.github.io/categories/机器学习/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"https://jermmy.github.io/tags/机器学习/"},{"name":"概率统计","slug":"概率统计","permalink":"https://jermmy.github.io/tags/概率统计/"}]},{"title":"论文笔记：Deep Residual Learning","slug":"2017-9-25-paper-notes-deep-residual-learning","date":"2017-09-25T08:41:35.000Z","updated":"2018-01-24T03:59:29.000Z","comments":true,"path":"2017/09/25/2017-9-25-paper-notes-deep-residual-learning/","link":"","permalink":"https://jermmy.github.io/2017/09/25/2017-9-25-paper-notes-deep-residual-learning/","excerpt":"之前提到，深度神经网络在训练中容易遇到梯度消失/爆炸的问题，这个问题产生的根源详见之前的读书笔记。在 Batch Normalization 中，我们将输入数据由激活函数的收敛区调整到梯度较大的区域，在一定程度上缓解了这种问题。不过，当网络的层数急剧增加时，BP 算法中导数的累乘效应还是很容易让梯度慢慢减小直至消失。这篇文章中介绍的深度残差 (Deep Residual) 学习网络可以说根治了这种问题。下面我按照自己的理解浅浅地水一下 Deep Residual Learning 的基本思想，并简单介绍一下深度残差网络的结构。\n\n\n","text":"之前提到，深度神经网络在训练中容易遇到梯度消失/爆炸的问题，这个问题产生的根源详见之前的读书笔记。在 Batch Normalization 中，我们将输入数据由激活函数的收敛区调整到梯度较大的区域，在一定程度上缓解了这种问题。不过，当网络的层数急剧增加时，BP 算法中导数的累乘效应还是很容易让梯度慢慢减小直至消失。这篇文章中介绍的深度残差 (Deep Residual) 学习网络可以说根治了这种问题。下面我按照自己的理解浅浅地水一下 Deep Residual Learning 的基本思想，并简单介绍一下深度残差网络的结构。 基本思想 回到最开始的问题，为什么深度神经网络会难以训练？根源在于 BP 的时候我们需要逐层计算导数并将这些导数相乘。这些导数如果太小，梯度就容易消失，反之，则会爆炸。我们没法从 BP 算法的角度出发让这个相乘的导数链消失，因此，可行的方法就是控制每个导数的值，让它们尽量靠近 1，这样，连乘后的结果不会太小，也不会太大。 现在，我们就从导数入手，看看如何实现上面的要求。由于梯度消失的问题比梯度爆炸更常见，因此只针对梯度消失这一点进行改进。 假设我们理想中想让网络学习出来的函数是 \\(F(x; {W_i})\\)，但由于它的导数 \\(\\frac{\\partial F}{\\partial x}\\) 太小，所以训练的时候梯度就消失了。所谓太小，就是说 \\(\\frac{\\partial F}{\\partial x} \\approx 0\\)，那么，我们何不在这个导数的基础上加上 1 或者减去 1，这样梯度不就变大了吗？（这里的 1 是为了满足之前提到的梯度靠近 1 这一要求，事实上，只要能防止梯度爆炸，其他数值也是可以的，不过作者在之后的实验中证明，1 的效果最好） 按照这种思路，我们现在想构造一个新的函数，让它的导数等于 \\(\\frac{\\partial F}{\\partial x}+1\\)。由这个导数反推回去，很自然地就得到一个我们想要的函数：\\(H(x)=F(x)+x\\)，它的导数为：\\(\\frac{\\partial H}{\\partial x} = \\frac{\\partial F}{\\partial x}+1\\)。这个时候你可能会想，如果将原来的 \\(F(x)\\) 变成 \\(H(x)\\)，那网络想要提取的特征不就不正确了吗，这个网络还有什么用？不错，我们想要的最终函数是 \\(F(x; {W_i})\\)，这个时候再加个 \\(x\\) 上去，结果肯定不是我们想要的。但是，为什么一定要让网络学出 \\(F(x; {W_i})\\)？为什么不用 \\(H(x)\\) 替换原本的 \\(F(x;{W_i})\\)，而将网络学习的目标调整为：\\(F(x)=H(x)-x\\)？要知道，神经网络是可以近似任何函数的，只要让网络学出这个新的 \\(F(x)\\)，那么我们自然也就可以通过 \\(H(x)=F(x)+x\\) 得到最终想要的函数形式。作者认为，通过这种方式学习得到的 \\(H(x)\\) 函数，跟当初直接让网络学习出的 \\(F(x, {W_i})\\)，效果上是等价的，但前者却更容易训练。 ==================== UPDATE 2018.1.23 ===================== 时隔几个月重新看这篇文章，发现当初的理解存在一个巨大的问题，在此，对那些被我误导的同学深深道歉🙇。 这里的问题在于，BP 算法中我们要计算的是参数 \\(W\\) 和 \\(b\\) 的导数，所以导数的形式不应该是 \\(\\frac{\\partial F}{\\partial x}\\)，而是 \\(\\frac{F}{W_i}\\)（bias 同理）。这样一来，我之前对残差网络改进梯度消失问题的理解就错了。不过，我依然固执地认为，残差学习是为了解决深度网络中梯度消失的问题，只是要换种方式理解。 对于最简单的神经网络（假设退化成一条链）： \\(C\\) 是网络的 loss 函数，\\(z^l\\) 表示第 l 层激活函数的输入，\\(a^l\\) 表示第 l 层激活函数的输出（\\(a^0\\) 就是网络最开始的输入了），则 \\(a^l = \\sigma(z^l)\\)，\\(z^l=a^{l-1}*w^l\\)（\\(W^l\\) 是第 l 层的权重参数，简单起见，不考虑 bias）。\\(\\delta^l\\) 是第 l 层的误差。 根据 BP 算法，先计算误差项： \\[ \\delta^3=\\frac{\\partial C}{\\partial a^3}\\frac{\\partial a^3}{\\partial z^3}=\\frac{\\partial C}{\\partial a^3}\\sigma&#39;(z^3) \\\\ \\delta^2=\\delta^3 \\sigma&#39;(z^2)w^3=\\frac{\\partial C}{\\partial a^3}\\sigma&#39;(z^3)\\sigma&#39;(z^2)w^3 \\\\ \\delta^1=\\delta^2\\sigma&#39;(z^1)w^2=\\frac{\\partial C}{\\partial a^3}\\sigma&#39;(z^3)\\sigma&#39;(z^2)w^3\\sigma&#39;(z^1)w^2 \\] 然后根据误差项计算 \\(w\\) 的导数： \\[ \\frac{\\partial C}{\\partial w^3}=\\delta^3a^2 \\\\ \\frac{\\partial C}{\\partial w^2}=\\delta^2a^1 \\\\ \\frac{\\partial C}{\\partial w^1}=\\delta^1a^0 \\] 一般来说，梯度的消失是这些项的累乘造成的：\\(\\sigma&#39;(z^3)\\sigma&#39;(z^2)w^3\\sigma&#39;(z^1)w^2\\)（因为 \\(\\sigma&#39;(z^l)\\) 和 \\(w^l\\) 一般都小于 1）。 那残差网络做了那些修改呢？其实就是简单地在激活函数的输出后面，加入上一层的输入： 假设原本的网络是要学习一个 \\(H(x)\\) 函数，那现在这个网络依然是要学习 \\(H(x)\\)。只不过，原本的网络要学习的是整个 \\(H(x)\\)，而残差网络中，和原本网络相同的那部分结构，要学习的就只是 \\(H(x)-x\\)。换句话说，它要学习的东西只是一个微小的变化，因此训练起来相对更容易一些。 另一方面，我们沿用之前对导数的分析思路，看看残差网络的梯度会发生什么变化。 首先，残差网络的前向传播发生了变化： \\[ z^1=a^0 \\\\ a^1=\\sigma(z^1)+a^0 \\\\ z^2=a^1w^2 \\\\ a^2=\\sigma(z^2)+a^1 \\\\ z^3=a^2w^3 \\\\ a^3=\\sigma(z^3)+a^2 \\] 反向传播计算的误差项为： \\[ \\delta^3=\\frac{\\partial C}{\\partial z^3}=\\frac{\\partial C}{\\partial a^3}\\frac{\\partial a^3}{\\partial z^3}=\\frac{\\partial C}{\\partial a^3}[\\sigma&#39;(z^3)+\\frac{\\partial a^2}{\\partial z^3}] \\\\ \\delta^2=\\delta^3 w^3 \\frac{\\partial a^2}{\\partial z^2}=\\frac{\\partial C}{\\partial a^3}[\\sigma&#39;(z^3)+\\frac{\\partial a^2}{\\partial z^3}]w^3 [\\sigma&#39;(z^2)+\\frac{\\partial a^1}{\\partial z^2}] \\\\ \\vdots \\] 由于 \\(z^3=a^2w^3\\)，所以 \\(a^2=\\frac{z^3}{w^3}\\)，故 \\(\\frac{\\partial a^2}{\\partial z^3}=\\frac{1}{w^3}\\)，同理 \\(\\frac{\\partial a^1}{\\partial z^2}=\\frac{1}{w^2}\\)。代入到上式中就变成： \\[ \\delta^3=\\frac{\\partial C}{\\partial a^3}[\\sigma&#39;(z^3)+\\frac{1}{w^3}] \\\\ \\delta^2=\\frac{\\partial C}{\\partial a^3}[\\sigma&#39;(z^3)+\\frac{1}{w^3}]w^3 [\\sigma&#39;(z^2)+\\frac{1}{w^2}]=\\frac{\\partial C}{\\partial a^3}[\\sigma&#39;(z^3)w^3+1] [\\sigma&#39;(z^2)+\\frac{1}{w^2}] \\\\ \\vdots \\] 对比之前没加残差结构的网络，这个新的网络结构中，误差项 \\(\\delta^l\\) 减小为 0 的可能性降低了。以 \\(\\delta^2\\) 为例，原本的 \\(\\delta^2=\\frac{\\partial C}{\\partial a^3}\\sigma&#39;(z^3)\\sigma&#39;(z^2)w^3\\)，而现在，连乘的项变成了 \\([\\sigma&#39;(z^3)w^3+1]\\) 和 \\([\\sigma&#39;(z^2)+\\frac{1}{w^2}]\\)，由于 \\(\\sigma&#39;(z^l)\\) 和 \\(w^l\\) 一般都小于 1，所以这两项的值会略大于 1，这样，无论连乘多少项，梯度都不会缩小到 0。 ================================================== 上面所说的 \\(F(x)=H(x)-x\\) 就是所谓的残差 (residual)，而式子内的 \\(x\\) 在论文中被称为 Identity Mapping，因为 x 可以看作是由自己到自己的映射函数。基于此，我们可以得到一个新的网络结构，如同开篇的图片所示，这个网络结构跟普通的网络结构类似，但在输出那里多加了一个 Identity Mapping，相当于在网络原有输出的基础上加一个 x，这样便得到我们想要的函数 \\(H(x)\\)。作者将这种相加称为 shortcut connection，意思就是说，\\(x\\) 没有经过中间的变换操作，像「短路」一样直接跳到输出那里和 \\(F(x)\\) 相加。需要注意的是，这个网络结构的输入并不一定是原始的数据，它可以是前面一层网络的输出结果。同理，网络的输出也可以继续输入到后面层的网络中。 我们用一个式子来表示这个网络：\\(y=F(x,{W_i})+x\\)，其中 \\(F(x,{W_i})=W_2 \\sigma(W_1x)\\) （这里忽略了 bias）。在论文中，这里的 \\(\\sigma\\) 函数采用的是 ReLu。得到 \\(y\\) 后，作者又对其做了一次 ReLu 操作，然后再进入下一层网络。 Talk is cheap，show you the code（这里用 tensorflow 表示一下上图那个网络结构）： 12345678# 假设 x 是该网络结构的输入c1 = tf.layers.conv2d(x, kernel, [w, h], strides=[s,s])b1 = tf.layers.batch_normalization(c1, training=is_training)h1 = tf.nn.relu(b1)c2 = tf.layers.conv2d(h1, kernel, [w, h], strides=[s,s])b2 = tf.layers.batch_normalization(c2, training=is_training)r = b2 + xy = tf.nn.relu(r) 因为 \\(x\\) 和 \\(F(x)\\) 是直接相加的，所以它们的维度必须相同，不同的情况下，需要对 \\(x\\) 的维度进行调整。可以通过做一次线性变换将它投影到所需的维度空间，也可以用其他简单粗暴的方法。比如，当维度太高时，可以用 pooling 的方法降低维度。而维度较低时，作者在实验中则是直接补 0 来扩展维度。 深度残差网络 好了，了解了残差网络的基本思路和简单的网络结构后，下面我们可以将它拓展到更深的网络结构中。 下图是一个普通的网络和改造后的残差网络： 左边的网络是没有添加残差层的网络，作者称它为 plain network，意思就是这个网络很「平」（每次看到这个名字我总是会浮出一些邪恶的想法～囧～）。右边的则是一个完整的深度残差网络，它其实就是由前文所说的小的网络结构组成的，虚线表示要对 \\(x\\) 的维度进行扩增。作者在两个网络中都加了 Batch Normalization（具体加在卷积层之后，激活层之前），我想目的大概是要在之后的实验中凸显 residual learning 优于 BN 的效果吧。 下面分析一下 identity mapping 对残差网络所起的作用，通过这个最简单的映射来了解 residual learning 不同于一般网络的地方。 首先，给出最通用的网络结构： 这里其实就是将之前的 \\(x\\) 换成 \\(h(x)\\)，将最后的 ReLu 换成 \\(f(x)\\)。因为事实上，\\(h(x)\\) 和 \\(f(x)\\) 的形式是很自由的，\\(h(x)\\) 可以是 \\(x\\)、\\(2x\\)、\\(x^2\\)，只要能防止梯度消失或爆炸即可。而 \\(f(x)\\) 也可以是其他各种激活函数。 不过，因为我们是要从 identity mapping 着手，所以这里还是令 \\(h(x)=x\\)，\\(f(x)=x\\)： 然后，我们用类推出： 到了这一步，可以发现，在 identity mapping 中，残差网络的输出其实就是在原始输入 \\(x_l\\) 的基础上，加上后面的一堆「残差」。如果对其求导，则可以得出： 我们发现，导数的形式也很类似，也是最后一层的导数加上前面的一堆「残差」导数，而这一步是残差网络中梯度不容易消失的原因。 作者经过对比实验发现，identity mapping 的效果要好于其他的 mapping，具体的实验细节请参考 tutorial 和后续的一篇论文 Identity Mappings in Deep Residual Networks。换句话说，使用 residual network 时，最好用上 identity mapping。 论文中的实验 实验部分，我只讲一下 ImageNet 的结果。 作者分别用 18 层和 34 层的网络做了两组对比实验（两组网络除了残差外，其他结构相同，并且都加了 BN 层。在对 \\(x\\) 升维时，直接使用 0 进行 padding，换句话说，残差网络的参数和 plain 的一样。34 层的网络见上一部分的说明），并分析了它们在 ImageNet 训练集上的误差下降情况： 上图中，左图是 plain 网络，右图是 ResNet。注意，训练刚开始的时候，ResNet 的误差下降的速度比 plain 网络要快，也就是说，残差网络的训练速度快于 plain 网络。对于 18 层的网络而言，两者最终的准确率持平，但对于 34 层的网络，使用残差的结果要好于一般的网络。另外，我们再看看验证集上的情况： 这个结果表明，当网络层数不多时，plain 网络和残差网络除了训练速度不一样外，对最终的结果影响不大。但如果层数比较深，残差网络可以提升准确率。作者在这里提出一个问题：既然我们已经在网络中加了 BN，那导致 plain 网络准确率降不下来的原因应该不会是梯度消失。但又会是其他什么原因呢？作者在论文中称这种问题为 degradation problem，即退化问题。它指的是随着网络层数增加，在梯度没有消失的情况下导致的网络训练缓慢或训练停止的问题。当然啦，按照我自己的理解和猜测，就如这篇文章开篇所讲的那样，梯度消失是由两个方面导致，而 BN 只是将数据从激活函数的收敛区调整到梯度更大的区域，但导数相乘后的累积效应仍然会使梯度变小，所以才导致这里所说的退化问题。不过具体的原因，还有待进一步研究。 参考 何恺明的tutorial Identity Mappings in Deep Residual Networks Deep Residual Learning for Image Recognition","raw":null,"content":null,"categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://jermmy.github.io/categories/机器学习/"}],"tags":[{"name":"深度学习","slug":"深度学习","permalink":"https://jermmy.github.io/tags/深度学习/"},{"name":"论文","slug":"论文","permalink":"https://jermmy.github.io/tags/论文/"}]},{"title":"论文笔记：Batch Normalization","slug":"2017-9-2-paper-notes-batch-normalization","date":"2017-09-02T14:06:20.000Z","updated":"2018-03-20T11:50:20.000Z","comments":true,"path":"2017/09/02/2017-9-2-paper-notes-batch-normalization/","link":"","permalink":"https://jermmy.github.io/2017/09/02/2017-9-2-paper-notes-batch-normalization/","excerpt":"在神经网络的训练过程中，总会遇到一个很蛋疼的问题：梯度消失/爆炸。关于这个问题的根源，我在上一篇文章的读书笔记里也稍微提了一下。原因之一在于我们的输入数据（网络中任意层的输入）分布在激活函数收敛的区域，拿 sigmoid 函数举例：\n\nsigmoid\n\n如果数据分布在 [-4, 4] 这个区间两侧，sigmoid 函数的导数就接近于 0，这样一来，BP 算法得到的梯度也就消失了。\n之前的笔记虽然找到了原因，但并没有提出解决办法。最近在实战中遇到这个问题后，束手无策之际，在网上找到了这篇论文 Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift。原来早在 2015 年就有人提出了解决方案，而且基本已经成为神经网络的组成部分之一了。本文仅仅是对这篇论文做一些梳理，并总结一些我个人的理解。论文中还有很多不解的地方，因此总结的过程中难免存在错误，还请发现的同学指正☺️","text":"在神经网络的训练过程中，总会遇到一个很蛋疼的问题：梯度消失/爆炸。关于这个问题的根源，我在上一篇文章的读书笔记里也稍微提了一下。原因之一在于我们的输入数据（网络中任意层的输入）分布在激活函数收敛的区域，拿 sigmoid 函数举例： sigmoid 如果数据分布在 [-4, 4] 这个区间两侧，sigmoid 函数的导数就接近于 0，这样一来，BP 算法得到的梯度也就消失了。 之前的笔记虽然找到了原因，但并没有提出解决办法。最近在实战中遇到这个问题后，束手无策之际，在网上找到了这篇论文 Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift。原来早在 2015 年就有人提出了解决方案，而且基本已经成为神经网络的组成部分之一了。本文仅仅是对这篇论文做一些梳理，并总结一些我个人的理解。论文中还有很多不解的地方，因此总结的过程中难免存在错误，还请发现的同学指正☺️ 什么是 Batch Normalization 在开始正文之前，不得不先泼一盆冷水：这篇论文虽然解决了梯度消失的一个痛点，但并没有办法根治问题。在实际操作中，我发现用了 Batch Normalization 后，训练的速度确实能得到很大的提升，但也存在完全不起作用的情况。那为什么还要学这篇论文呢？一来，文章的思想很有借鉴意义；二来，虽然方法未必奏效，但用了之后也不会产生什么副作用，况且万一它就奏效了呢；第三，在其他解决梯度消失的文章里面，都会用到这篇论文的方法。所以，不管怎样，在深度学习的优化道路上，这篇论文都是绕过不去的。 为了方便，下面统一用 BN 指代 Batch Normalization。 BN 解决了一个很大的「困扰」，也就是论文中提到的 Internal Covariate Shift。关于什么是 Internal Covariate Shift，我没有读过相关的论文，因此认识也不深。简单理解，就是文章开篇讲的，数据分布在激活函数的收敛区的问题。要「修正」这个问题，一个很自然的想法就是把输入的数据强行掰到中间梯度很大的区域。其实，这种想法我们在数据预处理阶段就用过了，即数据归一化。归一化数据的方式有很多，比如：白化 (whiten) 等。但最简单也最常用的方法是：\\(\\frac{x - \\mu}{\\sigma}\\)，其中 \\(\\mu\\) 是平均数，\\(\\sigma\\) 是标准差。虽然这种方法可以让网络的输入层避免梯度消失的问题，但中间层的数据分布依然不可控。因此，BN 的想法就是，让网络中间每一层的数据都归一化。 比如，我们在数据流向中间任意一层网络的激活函数之前，先做一遍归一化： \\[ \\hat x^{(k)} \\leftarrow \\frac{x_i^{(k)} -\\mu_{\\beta}^{(k)}}{\\sqrt{\\sigma_{\\beta}^{(k)^2}+\\epsilon}} \\tag{1} \\] 其中，\\(x_i^{(k)}\\) 表示第 k 层网络的输入，\\(\\mu_{\\beta}\\) 表示第 k 层所有输入的均值，\\(\\sigma_{\\beta}\\) 表示第 k 层所有输入的标准差，\\(\\epsilon\\) 是为了防止除数为 0 而添加的数值，一般取 0.001 即可。 BN 算法 训练阶段 上面我们介绍了 BN 算法的大概思路。不过要注意一个问题，对于神经网络而言，中间每一层的输出结果都是有意义的（比如可能抽取了一些图像特征之类的），如果我们擅自改变中间的数据分布，势必会影响到之后的网络层的结果，换言之，可能改变了网络原有的特征表达能力。因此，我们需要一些额外的措施来弥补这里的损失。 为了恢复网络的表达能力，作者在对数据归一化后，又进行了「还原」操作，这一步应该是整个 BN 算法的精髓。前面提到，归一化就是减去均值同时除以标准差，那还原的公式就变成： \\[ x_i^{(k)}=\\hat x^{(k)} \\sqrt{\\sigma_{\\beta}^2+\\epsilon}+\\mu_{\\beta}^{(k)} \\tag{2} \\] 即先对 \\(\\hat x^{(k)}\\) 的尺寸进行缩放，然后平移 \\(\\mu_{\\beta}^{(k)}\\) 单位。 不过，如果只是这样直接还原的话，那之前的归一化操作就没有意义了，因此作者兵行险招，引入两个参数：用 \\(\\gamma^{(k)}\\) 表示缩放因子，用 \\(\\beta^{(k)}\\) 表示平移距离。然后将之前的还原公式替换成： \\[ y_i^{(k)} \\leftarrow \\gamma^{(k)} \\hat x_i^{(k)}+\\beta^{(k)} \\tag{3} \\] 一开始看到这一步一直很纳闷，这不是换汤不换药吗？不过，这个公式跟之前那个公式有一点不同，那就是缩放的因子和平移的距离在这里不再是固定的，而是两个参数（当然是每一层都会对应两个参数）。这两个参数是可以当作网络的参数进行训练的，相当于我们在原先网络的每个激活函数前又加入一个 BN 层来预处理数据。将它们统一进网络后，就跟其他参数一样，可以通过 BP 进行梯度下降了。在初始化的时候，我们将 \\(\\gamma\\) 初始化为 1.0，将 \\(\\beta\\) 初始化为 0，那么在网络开始训练时，这两个参数对「还原」来说是不起作用的。即刚开始训练的时候，中间每层网络的数据都会经过归一化，从而避免激活函数导数为 0 的问题。随着网络逐渐优化，\\(\\gamma\\) 和 \\(\\beta\\) 会逐渐得到训练，因此中间层的「还原」力度会越来越明显。按照作者的意思（其实是我对论文的理解），当网络优化到一定程度时，\\(\\gamma\\) 和 \\(\\beta\\) 可以还原出原来的数值，这样就可以保证网络对特征的表达能力不会受影响，而此时网络也基本训练完毕了，因此即使梯度消失也就无关紧要了。 不过，我之所以认为这是兵行险招，是因为我不太确定这两个参数是否真能还原回原来的数值。论文中并没有很好的证明，因此这一步我目前还不是很理解。 BN 的算法流程（简单起见去掉符号 (k)）： 这里面的 \\(x_i\\) 并不是网络的训练样本，而是指原网络中任意一个隐藏层的激活函数的输入，当然这些输入也是靠训练样本在网络中前向传播得来的。加了 BN 层后，激活函数的输入就替换为 \\(y_i\\)。 注意到，网络是针对一个 Batch 进行训练的，中间的平均值 \\(\\mu_\\beta\\) 和标准差 \\(\\sigma_\\beta\\) 也是针对这个 Batch 计算的，这也是算法名称的由来。 由于我们把 BP 当作是网络中的一个隐藏层，在梯度下降时，可以用 BP 算法求出 \\(\\gamma\\) 和 \\(\\beta\\) 的导数： 然后把它们也当作网络的参数进行训练即可。 预测阶段 预测阶段不同于训练的地方在于，我们没法通过 Batch 计算出 \\(\\mu_\\beta\\) 和 \\(\\sigma_\\beta\\) 的值。于是作者用所有训练样本的均值和方差来进行估算。具体做法可以用下面两个式子概括： \\[ E[x] \\leftarrow E_\\beta [\\mu_\\beta] \\tag{4} \\] \\[ Var[x] \\leftarrow \\frac{m}{m-1}E_\\beta [{\\sigma_\\beta} ^2] \\tag{5} \\] 这里的 \\(\\mu_\\beta\\) 和 \\(\\sigma_\\beta\\) 指的是某个 mini-batch 样本的均值和标准差。也就是说，作者是在每个 batch 的均值和标准差的基础上，再求出整体的均值和标准差。要注意的是，在求标准差的时候，用了无偏估计 \\(\\frac{m}{m-1}\\)。 不过在预测阶段，其实我们不需要再做归一化加快训练了，所以理论上是可以把 BN 层去掉的。但由于训练时已经将 BN 层和其他的网络层当作一个整体了，直接去掉又会出问题。因此，作者用了一点 trick，他把 BN 中原来归一化的操作去掉，并在「还原」那一步里加入一些措施来抵消「还原」。具体来讲，就是将原来 BN 层的操作替换为： \\[ y=\\frac{\\gamma}{\\sqrt{Var[x]+\\epsilon}}x+(\\beta - \\frac{\\gamma E[x]}{\\sqrt{Var[x]+\\epsilon}}) \\tag{6} \\] 在分析这个式子之前，我们先回顾一下公式 (3)。在公式 (3) 里面，\\(y=\\gamma x+\\beta\\)，这一步是为了将归一化后的 \\(x\\) 还原，因此，在作者的想象中，\\(\\gamma\\) 和 \\(\\beta\\) 代表的真实含义其实是样本的标准差和均值，尽管它们也是训练出来的，但作者的想法应该是：它们最终会被优化成接近总样本的标准差和均值。按照这个假设，在公式 (6) 中，\\(\\frac{\\gamma}{\\sqrt{Var[x]+\\epsilon}}\\) 其实就抵消掉了，后面那个偏移量也同理抵消了，所以公式 (6) 本质上就是：\\(y=x\\)。相当于预测的时候，BN 层不起任何作用。 BN 算法总体流程： 这个算法流程图中的细节在前面都做了详细的解释（当然是按照我自己的理解），所以这里就不展开讲了。 关于代码实现，由于我平时只用 tensorflow，所以参考的实现代码也是基于 tensorflow 的，具体可以参考莫烦Python的教程。 另外，上面的介绍都是针对一般的全联接网络的，具体对于 CNN 或者 RNN 这些网络结构，BN 算法需要做一些修改，鉴于目前我还没有深入学习，因此也就不继续深入讲了，有兴趣的读者还请参考其他资料。 总结 总的来说，BN 其实就是将数据的分布由原来激活函数的收敛区调整到梯度较大的区域，类似于数据的归一化处理，不过为了保持原来网络的特征表达能力，引入一些措施将调整后的数据又还原回去。 但要注意的一点是，由于 BN 只关注于激活函数收敛导致的梯度消失问题，因此，在实际使用中，梯度仍然可能消失（比如：链式求导中，导数的累乘效应可能也会导致梯度消失，具体可以看之前的读书笔记）。在之后的文章中，我将介绍另一种解决梯度消失的方法——深度残差学习 (deep residual learning)，这种方法效果上比 BN 更好。 ================ UPDATE 2018.3.15 ================ 这篇文章刚开始写完时一直在思考一个问题：BN 提出来的时候，ReLu 函数已经大行其道了，而 ReLu 不存在 sigmoid 函数梯度消失的问题，那 BN 不是没什么卵用吗？ 今天看了李宏毅教授的视频，发现 BN 其实是要解决另一个更严重的问题，而顺带把函数梯度消失的问题解决了，所以，我上面扯了这么多，居然一直没抓住重点～囧～ BN 其实是要解决 Internal Covariate Shift，没错，这几个关键词就在论文题目里，居然给我省略了。。。这个问题我没有追根溯源去寻找其他文献，但李教授用了一张图让我明白了是怎么回事： 这张图里，同一个特征的两个维度上 \\(x_1\\)、\\(x_2\\) 的值的范围是不一样的，这种差别导致训练的时候会非常的缓慢，除非我们针对不同的维度设置不同的学习率，但这是很难办到的。其实，熟悉 Andrew NG 机器学习入门课的人也应该会回忆起 NG 提到的这个问题： 这个问题和李教授图里的内容是一样的，当时 NG 是提到用 feature scaling 的方法解决它。 BN 其实就是在做 feature scaling，而且它的目的也是为了在训练的时候避免这种 Internal Covariate Shift 的问题，只是刚好也解决了 sigmoid 函数梯度消失的问题。 参考 Batch Normalization导读 卷积神经网络CNN（2）—— BN(Batch Normalization) 原理与使用过程详解 莫烦PYTHON 白化 (whiten) 李宏毅 Batch Normalization","raw":null,"content":null,"categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://jermmy.github.io/categories/机器学习/"}],"tags":[{"name":"深度学习","slug":"深度学习","permalink":"https://jermmy.github.io/tags/深度学习/"},{"name":"论文","slug":"论文","permalink":"https://jermmy.github.io/tags/论文/"}]},{"title":"读书笔记：梯度是怎么消失的","slug":"2017-8-26-reading-notes-neuralnetworkanddeeplearning-5","date":"2017-08-26T03:06:11.000Z","updated":"2018-03-23T02:28:28.000Z","comments":true,"path":"2017/08/26/2017-8-26-reading-notes-neuralnetworkanddeeplearning-5/","link":"","permalink":"https://jermmy.github.io/2017/08/26/2017-8-26-reading-notes-neuralnetworkanddeeplearning-5/","excerpt":"(本文是根据 neuralnetworksanddeeplearning 这本书的第五章Why are deep neural networks hard to train? 整理而成的读书笔记，根据个人口味做了删减)\n在之前的笔记中，我们已经学习了神经网络最核心的 BP 算法，以及一些改进的方案（如：引入交叉熵函数）来提高网络训练的速度。但如果仔细思考会发现，前面例子中的网络都很「浅」，最多就一两个隐藏层，而一旦网络层数增加，有很多问题将不可避免地暴露出来。今天，我们就来认识一个最蛋疼的问题：深度神经网络非常难训练。","text":"(本文是根据 neuralnetworksanddeeplearning 这本书的第五章Why are deep neural networks hard to train? 整理而成的读书笔记，根据个人口味做了删减) 在之前的笔记中，我们已经学习了神经网络最核心的 BP 算法，以及一些改进的方案（如：引入交叉熵函数）来提高网络训练的速度。但如果仔细思考会发现，前面例子中的网络都很「浅」，最多就一两个隐藏层，而一旦网络层数增加，有很多问题将不可避免地暴露出来。今天，我们就来认识一个最蛋疼的问题：深度神经网络非常难训练。 网络越深效果越好 近几年的研究已经表明，网络的层数越深，模型的表达能力就越强。在图像识别中，网络的第一层学会如何识别边缘，第二层在第一层的基础上学会如何识别更复杂的形状，如三角形等，然后第三层又继续在第二层的基础上学会识别出更复杂的形状。如此往复，最后网络便学会识别出更高级的语义信息。这也是为什么深度学习近几年能取得突破的，因为深度神经网络的表达能力实在太强了。 不过，在训练深度神经网络的过程中，人们也遇到一个严重的问题：当后面层的网络飞快训练时，前面层的网络却「僵住」了，参数不再更新；有时候，事情又刚好相反，前面层的网络训练飞快，后面层的网络却收敛了。 通过本节的学习，我们将了解这一切背后的深层原因。 消失的梯度 沿袭之前 MNIST 的例子，我们先做几组实验，来看看什么是梯度消失。 这几组实验中，我们的网络结构分别如下： 1234net = network2.Network([784, 30, 10])net = network2.Network([784, 30, 30, 10])net = network2.Network([784, 30, 30, 30, 10])net = network2.Network([784, 30, 30, 30, 30, 10]) 这几个网络的唯一区别是，每一个网络都比前面的多了一个包含 30 个神经元的隐藏层。实验中，其他参数，包括训练数据完全一样。在 MNIST 数据集上，得出这四个实验的准确率分别为：96.48%，96.90%，96.57%，96.53%。 看得出来，第二个网络训练的结果比第一个好一些，但当隐藏层继续增加时，效果反而下降了。这让我们很惊奇，不是说网络层数越深，效果越好吗？况且，即使中间的网络什么都没有学习到，也总不至于其负作用吧。 为了进一步了解背后的原因，我们打算跟踪一下网络的参数，确认网络是否真的得到训练。 简单起见，我们分析一下第二个网络 ([784, 30, 30, 10]) 中两个隐藏层的梯度。下图演示了当训练开始时，这两个层里面每个神经元的梯度值，为了方便，只摘取了前六个神经元： gradient 图中神经元上的柱状图表示梯度值 \\(\\partial C/ \\partial b\\)，在 BP 的四个公式中，我们知道：\\(\\frac{\\partial C}{\\partial b_j^{l}}=\\delta_j^l \\tag{BP3}\\) \\(\\frac{\\partial C}{\\partial w_{jk}^{l}}=a_{k}^{l-1}\\delta_{j}^{l} \\tag{BP4}\\) 所以，柱状图表示的除了是偏差 bias 的梯度外，也多少可以反应权重 weights 的梯度。 由于权重的初始化是随机的，所以每个神经元的梯度都有所不同，不过很明显的一点是，第 2 个隐藏层的梯度总体上比第 1 个隐藏层要大，而梯度越大，学习速度也相对的越快。 为了探究这是否是偶然（也许这两层中后面的神经元会不同呢），我们决定用一个全局的梯度向量 \\(\\delta\\) 来比较这两个隐藏层参数的总体梯度情况。我们定义 \\(\\delta_j^l=\\partial C/ \\partial b_j^l\\)，所以你可以把 \\(\\delta\\) 看作是上图中所有神经元梯度的向量。我们用向量长度 \\(||\\delta^i||\\) 来代表每 i 个隐藏层的学习速度。 当只有两个隐藏层时（即上图），\\(||\\delta^1||=0.07\\)、\\(||\\delta^2||=0.31\\)，这进一步验证了：第二个隐藏层的学习率高于第一个隐藏层。 如果有三个隐藏层呢？结果是：\\(||\\delta^1||=0.012\\)、\\(||\\delta^2||=0.060\\)、\\(||\\delta^3||=0.283\\)。同样的，后面的隐藏层学习速度都比前面的要高一些。 有人可能会说，以上的梯度都是在刚开始训练后某个时刻计算得到的，在网络训练过程中，这些梯度又是否会进一步提升呢？为了解答这个问题，我们计算出之后更多轮学习后的梯度，并绘制成下面的曲线图： 显而易见地是，不管隐藏层有多少，后面层的学习速度都比前一层要高 5 到 10 倍，这样一来，第一个隐藏层的学习速度甚至只有最后一层的百分之一，当后面的参数正大踏步训练的时候，前面层的参数就基本停滞不前了。这种现象，就叫做梯度消失。梯度消失并不意味着网络已经趋于收敛，因为在实验中，我们特意在训练开始时计算出了梯度，对于一个参数随机初始化的网络，要想在刚开始时就让网络趋于收敛，这几乎是不可能的，因此我们认为梯度消失并不是网络收敛引起的。 另外，随着研究深入，我们也会发现，有时候前面层的梯度虽然没有消失，但却变得很大，几乎是后面层的成百上千倍，导致出现了 NaN，简直「爆炸」了。对于这种情况，我们又称之为梯度爆炸。 不管是梯度消失还是爆炸，都是我们不愿看到的。下面我们需要进一步研究这种现象产生的原因，并想办法解决它。 梯度消失的原因 这一节，我们来探讨一下：为什么网络的梯度会消失？或者说，为什么深度神经网络的梯度会如此不稳定。 简单起见，我们来分析一个只有一个神经元的网络： tikz37 \\(b\\) 和 \\(w\\) 表示参数，\\(C\\) 是损失函数，激活函数采用 sigmoid，每层网络的输出为 \\(a_j=\\sigma(z_j)\\)，\\(z_j=w_ja_{j-1}+b_j\\)。 下面，我们要求出 \\(\\partial C/\\partial b_1\\)，看看是什么原因导致这个值很小。 根据 BP 的公式可以推出： tikz38 这个公式看起来略微比较复杂，不急，我们来看看它是怎么来的。由于网络十分简单（只有一条链），所以我们准备从另一个更形象的角度来推出这个式子（BP 也是完全可以推出该式子的）。 假设有一个增量 \\(\\Delta b_1\\) 出现，由于 \\(a_1=\\sigma(z_1)=\\sigma(w_1a_0+b_1)\\)，可以推出： \\(\\Delta a_1 \\approx \\frac{\\partial \\sigma((w_1\\ a_0+b_1)}{\\partial b_1} \\Delta b_1=\\sigma&#39;(z_1)\\Delta b_1\\)（注意 \\(\\Delta a_1\\) 不是导数，而是由 \\(\\Delta b_1\\) 引起的增量，所以是斜率乘以 \\(\\Delta b_1\\)）。 然后进一步的，\\(\\Delta a_1\\) 又会引起 \\(z_2\\) 的变化，根据 \\(z_2=w_2 a_1+b_2\\) 可以得出： \\(\\Delta z_2 \\approx \\frac{\\partial z_2}{\\partial a_1}\\Delta a_1=w_2 \\Delta a_1\\)。 将之前 \\(\\Delta a_1\\) 的公式代入上式就可以得到： \\(\\Delta z_2 \\approx \\sigma&#39;(z_1)w_2 \\Delta b_1\\)。 可以看出，这个式子和我们最开始的式子已经很相似了。之后，我们依葫芦画瓢不断往后计算，就可以得到 \\(C\\) 的增量： \\(\\Delta C \\approx \\sigma&#39;(z_1)w_2 \\sigma&#39;(z_2) \\ldots \\sigma&#39;(z_4) \\frac{\\partial C}{\\partial a_4} \\Delta b_1 \\tag{120}\\) 除以 \\(\\Delta b_1\\) 后，就可以得到最开始的等式： \\(\\frac{\\partial C}{\\partial b_1} = \\sigma&#39;(z_1) w_2 \\sigma&#39;(z_2) \\ldots\\sigma&#39;(z_4) \\frac{\\partial C}{\\partial a_4}.\\tag{121}\\) 为什么梯度会消失 有了上面这个式子做铺垫，你是否已经猜出了梯度消失的原因。没错，就跟 \\(0.9^n \\approx 0\\) 道理一样。 首先，我们回顾一下 \\(\\sigma&#39;()\\) 函数的图像： 这个函数最大值才 1/4。加上我们的参数 \\(W\\) 是根据均值为 0，标准差为 1 的高斯分布初始化的，即 \\(|w_j|&lt;1\\) ，所以\\(|w_j \\sigma&#39;(z_j)&lt;1/4|\\)。这些项累乘起来，最后的结果就会越来越小。再注意看下面这幅图，由于不同隐藏层的导数累乘的数量不同，因此对应的梯度也就有了高低之分。 tikz39 以上的推导虽然不是很正式，但它已经足够阐明问题的根源。 梯度爆炸的问题这里就不再赘述了，原理和梯度消失一样，当每一项的值都大于 1 时，累乘起来就会变得很大。 记得在之前的学习笔记的最后，我曾经提出一个问题：尽管交叉熵函数解决了网络学习速度下降的问题，但它针对的只是最后一层，对于前面的隐藏层，学习速度依然可能下降。作者之前之所以避而不谈这个问题，是因为之前针对的网络层数都很少，而本文中也已经显示地点出并分析了问题的根源。 复杂网络中的梯度同样不稳定 上面的例子中我们只是用了一个简单的例子来解释原因，在更复杂的网络中，我们仍然可以用类似的方法解释梯度的不稳定现象。 例如，对于下面这个复杂的网络： tikz40 我们可以借助 BP 公式推出： \\[ \\begin{eqnarray} \\delta^l = \\Sigma&#39;(z^l) (w^{l+1})^T \\Sigma&#39;(z^{l+1}) (w^{l+2})^T \\ldots \\Sigma&#39;(z^L) \\nabla_a C \\tag{124}\\end{eqnarray} \\] 这里面，\\(\\Sigma&#39;(z^l)\\) 是对角矩阵，矩阵对角线上的元素由 \\(\\sigma&#39;(z)\\) 的值构成。\\(\\nabla_a C\\) 则是由 \\(C\\) 对 输出层求偏导后得来的向量。 这个式子尽管许多，但形式上依然是一样的，最后矩阵相乘的累积效应依然会导致梯度消失或者爆炸。 深度学习的其他障碍 虽然这一章中我们只是提到梯度不稳定的问题，但事实上，有很多研究显示：深度学习同样存在很多其他的障碍。 比如：激活函数的选择会影响网络的学习（参见论文：Understanding the difficulty of training deep feedforward neural networks）。 又比如：参数的初始化也会影响网络的训练（参见论文：On the importance of initialization and momentum in deep learning）。 可见，关于深度神经网络的训练障碍，目前还是一个复杂的问题，需要更进一步的研究。在下一章中，我们将继续学习一些深度学习的方法，这些方法在某种程度上，可以克服深度神经网络的这些学习障碍。 参考 Why are deep neural networks hard to train?","raw":null,"content":null,"categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://jermmy.github.io/categories/机器学习/"}],"tags":[{"name":"深度学习","slug":"深度学习","permalink":"https://jermmy.github.io/tags/深度学习/"}]},{"title":"TensorFlow学习笔记：共享变量","slug":"2017-8-25-learn-tensorflow-shared-variables","date":"2017-08-25T14:45:57.000Z","updated":"2018-03-08T07:41:44.000Z","comments":true,"path":"2017/08/25/2017-8-25-learn-tensorflow-shared-variables/","link":"","permalink":"https://jermmy.github.io/2017/08/25/2017-8-25-learn-tensorflow-shared-variables/","excerpt":"本文是根据 TensorFlow 官方教程翻译总结的学习笔记，主要介绍了在 TensorFlow 中如何共享参数变量。\n教程中首先引入共享变量的应用场景，紧接着用一个例子介绍如何实现共享变量（主要涉及到 tf.variable_scope()和tf.get_variable()两个接口），最后会介绍变量域 (Variable Scope) 的工作方式。","text":"本文是根据 TensorFlow 官方教程翻译总结的学习笔记，主要介绍了在 TensorFlow 中如何共享参数变量。 教程中首先引入共享变量的应用场景，紧接着用一个例子介绍如何实现共享变量（主要涉及到 tf.variable_scope()和tf.get_variable()两个接口），最后会介绍变量域 (Variable Scope) 的工作方式。 遇到的问题 假设我们创建了一个简单的 CNN 网络： 1234567891011121314def my_image_filter(input_images): conv1_weights = tf.Variable(tf.random_normal([5, 5, 32, 32]), name=\"conv1_weights\") conv1_biases = tf.Variable(tf.zeros([32]), name=\"conv1_biases\") conv1 = tf.nn.conv2d(input_images, conv1_weights, strides=[1, 1, 1, 1], padding='SAME') relu1 = tf.nn.relu(conv1 + conv1_biases) conv2_weights = tf.Variable(tf.random_normal([5, 5, 32, 32]), name=\"conv2_weights\") conv2_biases = tf.Variable(tf.zeros([32]), name=\"conv2_biases\") conv2 = tf.nn.conv2d(relu1, conv2_weights, strides=[1, 1, 1, 1], padding='SAME') return tf.nn.relu(conv2 + conv2_biases) 这个网络中用 tf.Variable() 初始化了四个参数。 不过，别看我们用一个函数封装好了网络，当我们要调用网络进行训练时，问题就会变得麻烦。比如说，我们有 image1 和 image2 两张图片，如果将它们同时丢到网络里面，由于参数是在函数里面定义的，这样一来，每调用一次函数，就相当于又初始化一次变量： 1234# First call creates one set of 4 variables.result1 = my_image_filter(image1)# Another set of 4 variables is created in the second call.result2 = my_image_filter(image2) 当然了，我们很快也能找到解决办法，那就是把参数的初始化放在函数外面，把它们当作全局变量，这样一来，就相当于全局「共享」了嘛。比如说，我们可以用一个 dict 在函数外定义参数： 12345678910111213141516171819variables_dict = &#123; \"conv1_weights\": tf.Variable(tf.random_normal([5, 5, 32, 32]), name=\"conv1_weights\") \"conv1_biases\": tf.Variable(tf.zeros([32]), name=\"conv1_biases\") ... etc. ...&#125;def my_image_filter(input_images, variables_dict): conv1 = tf.nn.conv2d(input_images, variables_dict[\"conv1_weights\"], strides=[1, 1, 1, 1], padding='SAME') relu1 = tf.nn.relu(conv1 + variables_dict[\"conv1_biases\"]) conv2 = tf.nn.conv2d(relu1, variables_dict[\"conv2_weights\"], strides=[1, 1, 1, 1], padding='SAME') return tf.nn.relu(conv2 + variables_dict[\"conv2_biases\"])# The 2 calls to my_image_filter() now use the same variablesresult1 = my_image_filter(image1, variables_dict)result2 = my_image_filter(image2, variables_dict) 不过，这种方法对于熟悉面向对象的你来说，会不会有点别扭呢？因为它完全破坏了原有的封装。也许你会说，不碍事的，只要将参数和filter函数都放到一个类里即可。不错，面向对象的方法保持了原有的封装，但这里出现了另一个问题：当网络变得很复杂很庞大时，你的参数列表/字典也会变得很冗长，而且如果你将网络分割成几个不同的函数来实现，那么，在传参时将变得很麻烦，而且一旦出现一点点错误，就可能导致巨大的 bug。 为此，TensorFlow 内置了变量域这个功能，让我们可以通过域名来区分或共享变量。通过它，我们完全可以将参数放在函数内部实例化，再也不用手动保存一份很长的参数列表了。 用变量域实现共享参数 这里主要包括两个函数接口： tf.get_variable(&lt;name&gt;, &lt;shape&gt;, &lt;initializer&gt;) ：根据指定的变量名实例化或返回一个 tensor 对象； tf.variable_scope(&lt;scope_name&gt;)：管理 tf.get_variable() 变量的域名。 tf.get_variable() 的机制跟 tf.Variable() 有很大不同，如果指定的变量名已经存在（即先前已经用同一个变量名通过 get_variable() 函数实例化了变量），那么 get_variable()只会返回之前的变量，否则才创造新的变量。 现在，我们用 tf.get_variable() 来解决上面提到的问题。我们将卷积网络的两个参数变量分别命名为 weights 和 biases。不过，由于总共有 4 个参数，如果还要再手动加个 weights1 、weights2 ，那代码又要开始恶心了。于是，TensorFlow 加入变量域的机制来帮助我们区分变量，比如： 12345678910111213141516171819def conv_relu(input, kernel_shape, bias_shape): # Create variable named \"weights\". weights = tf.get_variable(\"weights\", kernel_shape, initializer=tf.random_normal_initializer()) # Create variable named \"biases\". biases = tf.get_variable(\"biases\", bias_shape, initializer=tf.constant_initializer(0.0)) conv = tf.nn.conv2d(input, weights, strides=[1, 1, 1, 1], padding='SAME') return tf.nn.relu(conv + biases)def my_image_filter(input_images): with tf.variable_scope(\"conv1\"): # Variables created here will be named \"conv1/weights\", \"conv1/biases\". relu1 = conv_relu(input_images, [5, 5, 32, 32], [32]) with tf.variable_scope(\"conv2\"): # Variables created here will be named \"conv2/weights\", \"conv2/biases\". return conv_relu(relu1, [5, 5, 32, 32], [32]) 我们先定义一个 conv_relu() 函数，因为 conv 和 relu 都是很常用的操作，也许很多层都会用到，因此单独将这两个操作提取出来。然后在 my_image_filter() 函数中真正定义我们的网络模型。注意到，我们用 tf.variable_scope() 来分别处理两个卷积层的参数。正如注释中提到的那样，这个函数会在内部的变量名前面再加上一个「scope」前缀，比如：conv1/weights表示第一个卷积层的权值参数。这样一来，我们就可以通过域名来区分各个层之间的参数了。 不过，如果直接这样调用 my_image_filter，是会抛异常的： 123result1 = my_image_filter(image1)result2 = my_image_filter(image2)# Raises ValueError(... conv1/weights already exists ...) 因为 tf.get_variable()虽然可以共享变量，但默认上它只是检查变量名，防止重复。要开启变量共享，你还必须指定在哪个域名内可以共用变量： 1234with tf.variable_scope(\"image_filters\") as scope: result1 = my_image_filter(image1) scope.reuse_variables() result2 = my_image_filter(image2) 到这一步，共享变量的工作就完成了。你甚至都不用在函数外定义变量，直接调用同一个函数并传入不同的域名，就可以让 TensorFlow 来帮你管理变量了。 ==================== UPDATE 2018.3.8 ====================== 官方的教程都是一些简单的例子，但在实际开发中，情况可能会复杂得多。比如，有一个网络，它的前半部分是要共享的，而后半部分则是不需要共享的，在这种情况下，如果还要自己去调用 scope.reuse_variables() 来决定共享的时机，无论如何都是办不到的，比如下面这个例子： 123456789def test(mode): w = tf.get_variable(name=mode+\"w\", shape=[1,2]) u = tf.get_variable(name=\"u\", shape=[1,2]) return w, uwith tf.variable_scope(\"test\") as scope: w1, u1 = test(\"mode1\") # scope.reuse_variables() w2, u2 = test(\"mode2\") 这个例子中，我们要使用两个变量： w 和 u，其中 w 是不共享的，而 u 是共享的。在这种情况下，不管你加不加 scope.reuse_variables()，代码都会出错。因此，Tensorflow 提供另一种开启共享的方法： 12345678def test(mode): w = tf.get_variable(name=mode+\"w\", shape=[1,2]) u = tf.get_variable(name=\"u\", shape=[1,2]) return w, uwith tf.variable_scope(\"test\", reuse=tf.AUTO_REUSE) as scope: w1, u1 = test(\"mode1\") w2, u2 = test(\"mode2\") 这里只是加了一个参数 reuse=tf.AUTO_REUSE，但正如名字所示，这是一种自动共享的机制，当系统检测到我们用了一个之前已经定义的变量时，就开启共享，否则就重新创建变量。这几乎是「万金油」式的写法😈。 背后的工作方式 变量域的工作机理 接下来我们再仔细梳理一下这背后发生的事情。 我们要先搞清楚，当我们调用 tf.get_variable(name, shape, dtype, initializer) 时，这背后到底做了什么。 首先，TensorFlow 会判断是否要共享变量，也就是判断 tf.get_variable_scope().reuse 的值，如果结果为 False（即你没有在变量域内调用scope.reuse_variables()），那么 TensorFlow 认为你是要初始化一个新的变量，紧接着它会判断这个命名的变量是否存在。如果存在，会抛出 ValueError 异常，否则，就根据 initializer 初始化变量： 123with tf.variable_scope(\"foo\"): v = tf.get_variable(\"v\", [1])assert v.name == \"foo/v:0\" 而如果 tf.get_variable_scope().reuse == True，那么 TensorFlow 会执行相反的动作，就是到程序里面寻找变量名为 scope name + name 的变量，如果变量不存在，会抛出 ValueError 异常，否则，就返回找到的变量： 12345with tf.variable_scope(\"foo\"): v = tf.get_variable(\"v\", [1])with tf.variable_scope(\"foo\", reuse=True): v1 = tf.get_variable(\"v\", [1])assert v1 is v 了解变量域背后的工作方式后，我们就可以进一步熟悉其他一些技巧了。 变量域的基本使用 变量域可以嵌套使用： 1234with tf.variable_scope(\"foo\"): with tf.variable_scope(\"bar\"): v = tf.get_variable(\"v\", [1])assert v.name == \"foo/bar/v:0\" 我们也可以通过 tf.get_variable_scope() 来获得当前的变量域对象，并通过 reuse_variables() 方法来设置是否共享变量。不过，TensorFlow 并不支持将 reuse 值设为 False，如果你要停止共享变量，可以选择离开当前所在的变量域，或者再进入一个新的变量域（比如，再进入一个 with 语句，然后指定新的域名）。 还需注意的一点是，一旦在一个变量域内将 reuse 设为 True，那么这个变量域的子变量域也会继承这个 reuse 值，自动开启共享变量： 1234567891011121314with tf.variable_scope(\"root\"): # At start, the scope is not reusing. assert tf.get_variable_scope().reuse == False with tf.variable_scope(\"foo\"): # Opened a sub-scope, still not reusing. assert tf.get_variable_scope().reuse == False with tf.variable_scope(\"foo\", reuse=True): # Explicitly opened a reusing scope. assert tf.get_variable_scope().reuse == True with tf.variable_scope(\"bar\"): # Now sub-scope inherits the reuse flag. assert tf.get_variable_scope().reuse == True # Exited the reusing scope, back to a non-reusing one. assert tf.get_variable_scope().reuse == False 捕获变量域对象 如果一直用字符串来区分变量域，写起来容易出错。为此，TensorFlow 提供了一个变量域对象来帮助我们管理代码： 123456789with tf.variable_scope(\"foo\") as foo_scope: v = tf.get_variable(\"v\", [1])with tf.variable_scope(foo_scope) w = tf.get_variable(\"w\", [1])with tf.variable_scope(foo_scope, reuse=True) v1 = tf.get_variable(\"v\", [1]) w1 = tf.get_variable(\"w\", [1])assert v1 is vassert w1 is w 记住，用这个变量域对象还可以让我们跳出当前所在的变量域区域： 1234567with tf.variable_scope(\"foo\") as foo_scope: assert foo_scope.name == \"foo\"with tf.variable_scope(\"bar\") with tf.variable_scope(\"baz\") as other_scope: assert other_scope.name == \"bar/baz\" with tf.variable_scope(foo_scope) as foo_scope2: assert foo_scope2.name == \"foo\" # Not changed. 在变量域内初始化变量 每次初始化变量时都要传入一个 initializer，这实在是麻烦，而如果使用变量域的话，就可以批量初始化参数了： 1234567891011with tf.variable_scope(\"foo\", initializer=tf.constant_initializer(0.4)): v = tf.get_variable(\"v\", [1]) assert v.eval() == 0.4 # Default initializer as set above. w = tf.get_variable(\"w\", [1], initializer=tf.constant_initializer(0.3)): assert w.eval() == 0.3 # Specific initializer overrides the default. with tf.variable_scope(\"bar\"): v = tf.get_variable(\"v\", [1]) assert v.eval() == 0.4 # Inherited default initializer. with tf.variable_scope(\"baz\", initializer=tf.constant_initializer(0.2)): v = tf.get_variable(\"v\", [1]) assert v.eval() == 0.2 # Changed default initializer. 参考 TensorFlow官方教程","raw":null,"content":null,"categories":[{"name":"TensorFlow","slug":"TensorFlow","permalink":"https://jermmy.github.io/categories/TensorFlow/"}],"tags":[{"name":"深度学习","slug":"深度学习","permalink":"https://jermmy.github.io/tags/深度学习/"},{"name":"TensorFlow","slug":"TensorFlow","permalink":"https://jermmy.github.io/tags/TensorFlow/"}]},{"title":"读书笔记：如何初始化神经网络","slug":"2017-8-13-reading-notes-neuralnetworkanddeeplearning-3-3","date":"2017-08-13T03:50:05.000Z","updated":"2018-03-23T02:27:57.000Z","comments":true,"path":"2017/08/13/2017-8-13-reading-notes-neuralnetworkanddeeplearning-3-3/","link":"","permalink":"https://jermmy.github.io/2017/08/13/2017-8-13-reading-notes-neuralnetworkanddeeplearning-3-3/","excerpt":"(本文是根据 neuralnetworksanddeeplearning 这本书的第三章Improving the way neural networks learn整理而成的读书笔记，根据个人口味做了删减)\n上一章，我们介绍了神经网络容易出现的过拟合问题，并学习了最常用的正则化方法，以及其他一些技巧，今天，我们将介绍本章节最后两个问题：权重初始化和超参数的选择","text":"(本文是根据 neuralnetworksanddeeplearning 这本书的第三章Improving the way neural networks learn整理而成的读书笔记，根据个人口味做了删减) 上一章，我们介绍了神经网络容易出现的过拟合问题，并学习了最常用的正则化方法，以及其他一些技巧，今天，我们将介绍本章节最后两个问题：权重初始化和超参数的选择 权重初始化 到目前为止，我们都是用归一化高斯分布来初始化权值，但是，我们很想知道是否有其他初始化方法可以让网络训练得更好。 事实上，确实存在比高斯分布更好的方法。不过，我们需要先了解高斯分布的初始化会存在哪些缺陷。 假设我们有如下的网络结构，其中包含 1000 个输入神经元： tikz32 现在，我们聚焦于隐藏层第一个神经元。假设输入中，有一半的神经元是 0，一半的神经元是 1。输入到隐藏层的权重和为 \\(z=\\sum_j{w_j x_j}+b\\)。由于有一半的 \\(x_j=0\\)，所以 \\(z\\) 相当于是 501 个归一化的高斯分布随机变量的和。因此，\\(z\\) 本身也是一个高斯分布，其均值为 0，标准差为 \\(\\sqrt{501} \\approx 22.4\\)。这是一个很「宽」的分布： 也就是说，大部分情况下 \\(z \\gg 1\\) 或者 \\(z \\ll 1\\)。对于采用 sigmoid 函数的 \\(\\sigma(z)\\) 来说，这就意味着隐藏层可能已经收敛了（所谓收敛，就是训练开始变缓或停止了，而导致收敛的原因在于，偏导中的 \\(\\sigma&#39;(z)\\) 在 \\(|z|&gt;1\\) 时趋于 0，这样梯度下降就没法更新参数了）。之前我们用交叉熵函数解决了输出层中学习率低的问题，但对于中间的隐藏层并没有作用。而且，前一层隐藏层的输出如果也成高斯分布，那么再往后的隐藏层也会收敛。 改善这种问题的措施也很简单，既然问题根源在于高斯分布太「宽」，那么我们就想办法让它变「窄」，也就是标准差要变小。假设一个神经元有 \\(n_{in}\\) 个输入权值，那么我们只需要将所有权值按照均值为 0，标准差为 \\(1/\\sqrt{n_{in}}\\) 的高斯分布初始化即可。这样得到的新的高斯分布就会「瘦高」得多。对于之前的例子，在 500 个输入为 0，500 个为 1 的情况下，新高斯分布的均值为 0，标准差为 \\(\\sqrt{3/2}=1.22…\\)，如下图所示： 这样一来，\\(z\\) 的值普遍在 \\([0, 1]\\) 内，隐藏层过早收敛的情况也就有所缓解了。 我们再通过一组实验来看看不同初始化方法的效果： 其中，橙线是用上面提及的新的高斯分布初始化，而蓝线则是一般的高斯分布。从结果来看，新的初始化方法可以加速网络的训练，但最终的准确率两者相当。不过在某些情况下，\\(1/\\sqrt{n_{in}}\\) 的初始化方式会提高准确率，在下一章中，我们将看到类似的例子。 要注意的一点是，以上的初始化都是针对权值 weight 的，对偏差 bias 的初始化不影响网络的训练（原因暂时没想明白）。 如何选择超参数 到目前为止，我们都没有仔细讨论超参数该如何选择（如学习率 \\(\\eta\\)，正则化参数 \\(\\lambda\\) 等等）。超参数的选择对网络的训练和性能都会产生影响。由于神经网络的复杂性，一旦网络出现问题，我们将很难定位问题的根源，搞不清楚到底是网络结构有问题，还是数据集有问题，还是超参数本身没选好。因此，这一节我们将学习一些选择超参数的「灵感」或者「准则」，减少在超参数选择上的失误。 宽泛的策略 之所以称之为宽泛，是因为这种策略不告诉如何调整超参数，而是让你尽可能快地得到反馈。只有尽快把握网络的学习情况，我们才有耐心和信息继续 debug（总不能每调整一次要等个十来分钟才出结果吧）。我自己在 debug 网络的时候也经常采用这些做法，比如，只用很小的数据集训练，或者将网络的结构变小等等。这些做法只有一个目的：让网络尽可能快地反馈结果，不管结果好坏，这是我们能继续调试下去的前提。在反复调试后，我们往往能得到一些「灵感」，之后再慢慢将问题变的更复杂一些，然后继续调试。 好了，下面我们针对学习率 \\(\\eta\\)、L2 正则化参数 \\(\\lambda\\) 和批训练的数据集大小学习一些比较有效的准则。 学习率 关于学习率的选择，Andrew Ng 在他的 Machine Learning 课程中有过详细的讲解。这里面最重要的是要避免学习率过大给梯度下降带来「抖动」的问题，如下图中的橙线所示。在设置学习率时，我们可以先设置一个小一点的数值，如 0.1，如果这个数值太大，则调低一个数量级到 0.01，甚至 0.001…如果发现学习过程中损失函数没有出现「抖动」的情况，再适当提高学习率，如由原来的 0.1 提高到 0.2、0.5…但最终不能超过造成「抖动」的阈值。 early stopping 选择训练轮数 在神经网络中，并不是训练得越多越好，之前已经提到过，训练太多轮可能导致过拟合。因此，我们要采取尽可能合适的训练轮数。early stopping 的具体做法是：在每一轮训练后观察验证集上的准确率，当验证集准确率不再上升时，就停止训练。这里的准确率不再上升指的是，在连续几轮（比如 10 轮）的训练后，准确率都不再有新的突破，始终维持在一个稳定的数值。 调整学习率 前面说过，学习率过大可能导致梯度下降出现「抖动」，过小又会导致网络训练太慢。在实际过程中，我们常常会遇到这样的问题：当网络开始训练时，由于 weights 不够好，这个时候加大学习率可以快速改善网络；当网络训练一段时间后，梯度下降开始到达最低点，这个时候小一点的学习率可以防治其越过最低点而出现「抖动」。因此，在训练过程中，更好的方法不是固定一个学习率，而是根据验证集上的准确率情况，逐步调整学习率（比如一开始设为 0.1，当准确率上升到 80% 后，调低到 0.01，上升到 90% 后，再继续调低，直到学习率只有初始值的千分之一为止）。 正则化参数 刚开始训练时，最好将正则化参数 \\(\\lambda\\) 设为 0.0，等学习率确定并且网络可以正常训练后，再设置 \\(\\lambda\\)。具体该设置为什么，没有通用的准则，只能根据实际情况判断，可以是 1.0，或者 0.1，或者 10.0。总之，要根据验证集上的准确率来判断。 批训练的数据集大小 理论上，我们完全可以在每次训练时只用一个样本，但这样会导致训练过程相当漫长，而多个样本进行批训练，在当今计算机的快速矩阵运算下并不比单个样本慢，这样相当于同时训练多个样本的时间和单个样本一样（当然，将所有样本都用于训练还是会影响速度，所以才会采用随机梯度训练的批样本）。另外，个人认为，综合多个样本再取均值进行训练，可以抵消部分噪音样本的影响。 参考 Improving the way neural networks learn","raw":null,"content":null,"categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://jermmy.github.io/categories/机器学习/"}],"tags":[{"name":"深度学习","slug":"深度学习","permalink":"https://jermmy.github.io/tags/深度学习/"}]},{"title":"拉格朗日乘子法","slug":"2017-7-27-understand-lagrange-multiplier","date":"2017-07-27T14:50:27.000Z","updated":"2017-12-25T05:33:05.000Z","comments":true,"path":"2017/07/27/2017-7-27-understand-lagrange-multiplier/","link":"","permalink":"https://jermmy.github.io/2017/07/27/2017-7-27-understand-lagrange-multiplier/","excerpt":"最近在学习 SVM 的过程中，遇到关于优化理论中拉格朗日乘子法的知识，本文是根据几篇文章总结得来的笔记。由于是刚刚接触，难免存在错误，还望指出😁。另外，本文不会聊到深层次的数学推导，仅仅是介绍拉格朗日乘子法的内容，应用，以及个人对它的感性理解。\n什么是拉格朗日乘子法\n按照维基百科的定义，拉格朗日乘数法是一种寻找多元函数在其变量受到一个或多个条件的约束时的极值的方法。用数学式子表达为： \\[\n\\underset{x, y} {\\operatorname {minimize}} f(x, y)   \\\\\n\\operatorname{subject\\ to}  g(x, y) = c\n\\] 简单理解就是，我们要在满足 \\(g(x, y)=c\\) 这个等式的前提下，求 \\(f(x, y)\\) 函数的最小值（最大值道理相同）。这样的问题我们在高中的时候就遇到过了，只不过高中时遇到的限制条件 \\(g(x, y)=c\\) 都比较简单，一般而言都可以将 \\(y\\) 用 \\(x\\) 的式子表示出来，然后用变量替换的方法代回 \\(f(x, y)\\) 求解。但是，如果 \\(g(x, y)\\) 的形式过于复杂，或者变量太多时，这种方法就失效了。而拉格朗日乘子法就是解决这类问题的通用策略。","text":"最近在学习 SVM 的过程中，遇到关于优化理论中拉格朗日乘子法的知识，本文是根据几篇文章总结得来的笔记。由于是刚刚接触，难免存在错误，还望指出😁。另外，本文不会聊到深层次的数学推导，仅仅是介绍拉格朗日乘子法的内容，应用，以及个人对它的感性理解。 什么是拉格朗日乘子法 按照维基百科的定义，拉格朗日乘数法是一种寻找多元函数在其变量受到一个或多个条件的约束时的极值的方法。用数学式子表达为： \\[ \\underset{x, y} {\\operatorname {minimize}} f(x, y) \\\\ \\operatorname{subject\\ to} g(x, y) = c \\] 简单理解就是，我们要在满足 \\(g(x, y)=c\\) 这个等式的前提下，求 \\(f(x, y)\\) 函数的最小值（最大值道理相同）。这样的问题我们在高中的时候就遇到过了，只不过高中时遇到的限制条件 \\(g(x, y)=c\\) 都比较简单，一般而言都可以将 \\(y\\) 用 \\(x\\) 的式子表示出来，然后用变量替换的方法代回 \\(f(x, y)\\) 求解。但是，如果 \\(g(x, y)\\) 的形式过于复杂，或者变量太多时，这种方法就失效了。而拉格朗日乘子法就是解决这类问题的通用策略。 拉格朗日乘子法的原理 一个约束条件 我们先从只有一个约束条件的情况入手，看看拉格朗日乘子法到底是怎么做的。 假设，我们的问题如下： \\[ \\underset{x,y} {\\operatorname{minimize}} f(x, y)=x^2+y^2 \\\\\\\\ {\\operatorname {subject to}}\\ g(x, y)=xy-1=0 \\] 当然，这个问题比较简单，直接用 \\(g(x, y)\\) 解出 y 再代入 \\(f(x, y)\\) 也可以求解，但这里，我们准备用拉格朗日乘子法。 首先我们画出 \\(f(x, y)\\) 的图像，这个图像应该是 3 维的，但为了方便讲解，这里给出它的 2 维投影： 图中的红色圆表示 \\(f(x, y)\\)，越靠近原点的部分，值越小（表示“谷底”），这些圆又称为「等高线」，因为同一个圆代表的函数值相同。 图中的蓝线代表 \\(g(x, y)\\)，这里只取 \\(g(x, y)=0\\) 的部分。 整幅图像可以想象成一个巨大的山谷，原点是谷底，而我们的任务是在蓝线表示的道路上，找到最低的位置。 那要如何找到这个最低点呢？注意，图中用橙色和黑色标记了两个点。如果我们走到了橙色这个位置，那么很明显，可以发现这个点肯定不是最低的，因为我们可以沿着蓝线继续往内部的圆走，当我们走到黑色这个点时，会发现没法再往里面走了，而且，这个时候如果继续沿蓝线走，我们的位置反而升高了，这时，我们基本可以认为：我们找到了在蓝线这个限制条件下的最低点。 那么橙色这个点和黑色这个点有什么本质区别呢？拉格朗日观察到，黑点位置，蓝线和圆是相切的，而橙点位置显然不满足这个性质。那相切是否是必然的呢？拉格朗日告诉我们，是的，一定是相切的。而这一点，正是拉格朗日乘子法的核心。 梯度 在正式理解拉格朗日乘子法的原理之前，我们要回顾一下梯度的概念。 在数学里面，梯度指的是函数变化最快的方向。例如：在一元函数 \\(f(x)\\) 中，梯度只能沿 x 轴正方向或负方向，而在二元函数 \\(f(x,y)\\) 中，梯度则是一个二维向量 \\((\\partial f/\\partial x,\\partial f/\\partial y)\\)。 现在，我们要用到梯度一个重要的性质：梯度跟函数等高线是垂直的。 证明需要用到一点极限的知识。 梯度的数学定义为：\\(\\nabla f=(\\partial f / \\partial x, \\partial f / \\partial y)\\)。假设 \\(\\Delta x\\)，\\(\\Delta y\\) 是两个极小的变化量，根据全微分的知识，可以得到： \\[ f(x+\\Delta x, y+\\Delta y) \\approx f(x, y)+\\frac{\\partial f}{\\partial x}\\Delta x + \\frac{\\partial f}{\\partial y}\\Delta y \\] 如果 \\((\\Delta x, \\Delta y)\\) 是在等高线方向的增量，那么 \\(f(x+\\Delta x, y+\\Delta y) \\approx f(x, y)\\)，这意味着 \\(\\frac{\\partial f}{\\partial x}\\Delta x + \\frac{\\partial f}{\\partial y}\\Delta y=0\\)，换句话说，向量 \\(\\nabla f\\) 和向量 \\((\\Delta x, \\Delta y)\\) 的内积为 0。所以，梯度和函数的等高线是垂直的。 拉格朗日乘子法的几何认识 现在，我们来感性地认识一下，为什么拉格朗日认为相切才能找到最低点（只是感性认识，不添加任何数学推导）。 在橙点这个位置，由于两条曲线不相切，所以橙线的梯度（上图橙色箭头）和蓝线的切线（蓝色虚线）肯定不垂直。在这种情况下，蓝线的两个切线方向，必定有一个往函数高处走（与梯度的夹角小于 90 度），有一个往函数低处走（与梯度的夹角大于 90 度）。所以，在两条曲线相交时，我们肯定不在最低点或最高点的位置。 那么，反过来想，如果两条曲线相切（上图），那么在切点这个位置，蓝线的切线和橙线的梯度是垂直的，这个时候，蓝线的切线方向都指向橙线的等高线方向。换句话说，在切点的位置沿蓝线移动很小的一步，都相当于在橙线的等高线上移动，这个时候，可以认为函数值已经趋于稳定了。所以，我们认为这个点的值“可能”是最低（高）的（之后解释为什么是“可能“。另外，个人觉得拉格朗日乘子法最好用反证法从不相切的点入手思考，从相切的点思考总有点别扭）。 既然相切可以帮助我们找到最低点，那么接下来我们要研究的便是如何利用相切来找出最低点。 相切，意味着在切点的位置，两条曲线的等高线方向是平行的，考虑到梯度与等高线垂直， 我们可以用两条曲线的梯度平行来求出切点位置（最低点）。 因此，根据梯度平行，我们能够得到一个方程组：\\(\\nabla f = \\lambda \\nabla g\\)，其中 \\(\\lambda\\) 表示一个标量，因为我们虽然能保证两个梯度平行，但不能保证它们的长度一样（或者方向相同）。在高维函数中，\\(\\nabla f\\) 表示的是函数在各个自变量方向的偏导。对于上面的例子，我们可以求出函数 \\(f\\) 和 \\(g\\) 的偏导，再根据方程组： \\[ \\frac{\\partial f}{\\partial x}= \\lambda \\frac{\\partial g}{\\partial x} \\\\\\\\ \\frac{\\partial f}{\\partial y}=\\lambda \\frac{\\partial g}{\\partial y} \\\\\\\\ g(x,y)=0 \\] 求出切点。由于总共有三个方程和三个未知数，一般都能找到解（也可能存在多个解或无解的情况，之后会简单讨论）。 在实际求解时，人们会使用一个统一的拉格朗日函数：\\(L(x,y,\\lambda)=f(x,y)+\\lambda g(x,y)\\)，令这个函数偏导为 0，我们可以得到： \\[ \\partial L/ \\partial x=\\frac{\\partial f} {\\partial x}- \\lambda \\frac{\\partial g}{\\partial x}=0 \\\\\\\\ \\partial L/ \\partial y=\\frac{\\partial f}{\\partial y}- \\lambda \\frac{\\partial g}{\\partial y}=0 \\\\\\\\ \\partial L/ \\partial \\lambda=g(x,y)=0 \\] 结果和上面的方程组是一样的。 多个约束条件 多个约束条件和单个约束条件是一样的。如果是多个约束条件，那么这些约束函数肯定是相交的，否则无解。多个约束条件一般会把变量约束到一个更低维的空间，例如，下图中，紫色球面和黄色平面将变量约束到黑色线的位置。 求解过程和单个约束条件是一样的，我们定义一个新的拉格朗日函数： \\[ L(x_1,\\dots,x_n,\\lambda_1,\\dots,\\lambda_k)=f(x_1,\\dots,x_n)-\\sum_{j=1}^k{\\lambda_j g_j(x_1,\\dots,s_n)} \\] 然后同样令这个函数的导数 \\(\\nabla L=0\\)，最后可以得到 \\(n+k\\) 个方程以及 \\(n+k\\) 个未知数，一般也能求解出来。 总结 根据拉格朗日乘子法的定义，这是一种寻找极值的策略，换句话说，该方法并不能保证找到的一定是最低点或者最高点。事实上，它只是一种寻找极值点的过程，而且，拉格朗日乘子法找到的切点可能不只一个（也就是上面的方程组可能找到多个解），例如下图： 图中相切的点有两个，而红点的函数值明显比黑点小。事实上，要想判断找到的点是极低点还是极高点，我们需要将切点代入原函数再进行判断。 另外，在写作本文时，我仍然有一个疑惑没有解决：拉格朗日乘子法在哪些情况下无解（也就是上面的方程组 \\(\\nabla L\\) 无解）？换句话说，约束条件和函数没有切点时，我们要怎么求出最低点或最高点。这个问题留待之后想通再补上。 参考 wiki: Lagrange multiplier An Introduction to Lagrange Multipliers Understanding Lagrange Multipliers 拉格朗日乘子法如何理解？ SVM - Understanding the math - Duality and Lagrange multipliers","raw":null,"content":null,"categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://jermmy.github.io/categories/机器学习/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"https://jermmy.github.io/tags/机器学习/"},{"name":"优化理论","slug":"优化理论","permalink":"https://jermmy.github.io/tags/优化理论/"}]},{"title":"读书笔记：对抗过拟合","slug":"2017-7-22-reading-notes-neuralnetworkanddeeplearning-3-2","date":"2017-07-22T08:33:47.000Z","updated":"2018-03-23T02:26:30.000Z","comments":true,"path":"2017/07/22/2017-7-22-reading-notes-neuralnetworkanddeeplearning-3-2/","link":"","permalink":"https://jermmy.github.io/2017/07/22/2017-7-22-reading-notes-neuralnetworkanddeeplearning-3-2/","excerpt":"(本文是根据 neuralnetworksanddeeplearning 这本书的第三章Improving the way neural networks learn整理而成的读书笔记，根据个人口味做了删减)\n上一章，我们学习了改善网络训练的损失函数：交叉熵函数。今天要介绍神经网络容易遇到的过拟合（overfitting）问题，以及解决的方法：正则化（regularization）。","text":"(本文是根据 neuralnetworksanddeeplearning 这本书的第三章Improving the way neural networks learn整理而成的读书笔记，根据个人口味做了删减) 上一章，我们学习了改善网络训练的损失函数：交叉熵函数。今天要介绍神经网络容易遇到的过拟合（overfitting）问题，以及解决的方法：正则化（regularization）。 过拟合 过拟合现象 在了解过拟合这个问题之前，我们先做个实验。 假设我们使用一个有 30 个隐藏层，23860 个参数的网络来预测 MNIST 数据集。不过，我们只用数据集中的 1000 张图片进行训练。训练过程和以往一样，损失函数采用交叉熵函数，学习率 \\(\\eta = 0.5\\)，batch 大小为 10，并且训练 400 轮。 下图是训练过程中 cost 的变化： 可以看到，cost 是在逐渐变小的。不过这是否意味着网络被训练得越来越好呢？我们来看看每一轮的准确率情况： 在大概 280 轮训练之前，网络的准确率确实在缓慢上升，但之后，我们看到，准确率基本没有大的改进，始终维持在 82.20 上下。这和 cost 下降的情况是背道而驰的。这种看似得到训练，其实结果很差的情况，就是过拟合（overfitting）。 出现过拟合的原因在于，网络模型的泛化能力很差。也就是说，模型对训练数据的拟合程度非常好，但对未见过的新数据，就几乎没什么拟合能力了。 要更进一步了解过拟合现象，我们再来看看其他实验。 下图是训练过程中，在测试数据上的 cost（之前是训练数据上的）： 图中，cost 在前 15 轮训练中逐渐改善，但之后却又开始上升。这是网络出现过拟合的信号之一。 另一个过拟合的信号请看下图： 这是训练集上的准确率。可以看出，网络的准确率一路上升直到 100%。有人可能会疑惑，准确率高不是好事吗？确实，准确率高是我们需要的，但必须是测试集上的准确率。而训练集上的高准确率，带来的结果未必是好事。它可能意味着，网络在训练数据上「钻了牛角尖」。它并不是学习出如何识别手写体数字，而是仅仅记住了训练数据长什么样。换句话说，它在训练数据上拟合太过了。 过拟合在现代神经网络中是很常见的问题，因为网络参数巨大，一旦训练样本不够丰富，有些参数就可能没有训练到。为了有效地训练网络，我们需要学习能够减少过拟合的技巧。 交叉验证集 在解决过拟合这个问题上，我们需要引入另一个数据集——交叉验证集（validation dataset）。 交叉验证集可以认为是一种双保险措施。在解决过拟合时，我们会用到很多技巧，有些技巧本身就带有自己的参数（也就是我们说的超参数(hyper parameter)），如果只在测试集上试验，结果可能导致我们解决过拟合的措施有针对测试集的「嫌疑」，或者说，在测试集上过拟合了。因此，用一个新的交叉验证集来评估解决的效果，再在测试集上试验，可以让网络模型的泛化能力更强。 三个解决过拟合的小办法 之所以称为小办法，即这种方法虽然有效，但要么作用很小，要么实践意义不大。 early stop 检测过拟合有一个很明显的方法，就是跟踪测试集上的准确率。当准确率不再上升时，就停止训练（early stop）。当然，严格来讲，这不是过拟合的充要条件，可能训练集和测试集上的准确率都停止上升了。但这种策略仍然有助于缓解过拟合问题。 不过，在实践中，我们通常是跟踪验证集上的准确率，而非测试集。 增加训练数据 上图是用所有训练数据进行训练时，训练集和测试集上准确率的变化情况。 可以看出，相比之前只用 1000 个训练样本的情况，网络在训练集和测试集上的准确率只想差了 2.53%（之前是 17.73%）。也就是说，增加训练数据后，过拟合问题很大程度上缓解下来了。所以，增加训练数据也是解决过拟合的办法之一（而且是最简单有效的方法，所谓「算法好不如数据好」）。不过，增加数据不是简单地将数据拷贝复制，而是让数据的种类样式更加丰富。 在真实情况中，训练数据是很难获取的，所以这种方法实践起来很困难。 减少模型参数 减少模型参数本质上和增加训练数据是一样的，不过，对于神经网络而言，参数越多，效果一般会更好，所以这种方法不是逼不得已，我们一般不会采纳。 正则化 L2 正则化 正则化是解决过拟合常用的方法。在这一节中，我们将介绍最常用的正则化技巧：L2 正则化（weight decay）。 L2 正则化是在损失函数中添加正则化项(regularization term)。比如，下面是正则化后的交叉熵函数： \\[ C=-\\frac{1}{n}\\sum_{xj}{[y_j \\ln a_j^L+(1-y_j)\\ln(1-a_j^L)]}+\\frac{\\lambda}{2n}\\sum_w{w^2} \\tag{85} \\] 所谓正则化项，其实就是权值的平方和，前面的 \\(\\lambda / 2n\\) 是针对所有样本取均值，而 \\(\\lambda\\) 就是我们说的超参数。之后会讨论 \\(\\lambda\\) 的值该如何取。注意，正则项中并没有偏差，因为对偏差的正则化效果不明显，所以一般只对权值进行正则化。 L2 正则化也可以用在其他损失函数中，比如平方差函数： \\[ C=\\frac{1}{2n}\\sum_x{||t-a^L||^2}+\\frac{\\lambda}{2n}\\sum_w{w^2} \\tag{86} \\] 我们可以写出 L2 正则化的通式： \\[ \\begin{eqnarray} C = C_0 + \\frac{\\lambda}{2n}\\sum_w w^2,\\tag{87}\\end{eqnarray} \\] 其中，\\(C_0\\) 是原先的损失函数。 直观上，正则化的效果就是让学习的权值尽可能的小。可以说，正则化就是在最小化原损失函数和寻找小权值之间找折中。而两者之间的重要性由 \\(\\lambda\\) 控制。当 \\(\\lambda\\) 大时，网络会尽可能减小权重，反之，则尽可能减小原先的损失函数。 我们先通过一些实验看看这种正则化的效果。 添加正则化项后，梯度下降的偏导数会发生一点变化： \\[ \\begin{eqnarray} \\frac{\\partial C}{\\partial w} &amp; = &amp; \\frac{\\partial C_0}{\\partial w} + \\frac{\\lambda}{n} w \\tag{88}\\\\ \\frac{\\partial C}{\\partial b} &amp; = &amp; \\frac{\\partial C_0}{\\partial b}. \\tag{89}\\end{eqnarray} \\] 其中，\\(\\partial C_0/\\partial w\\) 和 \\(\\partial C_0/\\partial b\\) 可以通过 BP 算法计算，因此，新的偏导数很容易计算： \\[ \\begin{eqnarray} w &amp; \\rightarrow &amp; w-\\eta \\frac{\\partial C_0}{\\partial w}-\\frac{\\eta \\lambda}{n} w \\tag{91}\\\\ &amp; = &amp; \\left(1-\\frac{\\eta \\lambda}{n}\\right) w -\\eta \\frac{\\partial C_0}{\\partial w}. \\tag{92}\\end{eqnarray} \\\\ \\] \\[ \\begin{eqnarray} b &amp; \\rightarrow &amp; b -\\eta \\frac{\\partial C_0}{\\partial b}. \\tag{90}\\end{eqnarray} \\] 在批训练时，梯度下降公式变为： \\[ \\begin{eqnarray} w \\rightarrow \\left(1-\\frac{\\eta \\lambda}{n}\\right) w -\\frac{\\eta}{m} \\sum_x \\frac{\\partial C_x}{\\partial w}, \\tag{93}\\end{eqnarray} \\] （注意，式子前半部分除的是训练数据大小 n，后半部分是批训练的 m） 现在，在 1000 个训练样本的例子中，我们加入正则化项（\\(\\lambda\\) 设为0.1，其他参数和之前一样），并看看训练的结果如何： 可以看出，准确率较之前的 82.27% 有了明显的提高，也就是说，正则化确实在一定程度上抑制了过拟合。 现在，我们用所有的 50000 张图片训练，看看正则化能否起作用（这里我们设置 \\(\\lambda\\) 为 5.0，因为 n 由原来的 1000 变为 50000，如果 \\(\\lambda\\) 的值和之前一样，那么 \\(\\frac{\\eta \\lambda}{n}\\) 的值就会小很大，weight decay 的效果就会大打折扣）。 可以看到，准确率上升到 96.49%，而且测试集准确率和训练集准确率之间的差距也进一步缩小了。 为什么正则化能减小过拟合 这个问题可以用奥卡姆剃刀(Occam’s Razor)来解释。奥卡姆剃刀的思想是，如果两个模型都能拟合数据，那么我们优先选择简单的模型。 正则化给神经网络带来的影响是：权值 (绝对值) 会更小。权值小的好处是，当输入发生微小的变化时，网络的结果不会发生大的波动，相反地，如果权值 (绝对值) 过大，那么一点点变化也会产生很大的响应（包括噪声）。从这一点来看，我们可以认为正则化的网络是比较简单的模型。 当然，简单的模型也未必是真正有用的，更关键的是要看模型的泛化能力是否足够好。关于正则化，人们一直没法找出系统科学的解释。由于神经网络中，正则化的效果往往不错，因此大部分情况下，我们都会对网络进行正则化。 其他正则化技巧 L1 正则化 L1 正则化的形式和 L2 很像，只不过正则化项略有区别： \\[ C=C_0+\\frac{\\lambda}{n}\\sum_w{|w|} \\tag{95} \\] 下面来看看 L1 正则化对网络产生的影响。 首先，我们对 (95) 式求偏导： \\[ \\begin{eqnarray} \\frac{\\partial C}{\\partial w} = \\frac{\\partial C_0}{\\partial w} + \\frac{\\lambda}{n} \\, {\\rm sgn}(w), \\tag{96}\\end{eqnarray} \\] 其中，\\({\\rm sgn}(w)\\) 表示 \\(w\\) 的符号，如果 \\(w\\) 为正，则为 +1，否则为 -1。 这样，梯度下降的公式就变成： \\[ w \\rightarrow w&#39;=w-\\frac{\\eta \\lambda}{n}{\\rm sgn}(w)-\\eta \\frac{\\partial C_0}{\\partial w} \\tag{97} \\] 对比 L2 的公式 (93)，我们发现，两个式子都有缩小 weight 的功能，这跟之前分析正则化能起作用的原因是一致的。只不过 weight 缩小的方式不一样。在 L1 中，正则化项让 weight 以一个固定的常数向 0 靠近（weight 是正是负都一样），而 L2 中weight 减小的量跟 weight 本身存在一个比例关系（也就是说，weight 越小，这个量也越小）。所以，当 weight 的绝对值很大时，L2 对 weight 的抑制作用比 L1 大。 在上式中，存在一个缺陷：当 \\(w=0\\) 时，\\(|w|\\) 是没法求导的。这个时候，我们只需要简单地令 \\({\\rm sgn}(w)=0\\) 即可。 dropout dropout 和 L1、L2 存在很大区别，它不会修改损失函数，相反地，它修改的是网络的结构。 假设我们要训练如下的网络： 在梯度下降时，dropout 会随机删除隐藏层中一半的神经元，如下（虚线表示删除的神经元）： 让网络在这种「残缺」的状态下训练。 当开始下一轮 batch 训练时，我们先恢复完整的网络，然后继续随机删除隐藏层中一半的神经元，再训练网络。如此循环直到训练结束。 当要使用网络预测的时候，我们会恢复所有的神经元。由于训练时只有一半的神经元启动，因此每个神经元的权值等价于完整网络的两倍，所以，真正使用网络预测时，我们要取隐藏层的权值的一半。 dropout 的思想可以这么理解：假设我们按照标准模式 (没有 dropout) 训练很多相同结构的网络，由于每个网络的初始化不同，训练时的批训练数据也会存在差异，因此每个网络的输出都会存在差别。最后我们取所有网络的结果的均值作为最终结果（类似随机森林的投票机制）。例如，我们训练了 5 个网络，有 3 个网络将数字分类为「3」，那么我们就可以认为结果是「3」，因为另外两个网络可能出错了。这种平均的策略很强大，因为不同的网络可能在不同程度上出现了过拟合，而平均取值可以缓解一定程度的过拟合现象。dropout 每次训练时会 drop 一些神经元，这就像在训练不同的网络，dropout 的过程就像在平均很多网络的结果，因此最终起到减小 overfitfing 的作用。 人工扩展训练数据 除了 dropout，扩展训练数据也是缓解过拟合的有效策略。 为了解训练数据集对结果的影响，我们准备做几组实验。每组实验的训练集大小不同，训练的轮数和正则化的参数 \\(\\lambda\\) 也会做相应调整，其他参数则保持不变。 正如图中所示，训练数据量的增加有助于提高分类的准确率。图中的结果看似网络已经趋于收敛，但换成对数坐标后，这种效果就更加明显了： 因此，如果我们能将数据集扩大到几十万几百万，准确率应当能够持续上升。 获得更多训练数据是很困难的，不过好在我们有其他技巧达到类似的作用，那就是人工扩展数据。 例如，我们有一张 MNIST 的训练图片： 旋转 15º 后，我们就得到另一张样本图片： 这两张图片肉眼都可以看出是「5」，但在像素级别上，它们差别很大，因此不失为不错的训练样本。重复这种做法（旋转平移等等操作），我们可以获得数倍于原训练数据集大小的样本。 这种做法效果明显，在很多实验中都取得成功。而且，这种思想不仅仅局限于图片识别，在其他任务（如：语音识别）中，这种做法同样奏效。 另外，数据量也可以弥补机器学习算法的不足。假设在相同数据规模下，算法 A 要好于算法 B，但如果为算法 B 提供更多数据，后者的效果往往会超过前者。而且，即使两者数据规模一样，但算法 B 的数据比 A 的更加丰富，B 也可能超过 A，这就是所谓好的算法不如好的数据。 参考 Improving the way neural networks learn","raw":null,"content":null,"categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://jermmy.github.io/categories/机器学习/"}],"tags":[{"name":"深度学习","slug":"深度学习","permalink":"https://jermmy.github.io/tags/深度学习/"}]},{"title":"读书笔记：改进损失函数","slug":"2017-7-20-reading-notes-neuralnetworksanddeeplearning-3-1","date":"2017-07-20T14:49:21.000Z","updated":"2018-03-23T02:23:35.000Z","comments":true,"path":"2017/07/20/2017-7-20-reading-notes-neuralnetworksanddeeplearning-3-1/","link":"","permalink":"https://jermmy.github.io/2017/07/20/2017-7-20-reading-notes-neuralnetworksanddeeplearning-3-1/","excerpt":"(本文是根据 neuralnetworksanddeeplearning 这本书的第三章Improving the way neural networks learn整理而成的读书笔记，根据个人口味做了删减)\n上一章中，我们领略了神经网络中最重要的算法：后向传播算法(BP)。它使得神经网络的训练成为可能，是其他高级算法的基础。今天，我们要继续学习其他方法，这些方法使得网络的训练结果更好。\n这些方法包括：\n\n更好的损失函数：交叉熵(cross-entropy)函数\n四种正规化方法：L1、L2、dropout以及数据集的人工增广\n一种更好的初始化权值的方法\n一系列选择 hyper-parameters 的启发策略\n其他一些小技巧\n","text":"(本文是根据 neuralnetworksanddeeplearning 这本书的第三章Improving the way neural networks learn整理而成的读书笔记，根据个人口味做了删减) 上一章中，我们领略了神经网络中最重要的算法：后向传播算法(BP)。它使得神经网络的训练成为可能，是其他高级算法的基础。今天，我们要继续学习其他方法，这些方法使得网络的训练结果更好。 这些方法包括： 更好的损失函数：交叉熵(cross-entropy)函数 四种正规化方法：L1、L2、dropout以及数据集的人工增广 一种更好的初始化权值的方法 一系列选择 hyper-parameters 的启发策略 其他一些小技巧 交叉熵函数(cross-entropy) 实际生活中，我们都会有这样的经验：当遇到错误的时候，往往是我们学到东西最多的时候，而如果我们对自己的错误模糊不清，进步反而会变慢。 同样地，我们希望神经网络能够从错误中更快地学习。那实际情况是怎样的呢？来看一个简单的例子。 tikz28 这个例子只包含一个神经元，并且只有一个输入。我们会训练这个神经元，使得：当输入为 1 时，输出为 0。我们将权重和偏差分别初始化为 0.6 和 0.9。当输入为 1 时，网络输出为 0.82 (\\(\\frac{1}{1+e^{-1.5}} \\approx 0.82\\))。我们采用平方差函数来训练网络，并将学习率设为 0.15。 这个网络其实已经退化成一个线性回归模型。下面用一个动画来演示网络的训练过程： saturation1 从中我们可以看到，神经元快速地学习参数，最终输出 0.09 (已经很接近 0 了)。现在，我们将参数和偏差初始化为 2.0，网络的初始输出为 0.98 (跟我们想要的结果相差甚远)，学习率依然为 0.15。看看这一次网络会如何学习： saturation2 虽然学习率和上次一样，但网络一开始学习的速度却很慢，在最开始的 150 次学习里，参数和偏差几乎没有改变，之后，学习速度突然提高，神经元的输出快速降到接近 0.0。这一点很令人差异，因为当神经元的输出严重错误时，学习的速度反而不是很快。 下面我们需要了解问题发生的根源。神经元在训练的时候，学习速度除了受学习率影响外，还受偏导数 \\(\\partial C/ \\partial w\\) 和 \\(\\partial C / \\partial b\\) 影响。所以，学习速度很慢，也就是偏导数的值太小。根据 \\[ C=\\frac{(y-a)^2}{2} \\tag{54} \\] （其中，\\(a=\\sigma(z)\\)，\\(z=wx+b\\)），我们可以求出（下面两个式子中，已经将 x 和 y 的值替换为 1 和 0）： \\[ \\frac{\\partial C}{\\partial w} = (a-y)\\sigma&#39;(z)x=a\\sigma&#39;(z) \\tag{55} \\] \\[ \\frac{\\partial C}{\\partial b} = (a-y)\\sigma&#39;(z)=a\\sigma&#39;(z) \\tag{56} \\] 要想深入理解这两个式子，我们需要回顾一下 sigmoid 函数的内容，如下图： sigmoid 从函数图像我们可以发现，当函数值接近于 1 或 0 时，函数导数趋于 0，从而导致 (55) 和 (56) 两个式子的值趋于 0。这也是为什么神经元一开始的学习速率会那么慢，而中间部分学习速度会突然提升。 引入交叉熵损失函数 要解决学习速度下降的问题，我们需要从两个偏导数上面做文章。要么换一个损失函数，要么更换 \\(\\sigma\\) 函数。这里，我们采用第一种做法，将损失函数更换为交叉熵函数(cross-entropy)。 首先用一个例子来介绍交叉熵函数。 假设我们有如下神经元： tikz29 则交叉熵函数被定义为（这里假定 y 是个概率值，在 0～1 之间，这样才能跟 a 相搭）： \\[ C=-\\frac{1}{n}\\sum_x{[y \\ln a + (1-y) \\ln (1-a)]} \\tag{57} \\] 当然，直觉上看不出这个函数能解决学习速率下降的问题，甚至看不出这可以成为一个损失函数。 我们先解释为什么这个函数可以作为损失函数。首先，这个函数是非负的，即 \\(C&gt;0\\)（注意 \\(a\\) 的值在 0～1 之间）。其次，当神经元实际输出跟我们想要的结果接近时，交叉熵函数值会趋近 0。因此，交叉熵满足损失函数的基本条件。 另外，交叉熵解决了学习速率下降的问题。我们将 \\(a=\\sigma(z)\\) 代入 (57) 式，并运用链式法则可以得到（这里的 \\(w_j\\) 应该特指最后一层的参数，即 \\(w_j^L\\)）： \\[ \\begin{eqnarray} \\frac{\\partial C}{\\partial w_j} &amp; = &amp; -\\frac{1}{n} \\sum_x \\left( \\frac{y }{\\sigma(z)} -\\frac{(1-y)}{1-\\sigma(z)} \\right) \\frac{\\partial \\sigma}{\\partial w_j} \\tag{58}\\\\ &amp; = &amp; -\\frac{1}{n} \\sum_x \\left( \\frac{y}{\\sigma(z)} -\\frac{(1-y)}{1-\\sigma(z)} \\right)\\sigma&#39;(z) x_j. \\tag{59}\\end{eqnarray} \\] 化简上式并将 \\(\\sigma(z)=\\frac{1}{1+e^{-z}}\\) 代入后得到： \\[ \\frac{\\partial C}{\\partial w_j}=\\frac{1}{n}\\sum_x {x_j(\\sigma(z)-y)} \\tag{61} \\] 这个表达式正是我们想要的！它表明，学习速率由 \\(\\sigma(z)-y\\) 控制，也就是说，当误差越大时，学习速率越快。而且避免了 \\(\\sigma&#39;()\\) 导致的学习速率下降的问题。 类似地，我们可以计算出： \\[ \\frac{\\partial C}{\\partial b}=\\frac{1}{n}\\sum_x{(\\sigma(z)-y)} \\tag{62} \\] 现在，我们将交叉熵应用到之前的例子中，看看神经元的训练有什么变化。 首先是权重和偏差的初始值为 0.6 和 0.9 的例子： saturation3 可以看到网络的训练速度近乎完美。 然后是权重和偏差初始值均为 2.0 的例子： saturation4 这一次，正如我们期望的那样，神经元学习得非常快。 这两次实验中，采用的学习率是 0.005。事实上，对于不同的损失函数，学习率要作出相应的调整。 上面对交叉熵函数的讨论都只针对一个神经元，其实很容易将它延伸到多层神经元的网络结构。假设 \\(y=y_1, y_2, \\dots\\) 是想要的网络输出，而 \\(a_1^L, a_2^L, \\dots\\) 是网络的实际输出，则 cross-entropy 函数可以定义为： \\[ C=-\\frac{1}{n}\\sum_x \\sum_y {[y_j \\ln a_j^L + (1-y_j) \\ln(1-a_j^L)]} \\tag{63} \\] 好了，介绍了这么多，那我们什么时候用平方差函数，什么时候用交叉熵呢？作者给出的意见是，交叉熵几乎总是更好的选择，而原因也跟上文提到的一样，平方差函数容易在开始的时候遇到训练速率较慢的问题，而交叉熵则没有这种困扰。当然，这个问题出现的前提是平方差函数中用了 sigmoid 函数。 交叉熵到底是什么，它是怎么来的？ 这一节中，我们想知道，第一个吃螃蟹的人是怎么想到交叉熵函数的。 假设我们发现了学习速率下降的根源在于 \\(\\sigma&#39;(z)\\) 函数，我们要如何解决这个问题呢？当然，方法有很多，这里我们考虑这样的思路：是否能找一个新的损失函数，将 \\(\\sigma&#39;(z)\\) 这个项消掉？假如我们希望最终的偏导数满足下面的形式： \\[ \\frac{\\partial C}{\\partial w_j}=x_j (a-y) \\tag{71} \\] \\[ \\frac{\\partial C}{\\partial b}=(a-y) \\tag{72} \\] 这两个偏导数能使神经网络在误差越大时，训练速度越快。 回忆 BP 的四个公式，可以得到： \\[ \\frac{\\partial C}{\\partial b}=\\frac{\\partial C}{\\partial a}\\sigma&#39;(z) \\tag{73} \\] 这里的 \\(\\sigma()\\) 函数采用的是 sigmoid，所以 \\(\\sigma&#39;(z)=\\sigma(z)(1-\\sigma(z))=a(1-a)\\)，将这个式子代入 (73) ，得到： \\[ \\frac{\\partial C}{\\partial b}=\\frac{\\partial C}{\\partial a}a(1-a) \\] 跟我们最终的目标 (72) 式比较，需要满足： \\[ \\frac{\\partial C}{\\partial a}=\\frac{a-y}{1(1-a)} \\tag{75} \\] 对 (75) 进行积分后，便得到： \\[ C=-\\frac{1}{n}\\sum_x{[y\\ln a+(1-y)\\ln(1-a)]}+constant \\tag{77} \\] 至此，我们已经推出了交叉熵函数的形式。 当然啦，交叉熵真正的来源是信息论，更具体的介绍超出了本教程的范畴，所以就不再深入了。 Softmax 前一节中，我们重点介绍了交叉熵如何解决训练速度下降的问题，这是从损失函数的角度思考问题。其实，我们还有另一种方法，那就是更换 \\(\\sigma()\\) 函数。这里要简单介绍一个新的 \\(\\sigma()\\) ：Softmax。 Softmax 的功能和 sigmoid 类似，只不过前者的函数形式是这样的： \\[ a_j^L=\\frac{e^{z_j^L}}{\\sum_k{e^{z_k^L}}} \\tag{78} \\] ⚠️分母是所有输出神经元的总和。这意味着，经过 Softmax 函数后，所有神经元的输出会呈现出概率分布的样式。 softmax 当增大其中一个神经元输出时，其他神经元的输出值会变小，而且变小的总和等于前者增加的值。反之亦然。这是因为所有神经元的输出值总和始终为 1。 另外，Softmax 的输出始终为正值。 Softmax 解决学习速率下降的问题 这一次，我们定义一个 log-likelihood 损失函数，通过它来了解 Softmax 如何缓解 learning slowdown 的问题。 log-likelihood 的函数形式为： \\[ C \\equiv -\\ln a_y^L \\tag{80} \\] 先解释一下 \\(a_y^L\\)，比方说，在 MNIST 数据集中，我们要判断一张图片属于 10 类中的哪一类，那么，输出结果应该是一个 10 维的向量 \\(a^L\\)，而真实结果则是数字 \\(y\\)，比如 7。那么，\\(a_y^L\\) 则表示 \\(a_7^L\\) 这个项对应的概率值有多高。如果概率值（靠近 1）越高，证明猜测结果越正确，那么 \\(C\\) 的值就越小，反之越大。 有了损失函数后，我们照样求出偏导数： \\[ \\frac{\\partial C}{\\partial b_j^L}=a_j^L-y_j \\tag{81} \\] \\[ \\frac{\\partial C}{\\partial w_{jk}^L}=a_k^{L-1}(a_j^L-y_j) \\tag{82} \\] 这里不存在类似 sigmoid 导数那样使学习速率下降的情况。 （写到这里的时候，我突然产生一个疑惑：不管是这里的 Softmax，还是的交叉熵，我们都只是对最后一层的导数和偏差求了偏导，但前面层的偏导数却没有计算，怎么能肯定前面层的偏导就不会遇到 \\(\\sigma&#39;()\\) 趋于 0 的问题呢？要知道，根据 BP 算法的公式，误差有这样的传递公式：\\(\\delta^l\\)=\\(((W^{l+1})^T \\delta^{l+1}) \\odot \\sigma&#39;(z^l)\\)，注意，这里依然会出现 \\(\\sigma&#39;()\\)，而前面层的权重和偏差的偏导数又是根据这个误差计算的，这样的话，前面层的学习速率下降的问题不还是没解决吗？这个问题先暂时放着，看看之后作者有没有解答。） 写了这么多，我们又要问一个类似的问题：什么时候用 sigmoid 和 cross-entropy，什么时候用 softmax 和 log-likelihood。事实上，大部分情况下这两个选择都能带来不错的结果，当然，如果想要输出结果呈现概率分布的话，Softmax 无疑会更好。 参考 Improving the way neural networks learn","raw":null,"content":null,"categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://jermmy.github.io/categories/机器学习/"}],"tags":[{"name":"深度学习","slug":"深度学习","permalink":"https://jermmy.github.io/tags/深度学习/"}]},{"title":"读书笔记：神经网络是怎么训练的","slug":"2017-6-25-reading-notes-neuralnetworksanddeeplearning-2","date":"2017-06-25T05:56:18.000Z","updated":"2018-03-23T02:24:59.000Z","comments":true,"path":"2017/06/25/2017-6-25-reading-notes-neuralnetworksanddeeplearning-2/","link":"","permalink":"https://jermmy.github.io/2017/06/25/2017-6-25-reading-notes-neuralnetworksanddeeplearning-2/","excerpt":"(本文是根据 neuralnetworksanddeeplearning 这本书的第二章How the backpropagation algorithm works整理而成的读书笔记，根据个人口味做了删减)\n在上一章的学习中，我们介绍了神经网络可以用梯度下降法来训练，但梯度的计算方法却没有给出。在本章中，我们将学习一种计算神经网络梯度的方法——后向传播算法（backpropagation）。","text":"(本文是根据 neuralnetworksanddeeplearning 这本书的第二章How the backpropagation algorithm works整理而成的读书笔记，根据个人口味做了删减) 在上一章的学习中，我们介绍了神经网络可以用梯度下降法来训练，但梯度的计算方法却没有给出。在本章中，我们将学习一种计算神经网络梯度的方法——后向传播算法（backpropagation）。 backpropagation 算法起源于上个世纪 70 年代，但一直到 Hinton 等人在 1986 年发表的这篇著名论文后才开始受到关注。BP 算法使得神经网络的训练速度快速提升，因此它是学习神经网络的重中之重。 热身：一种基于矩阵的快速计算神经网络输出的方法 在开始讨论 BP 算法之前，我们先回顾一种基于矩阵形式的计算神经网络输出的方法。 首先，引入几个符号表示。 假设 \\(w_{jk}^{l}\\) 表示从第 l-1 层的第 k 个神经元到第 l 层的第 j 个神经元的权值，如下图所示。 tikz16 假设 \\(b_{j}^{l}\\) 表示 l 层第 j 个神经元的偏差，\\(a_{j}^{l}\\) 表示 l 层第 j 个神经元的激活层，如下图所示： tikz16 有了这些标记，第 l 层的第 j 个神经元的激活层 \\(a_{j}^{l}\\) 就可以和 l-1 层的激活层关联起来： \\[ a_{j}^l = \\sigma(\\sum_{k}{w_{jk}^{l}a_{k}^{l-1}+b_{j}^{l}}) \\tag{23} \\] 其中，\\(\\sigma()\\) 是一个激活函数，例如 sigmoid 函数之类的。 现在，为了方便书写，我们为每一层定义一个权值矩阵 \\(W^l\\)，矩阵的每个元素对应上面提到的 \\(w_{jk}^{l}\\)。类似地，我们为每一层定义一个偏差向量 \\(b^l\\) 以及一个激活层向量 \\(a^l\\)。 然后，我们将公式 (23) 表示成矩阵的形式： \\[ a^l=\\sigma(W^la^{l-1}+b^l) \\tag{25} \\] 注意，这里我们对 \\(\\sigma()\\) 函数做了点延伸，当输入参数是向量时，\\(\\sigma()\\) 会逐个作用到向量的每个元素上（elementwise）。 在 (25) 式中，有时为了书写的方便，我们会用 \\(z^l\\) 来表示 \\(W^la^{l-1}+b^l\\)。下文中，\\(z^l\\) 将会频繁出现。 损失函数的两个前提假设 BP 算法的目标是要计算偏导数 \\(\\partial C\\)/\\(\\partial w\\) 和 \\(\\partial C\\)/\\(\\partial b\\)，要让 BP 算法起作用，我们需要两个前提假设： 损失函数可以表示成 \\(C=\\frac{1}{n}\\sum_{x}{C_x}\\)，其中 \\(C_x\\) 是每个训练样本 x 的损失函数。 损失函数用神经网络的输出作为函数的输入： tikz16 BP 算法背后的四个基本公式 BP 算法本质上是为了计算出 \\(\\partial C\\) / \\(\\partial w_{jk}^{l}\\) 和 \\(\\partial C\\) / \\(\\partial b_{j}^{l}\\)。为了计算这两个导数，我们引入一个中间变量 \\(\\delta_{j}^{l}\\)，这个中间变量表示第 l 层第 j 个神经元的误差。BP 算法会计算出这个误差，然后用它来计算\\(\\partial C\\) / \\(\\partial w_{jk}^{l}\\) 和 \\(\\partial C\\) / \\(\\partial b_{j}^{l}\\)。 \\(\\delta_{j}^{l}\\) 被定义为： \\[ \\delta _{j}^{l}=\\frac{\\partial C}{\\partial z_{j}^{l}} \\tag{29} \\] 这个定义来源于这样一个事实：损失函数 \\(C\\) 可以看作是关于 \\(z\\) 的函数，而 \\(z\\) 是 \\(W\\) 和 \\(b\\) 的线性组合（考虑到损失函数的两个前提假设，\\(C\\) 是关于网络输出 \\(a\\) 的函数，而 \\(a\\) 又是 \\(z\\) 的函数，所以 \\(C\\) 也可以看作是 \\(z\\) 的函数）。其实，我们也可以将它定义为：\\(\\delta_{j}^{l}=\\frac{\\partial C}{\\partial a_{j}^{l}}\\)（\\(a\\) 是神经网络某一层的输出），但这样会导致之后的计算十分复杂，所以，我们还是保留原先的定义。 BP 算法基于 4 个基本公式，这些公式会告诉我们如何计算 \\(\\delta^{l}\\) 和损失函数的梯度。 输出层误差 \\(\\delta^{L}\\)的计算公式 \\[ \\delta_{j}^{L}=\\frac{\\partial C}{\\partial z_{j}^{L}}=\\frac{\\partial C}{\\partial a_{j}^{L}}\\sigma&#39;(z_{j}^{L}) \\tag{BP1} \\] 这个公式是最直接的，只需要知道 \\(a^{L}=\\sigma(z^{L})\\)，然后根据链式法则即可得到。 为了更好地运用矩阵运算，我们改变一下上面式子的形式： \\[ \\delta^{L}=\\nabla_a C \\odot \\sigma&#39;(z^L). \\tag{BP1a} \\] 其中，\\(\\odot\\) 表示 elementwise 运算，而 \\(\\nabla_a C\\) 可以看作是 \\(\\partial C / \\partial a_{j}^{L}\\) 组成的向量。 举个例子，假设 \\(C=\\frac{1}{2}\\sum_{j}{(y_j - a_{j}^{L})}^2\\)，则 \\(\\partial C / \\partial a_{j}^{L}=\\begin{bmatrix} \\partial C / \\partial a_0^l \\\\ \\partial C / \\partial a_1^l \\\\ \\vdots \\\\ \\partial C / \\partial a_n^l \\end{bmatrix}=(a_{j}^{L}-y_j)=\\begin{bmatrix} a_0^l-y_0 \\\\ a_1^l-y_1 \\\\ \\vdots \\\\ a_n^l-y_l \\end{bmatrix}\\)，那么公式(BP1)可以表示成：\\(\\delta^{L}=(a_{L}-y) \\odot \\sigma&#39;(z^L)\\)。 \\(\\delta^L\\)与\\(\\delta^{L+1}\\)的计算公式 \\[ \\delta^L=((w^{l+1})^T\\delta^{l+1}) \\odot \\sigma&#39;(z^l) \\tag{BP2} \\] 前面公式 (BP1) 可以让我们计算出最后输出层 \\(\\delta^L\\) 的值，而 (BP2) 这个公式可以依据最后一层的误差，逐步向前传递计算前面输出层的 \\(\\delta^L\\) 值。 bias 的导数计算公式 \\[ \\frac{\\partial C}{\\partial b_j^{l}}=\\delta_j^l \\tag{BP3} \\] 这个公式表明，第 l 层偏差 bias 的导数和第 l 层的误差值相等。 权重 W 的导数计算公式 \\[ \\frac{\\partial C}{\\partial w_{jk}^{l}}=a_{k}^{l-1}\\delta_{j}^{l} \\tag{BP4} \\] 同理，这个公式揭露出权重 W 的导数和误差以及网络输出之间的关系。用一种更简洁的方式表示为： \\[ \\frac{\\partial C}{\\partial w} = a_{in}\\delta_{out} \\tag{32} \\] 其中，\\(a_{in}\\) 是权重 \\(W\\) 的输入，而 \\(\\delta_{out}\\) 是权重 \\(W\\) 对应的 \\(z\\) 的误差。用一幅图表示如下： tikz20 公式 (32) 一个很好的效果是：当 \\(a_{in} \\approx 0\\) 时，梯度公式的值会很小，换句话说，当权重 \\(W\\) 的输入 \\(a_{in}\\)，也就是上一层激活层的输出接近 0 时，那么这个激活层对网络的影响就变得很小，\\(W\\) 的学习也会变得很慢。 一些启发（insights） 根据上面四个公式，可以发现，当最后输出层的导数 \\(\\sigma&#39;(z^L)\\) 变的很小时（即网络本身已经接近收敛），权重 \\(W\\) 和偏差 \\(b\\) 会逐渐停止学习（因为误差 \\(\\delta\\) 逐渐趋于 0）。 当然，不单单是最后一层会影响学习速度，根据公式 (BP2)，当中间层的导数 \\(\\sigma&#39;(z^l)\\) 也开始趋于 0 时，那么上一层的误差 \\(\\delta^l\\) 也会趋于 0，从而导致上一层权重 \\(W\\) 和偏差 \\(b\\) 的学习也会开始停止。 总之，当 \\(W\\) 的输入 \\(a\\) 变的很小或者输出层 \\(\\sigma(z^l)\\) 收敛时，网络权值的训练将会变得很慢。 需要注意的一点是，这四个公式的推导适用于任何激活函数。因此，我们完全可以用其他函数来取代 \\(sigmoid()\\)。比如，我们可以设计一个函数 \\(\\sigma()\\)，这个函数的导数 \\(\\sigma&#39;()\\) 永远为正，且 \\(\\sigma()\\) 函数值永远不会接近 0，那么就可以避免上面提到的学习停止的问题。 最后，总结一下 BP 的 4 个基本公式： tikz21 个人对于误差以及 BP 的理解 根据误差 \\(\\delta\\) 的定义，不难发现，它其实就是损失函数关于参数 \\(W\\) 和 \\(b\\) 的间接导数，这一点跟第一章中对梯度的定义是一致的。当 \\(\\delta\\) 越大时，证明网络还远没有收敛，即网络的「误差」还很大，因此需要学习更多，反之，则证明网络的「误差」比较小，学习可以停止了。 网络中每一层的误差都需要借助前一层的误差进行计算，这个过程其实是一个导数的叠加过程，可以感性地认为，整个神经网络其实是由一个个函数复合在一起形成的，因此，导数的计算其实就是链式法则的不断应用，前面层神经元的导数需要后面层神经元导数不断叠加，这个过程就构成了后向传播算法。 公式证明 BP1 公式 (BP1) 的证明是十分简单的，不过需要习惯向量或矩阵的 elementwise 的求导形式。 我们假设 \\(C=f(\\sigma(z^L))=f(\\sigma(z_0^L), \\sigma(z_1^L), \\cdots, \\sigma(z_n^L))\\)，根据定义 \\(\\delta_j^L=\\frac{\\partial C}{\\partial z_j^L}\\)，由于 \\(z_j^L\\) 只跟 \\(a_j^L\\) 相关，于是我们用链式法则可以得到（可以画个网络图帮助理解）： \\[ \\delta_j^L=\\frac{\\partial f}{\\partial \\sigma(z_j^L)}\\frac{\\partial \\sigma(z_j^L)}{\\partial z_j^L}=\\frac{\\partial C}{\\partial a_j^L}\\frac{\\partial a_j^L}{\\partial z_j^L} \\tag{38} \\] 其中，\\(a_j^L=\\sigma(z_j^L)\\)，我们也可以将它表示成另一种形式： \\[ \\delta_j^L=\\frac{\\partial C}{\\partial a_j^L}\\sigma&#39;(z_j^L) \\tag{39} \\] 上式就是 BP1 的形式了。 BP2 BP2 需要用到后一层计算出来的 \\(\\delta^{l+1}\\)，因此，我们先根据 BP1 得出：\\(\\delta_k^{l+1}=\\frac{\\partial C}{\\partial z_k^{l+1}}\\)。 由 \\(\\delta_k^{l}=\\frac{\\partial C}{\\partial z_k^l}\\) 和 \\(C=f(\\sigma(z_0^L), \\sigma(z_1^L), \\cdots, \\sigma(z_n^L))\\) 可以得到： \\[ \\begin{eqnarray} \\delta_j^{l} &amp; = &amp; \\frac{\\partial C}{\\partial z_0^{l+1}}\\frac{\\partial z_0^{l+1}}{\\partial z_j^{l}}+\\cdots+\\frac{\\partial C}{\\partial z_n^{l+1}}\\frac{\\partial z_n^{l+1}}{\\partial z_j^{l}} \\notag \\\\ &amp; = &amp; \\sum_k{\\frac{\\partial C}{\\partial z_k^{l+1}}\\frac{\\partial z_k^{l+1}}{\\partial z_j^j}} \\notag \\\\ &amp; = &amp; \\sum_k \\delta_k^{l+1}\\frac{\\partial z_k^{l+1}}{\\partial z_j^{l}} \\tag{42} \\end{eqnarray} \\] 我们还要进一步找出 \\(z_k^{l+1}\\) 和 \\(z_k^{l}\\) 之间的关系。根据前向传播，可以得到： \\[ z_k^{l+1}=\\sum_j{w_{kj}^{l+1}a_j^l+b_k^{l+1}}=\\sum_j{w_{kj}^{l+1}\\sigma(z_j^l)+b_k^{l+1}} \\tag{43} \\] 进而可以得到： \\[ \\frac{\\partial z_k^{l+1}}{\\partial z_j^l}=w_{kj}^{l+1}\\sigma&#39;(z_j^l) \\tag{44} \\] 将式 (44) 代入 (42) 得： \\[ \\delta_j^l=\\sum_k{w_{kj}^{l+1}\\sigma&#39;(z_j^l)\\delta_k^{l+1}}=\\sigma&#39;(z_j^l)\\sum_k{w_{kj}^{l+1}\\delta_k^{l+1}} \\tag{45} \\] 表示成矩阵的形式就是： \\[ \\delta^L=((w^{l+1})^T\\delta^{l+1}) \\odot \\sigma&#39;(z^l) \\] 即 BP2 的公式，注意矩阵的转置运算。 BP3 \\[ z_j^l=\\sum_k{W_{jk}^l a_k^{l-1}}+b_j^l \\] \\[ \\frac{\\partial z_j^l}{\\partial b_j^l}=1 \\] \\[ \\frac{\\partial C}{\\partial b_j^l}=\\frac{\\partial C}{\\partial z_j^l}\\frac{\\partial z_j^l}{\\partial b_j^l}=\\frac{\\partial C}{\\partial z_j^l}=\\delta_j^l \\] BP4 证明过程同 BP3： \\[ z_j^l=\\sum_k{W_{jk}^l a_k^{l-1}}+b_j^l \\] \\[ \\frac{\\partial z_j^l}{\\partial W_{jk}^l}=a_k^{l-1} \\] \\[ \\frac{\\partial C}{\\partial W_{jk}^l}=\\frac{\\partial C}{\\partial z_j^l}\\frac{\\partial z_j^l}{\\partial W_{jk}^l}=\\frac{\\partial C}{\\partial z_j^l}a_k^{l-1}=\\delta_j^la_k^{l-1} \\] 后向传播算法(BP) Input x: Set the corresponding activation \\(a^1\\) for the input layer. Feedforward: For each l = 2, 3, …, L compute \\(z^l=w^la^{l-1}+b^l\\) and \\(a^l=\\sigma(z^l)\\). Output error \\(\\delta^L\\): Compute the vector \\(\\delta^L=\\nabla_a C \\odot \\sigma&#39;(z^L)\\). Backpropagate the error: For each l = L-1, L-2, …, 2 compute \\(\\delta^l=((W^{l+1})^T \\delta^{l+1}) \\odot \\sigma&#39;(z^l)\\). Output: The gradient of the cost function is given by \\(\\frac{\\partial C}{\\partial w_{jk}^l}=a_k^{l-1}\\delta_j^{l}\\) and \\(\\frac{\\partial C}{\\partial b_j^l}=\\delta_j^l\\). 以上算法是针对一个训练样本进行的，实际操作中，通常是用随机梯度下降算法，用几个样本进行训练，因此我们将算法略微修改如下： Input a set of training examples For each training example x: Set the corresponding input activation \\(a^{x, 1}\\), and perform the following steps: Feedforward: For each l = 2, 3, …, L compute \\(z^{x, l}=w^la^{x, l-1}+b^l\\) and \\(a^{x, l}=\\sigma(z^{x,l})\\). Output error \\(\\delta^{x, L}\\): Compute the vector \\(\\delta^{x, L}=\\nabla_a C_x \\odot \\sigma&#39;(z^{x,L})\\). Backpropagate the error: For each l = L-1, L-2, …, 2 compute \\(\\delta^{x,l}=((W^{l+1})^T \\delta^{x,l+1}) \\odot \\sigma&#39;(z^{x,l})\\). Gradient descent: For each l = L, L-1, …, 2 update the weights according to the rule \\(W^l \\rightarrow W^l-\\frac{\\eta}{m} \\sum_x \\delta^{x,l}(a^{x,l-1})^T\\), and the biases according to the rule \\(b^l \\rightarrow b^l - \\frac{\\eta}{m} \\sum_x{\\delta^{x,l}}\\). 参考 How the backpropagation algorithm works","raw":null,"content":null,"categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://jermmy.github.io/categories/机器学习/"}],"tags":[{"name":"深度学习","slug":"深度学习","permalink":"https://jermmy.github.io/tags/深度学习/"}]},{"title":"读书笔记：敲开神经网络的大门","slug":"2017-5-13-reading-notes-neuralnetworksanddeeplearning-1","date":"2017-05-13T06:41:53.000Z","updated":"2018-03-23T02:21:29.000Z","comments":true,"path":"2017/05/13/2017-5-13-reading-notes-neuralnetworksanddeeplearning-1/","link":"","permalink":"https://jermmy.github.io/2017/05/13/2017-5-13-reading-notes-neuralnetworksanddeeplearning-1/","excerpt":"(本文是根据 neuralnetworksanddeeplearning 这本书的第一章 Using neural nets to recognize handwritten digits 整理而成的读书笔记，根据个人口味做了删减)\n对于人类来说，识别下面的数字易如反掌，但对计算机而言，却不是一个简单的任务。\n\ndigits\n\n在我们的大脑中，有一块跟视觉相关的皮层 V1，这里面包含着数以百万计的神经元，而这些神经元之间的连接，更是达到了数以亿计。在漫长的进化过程中，大自然将人类的大脑训练成了一个「超级计算机」，使它可以轻易地读懂、看懂、听懂很多目前的计算机仍然难以处理的问题。在本章中，作者介绍了一种可以帮助计算机识别手写体的程序：神经网络「neural network」。","text":"(本文是根据 neuralnetworksanddeeplearning 这本书的第一章 Using neural nets to recognize handwritten digits 整理而成的读书笔记，根据个人口味做了删减) 对于人类来说，识别下面的数字易如反掌，但对计算机而言，却不是一个简单的任务。 digits 在我们的大脑中，有一块跟视觉相关的皮层 V1，这里面包含着数以百万计的神经元，而这些神经元之间的连接，更是达到了数以亿计。在漫长的进化过程中，大自然将人类的大脑训练成了一个「超级计算机」，使它可以轻易地读懂、看懂、听懂很多目前的计算机仍然难以处理的问题。在本章中，作者介绍了一种可以帮助计算机识别手写体的程序：神经网络「neural network」。 首先，我们从神经网络的几个基本概念讲起。 Perceptrons Perceptrons，中文译为感知器，最早由科学家Frank Rosenblatt于上个世纪 50 至 60 年代提出。在现代神经网络中，Perceptrons 已经用得很少了（更多地使用 sigmoid neuron 等神经元模型）。但要了解 sigmoid neuron 怎么来的，就有必要先弄清楚 Perceptrons。 举例来说，最简单的 Perceptrons 类似如下结构： tikz0 它接受三个输入 \\(x_1\\)、\\(x_2\\)、\\(x_3\\)，输出 0 或者 1。为了衡量每个输入的重要程度，Rosenblatt 引入了权重的概念，假设 \\(w_1\\)、\\(w_2\\)、\\(w_3\\) 分别对应 \\(x_1\\)、\\(x_2\\)、\\(x_3\\)，那么，我们可以得到 Perceptrons 的输出为： \\[ output=\\begin{cases} 0 &amp;if \\ \\ \\sum_{j}{w_j x_j} &lt;= threshold \\\\\\\\ 1 &amp;if \\ \\ \\sum_{j}{w_j x_j} &gt; threshold \\end{cases} \\] 当然，Perceptrons 在处理较复杂的任务的时候，其结构也会更加复杂，比如： tikz1 在这个网络中，Perceptrons 的第一列称为第一层 (first layer)，这一层的感知器接受三个输入 (evidence) 来决定输出。Perceptrons 的第二层，则以第一层的输出结果作为输入来产生最后的输出，因此第二层可以认为是在处理比第一层更加复杂抽象的工作。 为了简化数学表达，我们将 \\(\\sum\\_{j}{w\\_jx\\_j}\\) 表示成 \\(WX\\)，\\(W\\)、\\(X\\) 分别代表权重和输入的向量。同时，我们将阈值的负值 (-threshold) 表示成 bias，即 \\(b = -threshold\\)。这样，Perceptrons 的输出可以重写为： \\[ output=\\begin{cases} 0 &amp;if \\ \\ WX+b &lt;= 0 \\\\\\\\ 1 &amp;if \\ \\ WX+b &gt; 0 \\end{cases}. \\] Sigmoid neurons 现在，我们考虑一下如何训练 Perceptrons 的参数（W 和 b）。假设网络的参数发生了一点点微小的变化，为了训练过程的可控，网络的输出也应该发生微小的变化。 tikz8 如果网络错误地将手写数字 8 分类为 9，那么我们希望在参数做一点点修改，网络的输出会更靠近 9 这个结果，只要数据量够多，这个修改的过程重复下去，最后网络的输出就会越来越正确，这样神经网络才能不断学习。 然而，对于 Perceptrons 来说，参数的微调却可能导致结果由 0 变为 1，然后导致后面的网络层发生连锁反应。换句话说，Perceptrons 的性质导致它的训练过程是相当难控制的。 为了克服这个问题，我们引入一种新的感知器 sigmoid neuron。它跟 Perceptrons 的结构一模一样，只是在输出结果时加上了一层 sigmoid 函数：\\(\\sigma(z)=\\frac{1}{1+e^{(-z)}}\\)。这样，网络的输出就变成了： \\[ output=\\frac{1}{1+exp(-(WX+b))} \\] sigmoid 函数的图像如下： sigmoid 当 \\(WX+b\\) 趋于 ∞ 的时候，函数值趋于 1，当 \\(WX+b\\) 趋于 0 的时候，函数值趋于 0。在这种情况下，sigmoid neuron 就退化成 Perceptrons。 sigmoid 函数也可以看作是对 step 函数的平滑，step 函数如下： step function 可以看出，Perceptrons neuron 的本质就是 step 函数。 那么，为什么 sigmoid neuron 就比 Perceptrons 更容易训练呢？原因在于，sigmoid 函数是平滑、连续的，它不会发生 step 函数那种从 0 到 1 的突变。用数学的语言表达就是，参数微小的改变（\\(\\Delta w_j\\)、\\(\\Delta b\\)）只会引起 output 的微小改变：\\(\\Delta output \\approx \\sum_j{\\frac{\\partial output}{\\partial w_j}\\Delta w_j}+\\frac{\\partial output}{\\partial b}\\Delta b\\)。可以发现，\\(\\Delta output\\) 和 \\(\\Delta w_j\\)、\\(\\Delta b\\) 是一个线性关系，这使得网络的训练更加可控。 事实上，正是 sigmoid 函数这种平滑的特性起了关键作用，而函数的具体形式则无关紧要。在本书后面的章节中，还会介绍其他函数来代替 sigmoid，这类函数有个学名叫激活函数 (activation function)。从数学上讲，函数平滑意味着函数在定义域内是可导的，而且导数有很好的数学特性（比如上面提到的线性关系），step 函数虽然分段可导，但它的导数值要么一直是 0，要么在突变点不可导，所以它不具备平滑性。 Learning with gradient descent 假设神经网络的输入是由图片像素组成的一维向量 $x $，输出是一个 one-hot 向量 \\(\\overline y = y(\\overline x)\\)。为了量化神经网络的输出结果，我们定义一个代价函数： \\[ C(w, b) = \\frac{1}{2n}\\sum_x||y(x)-a||^2 \\tag{6} \\] 其中，\\(w\\) 表示网络的权重参数，\\(b\\) 表示 biases，\\(n\\) 是样本数，\\(a\\) 是网络的输出结果。我们称 \\(C\\) 为二次代价函数，或者称为平方差(MSE)。当 \\(y(x)\\) 和 \\(a\\) 很接近的时候，\\(C \\approx 0\\)。因此，我们的训练算法就是为降低代价函数的值，而最常用的算法就是梯度下降(gradient descent)。 其实我们在高中阶段就遇到过类似的问题：已知函数曲线过几个点，求出这条曲线的方程。不同的是，这里是用代价函数间接求函数参数，而且，这里不是要让函数穿过这些点，而是去拟合、逼近这些点。 现在我们要思考一个问题，为什么要使用平方差作为代价函数？既然我们感兴趣的就是图片被正确分类的数量，那为什么不直接降低这个数量的值，而是绕个弯去降低一个二次代价函数？原因在于图片正确分类的数量这个函数不是一个平滑的函数，换句话说，\\(w\\) 和 \\(b\\) 的微小变化对这个函数的影响是不可控的，道理和上面的 sigmoid 函数一样。所以，我们采用这个上面的二次代价函数。 事实上，还有其他平滑的函数可以作为代价函数，这里我们只简单介绍二次代价函数。 讲到这里，我们提到了两次平滑函数：sigmoid 和 二次代价函数。其中，前者是神经网络的输出，后者是对神经网络结果的一种评估，是为了方便对网络参数进行训练。这里要求使用平滑函数是为了使训练的过程更加可控。虽然我们优化的时候是针对代价函数调整参数，但 sigmoid 函数会在代价函数中被使用，所以这两个函数都必须是平滑的。 gradient descent 下面，我们先将这些函数抛在一边，研究一下梯度下降方法。 假设我们要最小化一个函数 \\(C(\\overline v)\\)，其中 \\(\\overline v = v_1, v_2, …\\)。 简单起见，我们假设参数是二维的，函数图像长这个样子： valley 想求这个函数在哪个点取的最小值，数学家们的方法是对函数求导（多个参数就求偏导），然后判断在每一维上的单调性，最后求出在每个维度上的最小值点。这种方法理论上一定可以求出这个函数的最低点，不过，实际上却很难执行，因为函数图像可能会非常复杂，维度可能很高（上图只是一个简单的例子）。 所以，科学家们提出一种看似简单但实际上却屡试不爽的技巧：梯度下降。这种方法的思路是：不管函数图像是什么样的，反正我只往函数每一维度的梯度方向前进。所谓函数梯度，其实就是函数的导数方向：\\(\\nabla C=(\\frac{\\partial C}{\\partial {v_1}}, \\frac{\\partial C}{\\partial {v_2}})^T\\)。然后，我们让函数参数也往这个方向移动：\\(v → v&#39; = v + \\Delta v = v -\\eta \\nabla C\\)，其中，\\(\\eta\\) 称为学习率，\\(\\Delta v\\) 称为步长。这样，函数每次的偏移量为 \\(\\Delta C \\approx \\nabla C \\Delta v = \\frac{\\partial C}{\\partial v_1} \\Delta v_1 + \\frac{\\partial C}{\\partial v_2} \\Delta v_2\\)。不管函数导数的值是正是负（函数图像向上还是向下），只要学习率适当，这个式子都能保证函数往最低点走，当然，如果学习率的取值过大，函数的下降可能会出现曲折抖动的情况。 梯度下降也存在一些不足之处，比如，如果函数存在多个局部最低值，梯度下降可能会陷入局部最低点出不来。 回到实际问题，现在我们将梯度下降应用到网络参数中： \\[ w_k → w_{k}&#39; = w_k-\\eta \\frac{\\partial C}{\\partial w_k} \\] \\[ b_l → b_{l}&#39; = b_l-\\eta \\frac{\\partial C}{\\partial b_l} \\] 通过不断迭代上面的过程，代价函数会不断下降，运气好的话就可能下降到全局最低点的位置。 stochastic gradient descent 不过，这里有个计算上的问题，我们之前的二次代价函数为：\\(C(w,b)=\\frac{1}{2n}\\sum_x ||y(x)-a||^2\\)，因此，在计算导数的时候，需要将每个样本的导数值都加起来取平均，这在概率学上是有意义的（防止个别噪声样本的影响），但实际计算的时候，由于样本数量很大，这无疑会造成巨大的计算量。因此，有人又提出一种随机梯度下降(stochastic gradient descent)的方法来加快训练。这种方法每次只挑选少量的随机样本进行训练（当然，所有样本在一轮训练中都需要被挑选到）。 具体来说，假设我们每次挑选 m 个随机样本进行训练，总样本数为 n，那么，只要 m 足够大，我们可以得到一个近似关系（大数定理？）： \\[ \\frac{\\sum_{j=1}^{m}\\Delta C_{X_{j}}}{m} \\approx \\frac{\\sum_{x} \\Delta C_x}{n} = \\Delta C \\tag{18} \\] 然后，每次对参数的训练就变成： \\[ w_k→w_{k}&#39;=w_k-\\frac{\\eta}{m} \\sum_j \\frac{\\partial C}{\\partial w_k} \\tag{20} \\] \\[ b_l→b_l&#39;=b_l-\\frac{\\eta}{m} \\sum_j \\frac{\\partial C}{\\partial b_l} \\tag{21} \\] 有时候，人们会忽略等式前面的\\(\\frac{1}{n}\\)或\\(\\frac{1}{m}\\)，只在单一的样本上进行训练。这种方法在样本事先不知道（例如，样本是实时产生的）的情况下比较有效。 参考 Using neural nets to recognize handwritten digits","raw":null,"content":null,"categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://jermmy.github.io/categories/机器学习/"}],"tags":[{"name":"深度学习","slug":"深度学习","permalink":"https://jermmy.github.io/tags/深度学习/"}]},{"title":"论文笔记：Joint Embeddings of Shapes and Images via CNN Image Purification","slug":"2017-5-10-paper-notes-joint-embeddings-of-shapes-and-images-via-cnn-image-purification","date":"2017-05-10T11:53:56.000Z","updated":"2018-05-11T10:35:07.000Z","comments":true,"path":"2017/05/10/2017-5-10-paper-notes-joint-embeddings-of-shapes-and-images-via-cnn-image-purification/","link":"","permalink":"https://jermmy.github.io/2017/05/10/2017-5-10-paper-notes-joint-embeddings-of-shapes-and-images-via-cnn-image-purification/","excerpt":"今天分享的这篇论文是 SIGGRAPH 2015 的入选论文，标题比较长，但它做的事情其实很简单：通过一张图片，找到和这张图片最相似的 3D 形状👇。\n\nimage query\n","text":"今天分享的这篇论文是 SIGGRAPH 2015 的入选论文，标题比较长，但它做的事情其实很简单：通过一张图片，找到和这张图片最相似的 3D 形状👇。 image query 论文的思路 一开始看到论文的结果图的时候，觉得这个想法还是很有新意的。由于我刚刚踏入图形学的领域，对这类技术的认识还比较肤浅，就在网上搜了下相关的内容，发现十多年前就已经有人在研究相关的算法了。有利用 3D 形状的拓扑性质的，有提取一些特征算子的，也有通过图像检索的。总的来说，3D 形状的检索已经是一个较老的话题了。这篇论文也是从图像检索的角度出发，结合 3D 模型的投影图像，创造性地将图像和形状融合进一个空间中。通过空间中的点之间的距离，可以很方便地计算出图像与形状的相似度。 论文最主要的工作就是构造一个可以嵌套图像和形状的空间。要将 3D 和 2D 统一在一起，难度是比较大的。一般来说，3D 模型包含的信息更多，而图像方面的技术则比较成熟。因此，论文选择找一块「二向箔」将 3D 投影成 2D 后，再通过图像特征提取技术构造出这个空间。 具体流程分为四步： 构造形状嵌套空间（Shape Embedding Space）； 合成训练图片（训练 CNN）； 通过 CNN 将图片映射进 Shape Embedding Space； 输入图片预测 3D 模型。 Embedding Space Construction 第一步，我们先根据 3D 模型构造出这个空间。 Shape Similarity 在构造出这个空间之前，我们先思考一下，3D 模型之间的相似度要怎么测量？ 前面说了，论文是通过将 3D 形状投影成图像的方法，将该问题转化为我们熟悉的图像相似度的比较。 假设我们有一个 3D 模型的集合（这里面可能包括椅子、桌子、飞机等，参考上图）：\\(S={\\lbrace S_i \\rbrace}_{i=1}^{n}\\)。这个集合里的模型形状要事先做好对齐。 然后，我们从 k 个不同的视角（viewpoint）对各个模型进行投影，得到每个模型的投影图片集合：\\(I_i={\\lbrace I_{i,v} \\rbrace}_{v=1}^{k} \\ \\ for\\ each \\ \\ S_i\\)。论文中 k 取 20。 接着，对于每张投影图片 \\(I_i\\)，提取图片的 HoG 特征：\\(H_{i, v} \\in R^{10188}\\)。这里提取的 HoG 特征由三个尺寸的图像特征组合而成：120x120，60x60，30x30。将每个尺寸下的 HoG 特征组合在一起，形成一个 10188 维的向量。由于 3D 形状事先已经对齐了，所以 HoG 在鲁棒性上得到了保证。 对于每一个形状 \\(S_i\\)，我们组合所有投影图片的 HoG 特征（这个特征又称为 LightField HoG），得到形状本身的特征 \\(F_i=(H_{i,1}; H_{i,2}; …; H_{i,k}) \\in R^{203760}\\)，这个特征的维度为 203760（10188 x 20）。对于两个不同形状间的相似度比较，可以由这个特征向量的欧氏距离计算得到：\\(d_{i,j}=||F_i - F_j||_2\\)。 Embedding Space 有了这个基本的相似度计算方法后，我们可以开始着手构建这个空间了。 论文中对这个空间提出了两点要求： 空间中的距离要能够体现相似性程度； 空间维度要尽可能低。 构造空间最简单的方法就是直接使用上面提出来的特征向量 \\(F_i\\)，构造一个特征空间 \\(F=\\lbrace F_i \\rbrace\\)。这个特征空间的维度明显偏高（203760维），因此，论文中又使用 PCA 算法对特征向量进行降维，得到一个新的特征空间 \\(F^{-}\\)，维度为 128 维。 但这个降维后的空间存在一个问题：虽然 \\(F^{-}\\) 能够表示形状之间的相似度，但却难以体现不相似度。换句话说，相似的形状之间的距离都很相近，但不相似的形状（比如：椅子和汽车）之间的距离则完全没有意义（可能很近也可能很远）。论文中的说法是：\\(F^{-}\\) does not respect this distinction between the greater importance of the samll distances and the lesser one of the larger ones. 由于我英语水平有限，对这句顺口溜不是特别理解，但不管怎样，\\(F^{-}\\) 是存在不足的。 因此，论文提出一个新的空间 \\(D_{n*n}\\)，代表一个距离矩阵：\\(D(i, j) = d_{i, j}\\)。由于 \\(F\\) 是原特征向量组成的空间，而 \\(D\\) 则是一个距离矩阵，因此，对 \\(D\\) 降维后，仍然可以很好地表达相似性和不相似性（这一点暂时不是很理解）。降维方面，作者比较了多种方法，最后发现，使用 non-linear Multi-Dimensional Scaling(MDS) 方法效果上最好。该方法的提出是在 1964 年，听起来相当的有格调又很有历史感，我暂时没有去了解，所以先姑且把它当作一个黑盒子吧。总之，我们通过这个方法，将距离矩阵 \\(D_{n*n}\\) 降维成 \\(D^{-}\\)，维度同样保持 128 维（见下图）。然后，将降维后的矩阵 \\(D^{-}\\) 的行向量当作形状的特征向量。这种用距离来表示特征的方法我是第一次遇到，还不是很理解。作者在论文的 PPT 中这样解释：如果两个形状跟其他所有形状之间的 distance 差不多，也就是说它们在 \\(D^{-}\\) 的行向量很相似，那么这两个形状也应该是类似的。 CNN for Image Embedding 构造出 \\(D^{-}\\) 空间后，我们相当于把一个 3D 模型 \\(S_i\\) 映射到 \\(D^{-}\\) 中的一个点 \\(P_{S_{i}}\\)。下一步，就是要把图像也映射进这个空间，这相当于一个特征提取的工作，而 CNN 正是完成该工作的最佳选择之一。 我们的目标是训练一个 CNN 网络，这个网络的输入是一张图片，输出是一个向量 \\(P_{I} \\in D^{-}\\)。要求是，如果图片 \\(I\\) 描述的是一个形状类似于 \\(S_i\\) 的物体，那么 \\(P_{I}\\) 要接近于 \\(P_{S_{i}}\\)。 Training Image Synthesis 这一步将准备合成训练图片。需要先明确一点，论文中用到的所有训练图片，都是对 3D 模型进行投影渲染合成的。换句话说，这些图片并不是真实世界中的物体图片。不过，由于论文中用到的 3D 模型在制作上都非常真实，因此这些模型渲染出来的图片和真实图片很相似。 由于我们是要把一张图片转换成一个特征向量，因此训练数据的格式应该是:（图片，向量）。由于之前构造空间 \\(D^{-}\\) 的时候，我们已经计算出了每个形状 \\(S_i\\) 对应的特征向量，因此，只需要对 \\(S_i\\) 各个角度的投影进行渲染得到一个图片集 \\(R_i\\)，对于图片集中的每张图片 \\(R_{i,r}\\)，我们相当于得到了一个训练样本 \\((R_{i,r}, P_{S_{i}})\\)。由于 CNN 容易过拟合，因此，除了生成大量训练样本外，论文还在渲染投影图片的时候，加入一些额外的「噪声」，比如：添加不同的光照、采用多个角度进行投影，以及融合一些随机背景等。 论文中总共生成了约 100 万张训练图片。 Network Architecture and Training 论文中直接采用了 AlexNet 的架构，唯一不同的只是将网络最后一层 fc8 的输出改为 128 维的向量（原 AlexNet 是将 fc8 输入到 softmax 层），同时将 Softmax Loss Layer 换成 Euclidean Loss Layer。这个损失函数具体为： \\(L(\\theta)=\\sum_{i,r}{||f(R_{i,r}; \\theta) - P_{S_{i}}||_{2}^{2}}\\) 上图中，左图为原来的 AlexNet，右图为论文中采用的 CNN。 由于真实环境的照片中，背景会更加复杂，因此，网络前面五层卷积层的参数直接使用其他人在 ImageNet 上训练好的参数。而网络后面三层全连接层 FC Layer，则将参数随机初始化，并用之前生成的数据进行训练，这样，每次梯度下降的时候只优化后面三层网络的参数。这个过程又被称为 fine-tune。 作者称这个 CNN 的工作为 purifying，译为「清理」的意思。也就是说，CNN 可以把图片中一些无关的背景等噪音剔除出去，识别出物体特有的特征向量。 Predicting 预测的时候，我们需要将图片输入到 CNN 中，得到一个特征向量。然后在 \\(D^{-}\\) 空间内，用 KNN 之类的方法找到距离最近的向量，这些向量对应的形状就是图片中的物体最相似的形状。而如果要用形状来检索，则需要先把形状投影成图片后，再放入 CNN 中，然后用同样的方法找到最相似的形状。另外，这个空间还可以用来以图搜图，只要将所有图片都输入到 CNN 中，找出它们在空间中的位置，根据欧氏距离就可以判断出最相似的图片了。个人觉得这一点正是这个形状嵌套空间的优势所在。 实验结果及应用 论文从多角度图片检索(cross-view image retrieval)、基于图片的形状检索，以及基于形状的图片检索上，和其他方法进行了对比，结果均有提高。当然，针对不同的形状模型，提高的幅度也是不尽相同。 我更感兴趣的是这个东西有哪些应用。 最容易想到的，自然是 3D 模型以及图片的检索上。而且，由于论文构造的空间将 3D 和 2D 融合在一起，因此，对以图搜形，以形搜图、以图搜图、以形搜形来说，都是通用的。不过，由于 CNN 只接收图片作为输入，如果想用形状来检索，就必须先把形状投影成图片才行。 论文还提到另一种好玩的应用 Shape-Guided Image Editing。 比如，我们想在图片中为椅子增加一个台灯照射下的阴影效果，这个时候，直接基于图像处理的方法是比较难做到的，而换个角度，可以先用一个类似的 3D 模型投射出这个阴影后，再把阴影加入到原图中。 参考 Distance matrix","raw":null,"content":null,"categories":[{"name":"计算机图形学","slug":"计算机图形学","permalink":"https://jermmy.github.io/categories/计算机图形学/"}],"tags":[{"name":"计算机视觉","slug":"计算机视觉","permalink":"https://jermmy.github.io/tags/计算机视觉/"},{"name":"计算机图形学","slug":"计算机图形学","permalink":"https://jermmy.github.io/tags/计算机图形学/"},{"name":"深度学习","slug":"深度学习","permalink":"https://jermmy.github.io/tags/深度学习/"},{"name":"论文","slug":"论文","permalink":"https://jermmy.github.io/tags/论文/"}]},{"title":"论文笔记：Rich feature hierarchies for accurate object detection and semantic segmentation","slug":"2017-5-8-paper-notes-rcnn","date":"2017-05-08T12:40:06.000Z","updated":"2019-05-06T12:25:21.000Z","comments":true,"path":"2017/05/08/2017-5-8-paper-notes-rcnn/","link":"","permalink":"https://jermmy.github.io/2017/05/08/2017-5-8-paper-notes-rcnn/","excerpt":"在上计算机视觉这门课的时候，老师曾经留过一个作业：识别一张 A4 纸上的手写数字。按照传统的做法，这种手写体或者验证码识别的项目，都是按照定位+分割+识别的套路。但凡上网搜一下，就能找到一堆识别的教程，分割的文章次之，而定位的文章就少之又少了。这其中的缘由也很简单：识别目前来说已经不是什么难事了，所以容易写，但分割和定位却仍然是一个头疼不已的问题，不同场景方法不同，甚至同一场景也要结合多种图像处理方法，因此很难有通用的解决策略。在深度学习火起来之后，很多研究人员开始尝试用深度学习的特征提取能力来解决定位的问题，本文学习的这篇论文，就是被誉为用 CNN 解决物体定位的开山之作：R-CNN。\n这篇文章打算浅浅地分析一下 R-CNN。\n\n\n","text":"在上计算机视觉这门课的时候，老师曾经留过一个作业：识别一张 A4 纸上的手写数字。按照传统的做法，这种手写体或者验证码识别的项目，都是按照定位+分割+识别的套路。但凡上网搜一下，就能找到一堆识别的教程，分割的文章次之，而定位的文章就少之又少了。这其中的缘由也很简单：识别目前来说已经不是什么难事了，所以容易写，但分割和定位却仍然是一个头疼不已的问题，不同场景方法不同，甚至同一场景也要结合多种图像处理方法，因此很难有通用的解决策略。在深度学习火起来之后，很多研究人员开始尝试用深度学习的特征提取能力来解决定位的问题，本文学习的这篇论文，就是被誉为用 CNN 解决物体定位的开山之作：R-CNN。 这篇文章打算浅浅地分析一下 R-CNN。 R-CNN是什么 R-CNN 中的 R 指的是 Region，所以，R-CNN 其实就是用 CNN 来定位 Region。它由 Ross Girshick 等人于 2014 年提出，当时实现了物体检测上 state-of-art（最好） 的精度，而且速度上也比以往的方法有所提高。 R-CNN 的流程 文章最开始的图片中给出了 R-CNN 的基本框架。R-CNN 的算法流程其实很简单，主要分为三步： 通过 Selective Search 选出大约 2000 个 bounding box（不了解 Selective Search 的同学可以上网自学，不介意的话还可以查看我上一篇文章，由于本文重点是 R-CNN，所以这里直接把它当作一个黑盒子使用了）； 用 CNN 对这些 bounding box 提取特征； 使用 linear SVM 对这些特征进行分类，判断 bounding box 内是否是物体，或者是什么物体。 训练阶段 训练阶段分为 CNN 和 SVM 两部分。 1. Supervised pre-training CNN 模型直接采用了 Krizhevsky 等人在 2012 年的分类网络模型 这个网络的参数比较多（超过一百万），因此需要大量的训练数据。作者直接将 Krizhevsky 在 ILSVRC 2012 上训练好的参数复制过来，有人称这种做法为「迁移学习」，因为 CNN 的本质就是提取图片的特征（类似于 SIFT ），因此，尽管训练集不同，训练出来的参数其实是可以通用的。当然啦，对于一些更具体的任务（如手写体识别），个人觉得最好还是单独训练一下。 2. Domain-specific fine-tuning 前面刚说了，直接用其他数据集训练好的参数不太保险，作者马上用行动打消了我的疑虑。这一步，为了让 CNN 适应新的数据集，作者在原来参数的基础上，用 PASCAL VOC 的数据继续训练网络（PASCAL VOC 是论文采用的数据集）。另外，考虑到 VOC 的数据集比较小（ILSVRC 2012 有 1000 种类别，VOC 只有 20 种），论文将原网络最后一层 1000 classification 的输出层减小为 21 classification（20个类别 + 背景），并将这一层 fc 的参数随机初始化，这一步相当于优化参数 (fine-tuning) 的作用。除此之外，为了获得大量的训练样本，还需要事先对 VOC 数据集标定出物体所在的矩形区域（这一步比单纯的图片类别标记难度要大得多，因此数据量很少，且缺乏背景数据）。 之后，论文采用 Selective Search 在 VOC 上选出很多矩形框，将与标准矩形框的覆盖区域 &gt;= 0.5 IoU 的标为正样本，其余的标为负样本（负样本应该就是背景了）。然后，采用 SGD，以 0.001 为学习率进行训练。每次训练的 batch 为 32 个正样本和 96 个背景样本。这里之所以让背景样本多于正样本，原因在于图片中的正样本窗口相比于背景窗口，数量很少。 关于 IoU，这里简单提一下。IoU 指的是 intersection-over-union，是一个定位精度的评价标准。因为算法的结果不可能百分之百跟人工标定的矩形数据吻合，因此就需要有一个标准判断结果的准确性。 上面这幅图摘自文末链接。IoU 的计算方法为：\\(IOU=(A \\bigcap B) / (A \\bigcup B)\\)。 3. Object category classifiers 训练完 CNN 后，我们就可以通过它提取物体的特征了。这里使用 fc7 层输出的 4096 维的向量作为特征。 对于每一类物体，我们要训练一个二分类的 SVM 模型，由 SVM 来判断矩形区域内是否有物体存在。SVM 的训练数据是 CNN 提取的特征向量，对应的 label 和上面一样分为 21 类。与 CNN 不同的是，我们重新调整了 IoU 的阈值。在 CNN 中，我们将 &gt;= 0.5 IoU 的矩形都标为正样本，但在训练 SVM 的时候，作者发现，取 0.3 IoU 效果最好。所以，这一次我们将 &gt;= 0.3 IoU 的矩形都标记为正样本作为 SVM 的训练数据。 预测 预测的时候，我们先通过 Selective Search 选出 2000+ 矩形区域，用 CNN 的 fc7 层提取特征向量，再用 SVM 对这些矩形区域进行评分判断。由于矩形区域存在大量的重叠，因此论文最后用非极大值抑制（non-maximum suppression）的方法对这些区域进行筛选。 具体做法是：先从所有矩形区域中，选出 SVM 得分最高的区域，将与该区域的 IoU 面积超过阈值的都删除，然后从剩下的矩形区域中再挑选出得分最高的，继续剔除 IoU 超过阈值的区域，如此往复直到没有矩形区域可选为止。 做完这一步，剩下的矩形基本就是比较可靠的物体所在的位置了，但作者还想进一步提高精度。根据前面对 IoU 的介绍，想提高定位的精度，就必须保证预测出来的矩形框和 ground truth 尽量靠近。文章对这一步没有做过多的介绍，只是说结合 CNN 的 pool5 层提取的特征以及 DPM 中的 box regression，训练一个线性回归模型对矩形位置进一步微调。这里我没有仔细去看论文，只是参考了这篇博客，所以简单说一下我的理解。通常，预测出来的矩形框和 ground truth 之间存在位置和尺度上的差别，因此，我们可以计算出两个矩形中心点之间的偏移 (\\(\\Delta x\\), \\(\\Delta y\\))，以及长宽尺寸上的比例 (\\(S_x\\), \\(S_y\\))。然后，训练一个线性回归模型，根据 pool5 的特征来拟合这四个变量。预测的时候，这个模型在看到一个 pool5 的特征后，就知道这种特征应该微调的尺寸是多少，继而对矩形框实现微调。 几个细节问题 1. CNN的输入 论文中使用的 CNN 需要输入 227*227 的图片，但 Selective Search 找出来的矩形框尺寸各异，因此需要提供一种方法对矩形中的图片进行归一化。 归一化方法无非分为两种：各向异性的缩放和各向同性的缩放。 各向异性的缩放就是直接将图片缩放成 227*227 的规格，而不管图片内容是否扭曲，这种方法最简单，但效果应该比较差。 各向同性的缩放就是要保持长宽的比例，一种方法是将 bounding box 裁剪出来后，先缩放，再用固定的背景颜色填充到需要的尺寸；另一种方法是将原本的 bounding box 的边界延伸后，再进行裁剪及缩放，如果延伸后的 bounding box 超过图片边界，就用 bounding box 中的颜色均值进行填充。 论文中采用的应该是各向异性的方法，但我倾向于采用各向同性的方法。 2. CNN对特征的提取 CNN 中包含很多层，具体要选用哪一层的输出作为特征呢？为此，作者进行了多次对比试验： 实验结果显示，如果没有进行 fine-tuning，那么 pool5、fc6 以及 fc7 的结果都相差无几，而如果进行 fine-tuning 后，fc6 和 fc7 的结果有了显著提升。 前面说过，FT 就是针对特定情景的训练集，将原本最后一层的 1000 维输出转变为 21 维的输出。如果没有进行 FT，那么相当于把 CNN 当作一种通用的图像特征来使用（只在 ILSVRC 2012 上训练的 CNN，效果上类似 SIFT），其结果表明，pool5、fc6 以及 fc7 效果上差不多，也就是说，在这种情况下，fc6 和 fc7 对特征的表达几乎不起作用，因此我们可以得出这样一个结论：在没有 FT 的时候，CNN 的特征主要是由卷积层（这里是 pool5）学习出来的。 但如果对 CNN 做过 FT 处理，fc6 和 fc7 的效果会有显著提升，反倒是 pool5 的提升效果比较小。对此，我的理解是这样的：pool5 提取的更像是一种通用的特征，不针对特定的训练集，而 fc6 和 fc7 则更具有导向性，对具体的应用场景比较敏感。套用文末参考链接的例子，如果我们要通过图像中的人脸预测性别，那么 pool5 提取的是通用的人脸特征，而 fc6 和 fc7 提取的则是更多跟性别相关的信息。 表中第 7 行是用 BB（binding box regression）微调后的结果，精度上有了进一步提高。 参考 RCNN学习笔记(2):Rich feature hierarchies for accurate object detection and semantic segmentation R-CNN详解 【目标检测】RCNN算法详解 边框回归(Bounding Box Regression)详解","raw":null,"content":null,"categories":[{"name":"计算机视觉","slug":"计算机视觉","permalink":"https://jermmy.github.io/categories/计算机视觉/"}],"tags":[{"name":"计算机视觉","slug":"计算机视觉","permalink":"https://jermmy.github.io/tags/计算机视觉/"},{"name":"深度学习","slug":"深度学习","permalink":"https://jermmy.github.io/tags/深度学习/"},{"name":"论文","slug":"论文","permalink":"https://jermmy.github.io/tags/论文/"}]},{"title":"论文笔记：Selective Search for Object Recognition","slug":"2017-5-4-paper-notes-selective-search","date":"2017-05-04T14:18:01.000Z","updated":"2019-05-05T01:55:35.000Z","comments":true,"path":"2017/05/04/2017-5-4-paper-notes-selective-search/","link":"","permalink":"https://jermmy.github.io/2017/05/04/2017-5-4-paper-notes-selective-search/","excerpt":"与 Selective Search 初次见面是在著名的物体检测论文 Rich feature hierarchies for accurate object detection and semantic segmentation ，因此，这篇论文算是阅读 R-CNN 的准备。\n这篇论文的标题虽然也提到了 Object Recognition ，但就创新点而言，其实在 Selective Search 。所以，这里只简单介绍 Selective Search 的思想和算法过程，对于 Object Recognition 则不再赘述。\n什么是 Selective Search\nSelective Search，说的简单点，就是从图片中找出物体可能存在的区域。\n\n\n\n上面这幅宇航员的图片中，那些红色的框就是 Selective Search 找出来的可能存在物体的区域。","text":"与 Selective Search 初次见面是在著名的物体检测论文 Rich feature hierarchies for accurate object detection and semantic segmentation ，因此，这篇论文算是阅读 R-CNN 的准备。 这篇论文的标题虽然也提到了 Object Recognition ，但就创新点而言，其实在 Selective Search 。所以，这里只简单介绍 Selective Search 的思想和算法过程，对于 Object Recognition 则不再赘述。 什么是 Selective Search Selective Search，说的简单点，就是从图片中找出物体可能存在的区域。 上面这幅宇航员的图片中，那些红色的框就是 Selective Search 找出来的可能存在物体的区域。 在进一步探讨它的原理之前，我们分析一下，如何判别哪些 region 属于一个物体？ 作者在论文中用以上四幅图，分别描述了四种可能的情况： 图 a ，物体之间可能存在层级关系，比如：碗里有个勺； 图 b，我们可以用颜色来分开两只猫，却没法用纹理来区分； 图 c，我们可以用纹理来区分变色龙，却没法用颜色来区分； 图 d，轮胎是车的一部分，不是因为它们颜色相近、纹理相近，而是因为轮胎包含在车上。 所以，我们没法用单一的特征来定位物体，需要综合考虑多种策略，这一点是 Selective Search 精要所在。 需要考虑的问题 在学习 Selective Search 算法之前，我曾在计算机视觉课上学到过关于物体（主要是人脸）检测的方法。通常来说，最常规也是最简单粗暴的方法，就是用不同尺寸的矩形框，一行一行地扫描整张图像，通过提取矩形框内的特征判断是否是待检测物体。这种方法的复杂度极高，所以又被称为 exhaustive search。在人脸识别中，由于使用了 Haar 特征，因此可以借助 Paul Viola 和 Michael Jones 提出的积分图，使检测在常规时间内完成。但并不是每种特征都适用于积分图，尤其在神经网络中，积分图这种动态规划的思路就没什么作用了。 针对传统方法的不足，Selective Search 从三个角度提出了改进： 我们没法事先得知物体的大小，在传统方法中需要用不同尺寸的矩形框检测物体，防止遗漏。而 Selective Search 采用了一种具备层次结构的算法来解决这个问题； 检测的时间复杂度可能会很高。Selective Search 遵循简单即是美的原则，只负责快速地生成可能是物体的区域，而不做具体的检测； 另外，结合上一节提出的，采用多种先验知识来对各个区域进行简单的判别，避免一些无用的搜索，提高速度和精度。 算法框架 论文中给出的这个算法框架还是很详细的，这里再简单翻译一下。 输入：彩色图片。 输出：物体可能的位置，实际上是很多的矩形坐标。 首先，我们使用这篇论文的方法将图片初始化为很多小区域 \\(R={r_i, …, r_n}\\)。由于我们的重点是 Selective Search，因此我直接将该论文的算法当成一个黑盒子。 初始化一个相似集合为空集： \\(S=\\varnothing\\)。 计算所有相邻区域之间的相似度（相似度函数之后会重点分析），放入集合 S 中，集合 S 保存的其实是一个区域对以及它们之间的相似度。 找出 S 中相似度最高的区域对，将它们合并，并从 S 中删除与它们相关的所有相似度和区域对。重新计算这个新区域与周围区域的相似度，放入集合 S 中，并将这个新合并的区域放入集合 R 中。重复这个步骤直到 S 为空。 从 R 中找出所有区域的 bounding box（即包围该区域的最小矩形框），这些 box 就是物体可能的区域。 另外，为了提高速度，新合并区域的 feature 可以通过之前的两个区域获得，而不必重新遍历新区域的像素点进行计算。这个 feature 会被用于计算相似度。 相似度计算方法 相似度计算方法将直接影响合并区域的顺序，进而影响到检测结果的好坏。 论文中比较了八种颜色空间的特点，在实际操作中，只选择一个颜色空间（比如：RGB 空间）进行计算。 正如一开始提出的那样，我们需要综合多种信息来判断。作者将相似度度量公式分为四个子公式，称为互补相似度测量(Complementary Similarity Measures) 。这四个子公式的值都被归一化到区间 [0, 1] 内。 1. 颜色相似度\\(s_{color}\\ (r_i, r_j)\\) 正如本文一开始提到的，颜色是一个很重要的区分物体的因素。论文中将每个 region 的像素按不同颜色通道统计成直方图，其中，每个颜色通道的直方图为 25 bins （比如，对于 0 ～ 255 的颜色通道来说，就每隔 9(255/25=9) 个数值统计像素数量）。这样，三个通道可以得到一个 75 维的直方图向量 \\(C_i={c_{i}^{1}, …, c_{i}^{n}}\\)，其中 n = 75。之后，我们用 L1 范数（绝对值之和）对直方图进行归一化。由直方图我们就可以计算两个区域的颜色相似度： \\[ s_{color}(r_i, r_j) =\\sum_{k=1}^{n}{min(c_{i}^{k}, c_{j}^{k})} \\] 这个相似度其实就是计算两个区域直方图的交集。 这个颜色直方图可以在合并区域的时候，很方便地传递给下一级区域。即它们合并后的区域的直方图向量为：\\[C_t=\\frac{size(r_i)*C_i+size(r_j)*C_j}{size(r_i)+size(r_j)}\\]，其中 \\(size(r_i)\\) 表示区域 \\(r_i\\) 的面积，合并后的区域为 \\(size(r_t)=size(r_i)+size(r_j)\\)。 2. 纹理相似度\\(s_{texture}\\ (r_i, r_j)\\) 另一个需要考虑的因素是纹理，即图像的梯度信息。 论文中对纹理的计算采用了 SIFT-like 特征，该特征借鉴了 SIFT 的计算思路，对每个颜色通道的像素点，沿周围 8 个方向计算高斯一阶导数(\\(\\sigma = 1\\))，每个方向统计一个直方图（bin = 10），这样，一个颜色通道统计得到的直方图向量为 80 维，三个通道就是 240 维：\\(T_i={t_i^{(1)}, …, t_i^{(n)}}\\)，其中 n = 240。注意这个直方图要用 L1 范数归一化。然后，我们按照颜色相似度的计算思路计算两个区域的纹理相似度： \\[ s_{texture}(r_i, r_j) =\\sum_{k=1}^{n}{min(t_{i}^{k}, t_{j}^{k})} \\] 同理，合并区域后，纹理直方图可以很方便地传递到下一级区域，计算方法和颜色直方图的一模一样。 3. 尺寸相似度\\(s_{size}\\ (r_i, r_j)\\) 在合并区域的时候，论文优先考虑小区域的合并，这种做法可以在一定程度上保证每次合并的区域面积都比较相似，防止大区域对小区域的逐步蚕食。这么做的理由也很简单，我们要均匀地在图片的每个角落生成不同尺寸的区域，作用相当于 exhaustive search 中用不同尺寸的矩形扫描图片。具体的相似度计算公式为： \\[ s_{size}(r_i, r_j)=1-\\frac{size(r_i) + size(r_j)}{size(im)} \\] 其中，\\(size(im)\\) 表示原图片的像素数量。 4. 填充相似度\\(s_{fill}(r_i, r_j)\\) 填充相似度主要用来测量两个区域之间 fit 的程度，个人觉得这一点是要解决文章最开始提出的物体之间的包含关系（比如：轮胎包含在汽车上）。在给出填充相似度的公式前，我们需要定义一个矩形区域 \\(BB_{ij}\\)，它表示包含 \\(r_i\\) 和 \\(r_j\\) 的最小的 bounding box。基于此，我们给出相似度计算公式为： \\[ s_{fill}(r_i, r_j)=1-\\frac{size(BB_{ij})-size(r_i)-size(r_j)}{size(im)} \\] 为了高效地计算 \\(BB_{ij}\\)，我们可以在计算每个 region 的时候，都保存它们的 bounding box 的位置，这样，\\(BB_{ij}\\) 就可以很快地由两个区域的 bounding box 推出来。 5. 相似度计算公式 综合上面四个子公式，我们可以得到计算相似度的最终公式： \\[ s(r_i, r_j) = a_1 s_{color}(r_i, r_j) +a_2s_{texture}(r_i, r_j) \\\\\\\\ +a_3s_{size}(r_i, r_j)+a_4s_{fill}(r_i, r_j) \\] 其中，\\(a_i\\) 的取值为 0 或 1，表示某个相似度是否被采纳。 Combining Locations 前面我们基本完成了 Selective Search 的流程，从图片中提取出了物体可能的位置。现在，我们想完善最后一个问题，那就是给这些位置排个序。因为提取出来的矩形框数量巨大，而用户可能只需要其中的几个，这个时候我们就很有必要对这些矩形框赋予优先级，按照优先级高低返回给用户。原文中作者称这一步为 Combining Locations，我找不出合适的翻译，就姑且保留英文原文。 这个排序的方法也很简单。作者先给各个 region 一个序号，前面说了，Selective Search 是一个逐步合并的层级结构，因此，我们将覆盖整个区域的 region 的序号标记为 1，合成这个区域的两个子区域的序号为 2，以此类推。但如果仅按序号排序，会存在一个漏洞，那就是区域面积大的会排在前面，为了避免这个漏洞，作者又在每个序号前乘上一个随机数 \\(RND \\in [0, 1]\\)，通过这个新计算出来的数值，按从小到大的顺序得出 region 最终的排序结果。 参考 Selective Search for Object Recognition(阅读) Efficient Graph-Based Image Segmentation","raw":null,"content":null,"categories":[{"name":"计算机视觉","slug":"计算机视觉","permalink":"https://jermmy.github.io/categories/计算机视觉/"}],"tags":[{"name":"计算机视觉","slug":"计算机视觉","permalink":"https://jermmy.github.io/tags/计算机视觉/"},{"name":"论文","slug":"论文","permalink":"https://jermmy.github.io/tags/论文/"}]},{"title":"微漫项目总结","slug":"2017-5-1-weiman-project-summary","date":"2017-05-01T01:43:25.000Z","updated":"2017-10-23T08:21:11.000Z","comments":true,"path":"2017/05/01/2017-5-1-weiman-project-summary/","link":"","permalink":"https://jermmy.github.io/2017/05/01/2017-5-1-weiman-project-summary/","excerpt":"\nweiman\n\n上个学期花了半年的时间做了这个应用，程序总体来说不大，而且中间还有大量的时间花在技术调研以及试错上，但实际有效开发时间也有两个月之久，因此值得总结的东西还是挺多。由于我主要开发 Android 端，所以这里的总结也只是针对 Android。至于算法方面，就先略过了。","text":"weiman 上个学期花了半年的时间做了这个应用，程序总体来说不大，而且中间还有大量的时间花在技术调研以及试错上，但实际有效开发时间也有两个月之久，因此值得总结的东西还是挺多。由于我主要开发 Android 端，所以这里的总结也只是针对 Android。至于算法方面，就先略过了。 微漫项目介绍 微漫其实就是一个自动版的脸萌。原版的脸萌是高度 DIY 的形式，大部分情况下很难拼出本人的特点，而微漫就是想针对这一点做出改进，通过一些更加符合真实五官比例的素材，以及人脸特征匹配技术，帮用户筛选出一些跟图片比较相似的素材，提高漫画头像的相似性。 这个出发点一开始觉得蛮有前途的，但越到后面，不合理的地方就越多。其中，最大的一点，在于卡通漫画本身追求的往往并不是通常意义上的相似性。并不是说，把用户本人的照片临摹成卡通头像，效果就越好。相反地，有时候对一些五官特征进行夸张（当然要恰到好处）反而会更加神似。当然啦，我们后面也考虑过用一些算法对原有的素材进行夸张化。由于我们是纯粹模仿脸萌的思路，素材都是 SVG 格式的，因此，对 SVG 格式的图片进行夸张化也给我们带来了巨大的挑战。且不说夸张化算法本身是否可行，光是对矢量图形的夸张化操作，目前也很难在业界找到一些实例或理论研究。这一点可能是导致项目最后中断的原因。当然啦，导致项目失败的最直接原因是我们的 SVG 素材实在太少了，画家方面所能提供的资源也是少得可怜，最后团队里做前端的同学居然也得帮忙操刀，惨。。 尽管目前这个项目算是废弃了，但中间遇到的坑还是有必要总结一下，也算是对过去半年的努力发一个安慰奖吧。 Android 端踩坑总结 数据库 Android 端的数据库采用的是 SQLite，主要用来保存 SVG 素材，后期为了素材加密又采用了 sqlcipher 。我之前对数据库的使用很少，因此这个项目算是正儿八经地使用数据库。其中遇到不少小问题，比如如何存储数组等。当然，最重要的一点是，数据库 SQLiteOpenHelper 要做成单例类，否则多线程读取数据库会导致数据库 lock。具体可以看这个链接SQLiteException: error code 5: database is locked. When accessing ContentProvider from AsyncTask。而且，选择单例类，能够有效避免数据库因为只 open 而没有 close 导致的内存泄漏问题，可以看另一个链接Correctly Managing your SQLite Database。 单例模式的内存泄漏问题 这个主要是在单例类中传入 Context 引起的，解决办法是将传入的 Activity 换成 Application。 SVG 文件的存储 五官素材都是以 SVG 文件的形式存在，文件数目比较多，每个文件大小大概在 1k ～ 2k。我考虑了两张方案，一种是直接以文件形式存放，再在数据库存放素材文件路径；另一种方案是将文件内容以 BLOB 格式写入。通过开多线程读取多个 SVG 素材并统计读取时间，最后我发现存数据库 BLOB 的方案效率较高，所以选择 BLOB 的方式存储。当然，由于素材数量很多，需要先压缩再存入数据库。而关于压缩算法，我从众多常用的压缩策略中，选择了压缩比更大的 Deflate 算法，时间上虽然开销略大，但总体还能接受（渲染头像的时候可以在 1～2s 内出现）。这是当时参考的链接 http://www.importnew.com/14410.html。 Java、JS通信 这个应用里面用到 JS 的地方主要集中在 WebView 渲染部分。虽然也只是简单调用 JS 的函数，并传一些参数什么的，但为了统一我还是用一个类来管理这些函数。后来需要导出 SVG 素材的时候，需要通过 JS 获得 HTML 中 SVG 标签的内容，这一步比较耗时，还需要给 JS 注册回调函数。这时我才意识到，随着交互的东西越来越多，一个通用的交互框架将变得十分必要。当然，后来由于项目进展受阻，也就不了了之了。 WebView的各种小问题 WebView 的问题几乎是最吃力不讨好的。熟悉它的朋友都知道，Android 的 WebView 一直有内存泄漏问题，当然我最后找到一个最简单粗暴的方法，暂时解决了它。那就是不在 XML 中声明 WebView，而是在 Java 中实例化，并传入 Application 作为 Context。在网上查资料的过程中，我发现，微信使用了一个 tool 进程来管理跟 WebView 相关的东西，而开多进程会导致数据通信十分麻烦，而且，由于我的数据类是单例的（这样可以加快渲染速度），会导致传参更加复杂，也就果断放弃多进程的方案了。 SurfaceView如何做放大镜 SurfaceView 涉及到 Android 很低层的机制了。它跟一般的 View 有一个比较明显的区别，就是我们没法得到 SurfaceView 的帧。在制作放大镜的时候，帧的获取又是必须的，结果导致 SurfaceView 的放大镜实现不是那么直接。在搜了一些资料后，我也找到了一种最常见的实现思路，具体可见另一篇博文Android：在SurfaceView上做放大镜效果。 JNI的使用 之前一直都很想学一下 JNI，这次终于有了这个机会。其实 JNI 也没什么神秘的，无非是先用 C/C++ 实现好核心功能后，再通过 JNI 接口和 Java 通信。不过真正写代码的时候，Debug 是一个很蛋疼的问题，因为 Android Studio 对 JNI 的支持还不是很好（当然，写这篇文章的时候 Google 已经增强了这方面的功能），当时为了加快调试的速度，我自己也找了一些技巧。 JNI 虽然看似酷炫，却未必有卵用。我觉得它最大的好处是跨平台，毕竟目前主流的操作系统（Android、ios等）都是对 C/C++ 很友好的。但对于一般的程序员，还真的没必要用到 JNI。原因除了 C/C++ 比 Java 更容易出错外，还在于它并不一定能给你带来性能上的提升。不过，如果技术真的屌，倒是可以把一些算法模块写在 C/C++ 里面，这样代码被反编译的可能性也更小。如果有人想直接调用你的 .so 库，你可以在 C/C++ 里面判断一下应用的打包密钥，这样，你的核心算法除了你的应用，其他人都动不了了。 SVG导出成位图以及动画相关 虽然我们的应用是针对矢量图的漫画制作，但业界主流平台支持的还是位图（PNG、JPG、GIF）等，因此，如果真的想在手机上查看或者在其他平台上传播头像，必须先把 SVG 导出成位图。我在 github 上搜了一下，能够完美支持 SVG 的库其实不多，后来找到一个最新的 androidsvg，虽然不是所有标签都支持，但基本满足要求了，渲染速度在图片尺寸不大的前提下还是能接受的。另外，微信对 SVG 的研究似乎也很有参考价值，不过，也仅仅供参考而已。 其实 SVG 目前推广的力度也只是在 Web 方面，Android 对它的支持并不是很好，虽然老早就出了 VectorDrawable，但它需要开发者遵循特定的语法规则（非标准的 SVG 格式）。这就导致它只适用于一些小图标的制作，而从浏览器导出的 SVG 标签可能会不适用（为了前端的便利，我们将 CSS 样式都单独抽了出来）。对于用户编辑并保存的头像，我们需要在应用中呈现给用户。这个时候，我面临两种选择，一种是根据用户头像的 SVG 数据直接渲染成位图，这种方法应该是最省空间的；另一种方法是，在用户保存头像的时候，除了保存 SVG 数据，再将 WebView 上的头像也截下来，这种方法会消耗一些额外的存储空间，但渲染速度却是最快的。我用 Glide 框架尝试了第一种方案，这里之所以选用 Glide 是因为它的灵活性真的太强，对 SVG 的支持很好（配合 androidsvg）。但由于硬件对 SVG 的渲染不支持，最后渲染速度成了一个很大的瓶颈，当删除一张头像重新刷新列表的时候，能够明显感觉到渲染上的「迟钝」 。最终，我还是选择了第二种方案。 SVG 的另一个坑是动画。SVG 的动画只在 Web 世界里通用，一旦离开浏览器的引擎，就完全跑不起来了。不管是 SVG 本身提供的 animation，还是基于 CSS 的动画，抑或是通过 Js 控制，都属于一种「解释型」的动画。而主流的平台其实还是倾向于帧动画的（主要是 GIF）。当时花了很多时间寻找一座能沟通这两种动画的桥梁（主要是寻找有没有类似的 Js 库），后来我觉得这就好像要把两个完全不同的流派统一起来，理论上可行，实际上忒难，最后也就放弃了。 其他问题 除了以上这些问题外，这个应用的架构方面也存在很多不足，灵活性方面其实很差。虽然我将算法的核心部分以及 Model 层都封装成全局的控制类，但后来需要添加素材或做一些修改的时候，发现需要改动的地方还是很多，改起来 bug 不断。我觉得一个好的架构其实就是一个改动很轻松，也不容易出 bug 的设计，所以这个应用的结构上是存在很大短板的。","raw":null,"content":null,"categories":[{"name":"Android","slug":"Android","permalink":"https://jermmy.github.io/categories/Android/"}],"tags":[{"name":"Android","slug":"Android","permalink":"https://jermmy.github.io/tags/Android/"}]},{"title":"浅析 Bag of Feature","slug":"2017-4-28-understand-bag-of-feature","date":"2017-04-28T14:40:12.000Z","updated":"2019-04-29T09:53:19.000Z","comments":true,"path":"2017/04/28/2017-4-28-understand-bag-of-feature/","link":"","permalink":"https://jermmy.github.io/2017/04/28/2017-4-28-understand-bag-of-feature/","excerpt":"Bag of Feature 是一种图像特征提取方法，它借鉴了文本分类的思路（Bag of Words），从图像抽象出很多具有代表性的「关键词」，形成一个字典，再统计每张图片中出现的「关键词」数量，得到图片的特征向量。\n\n\n","text":"Bag of Feature 是一种图像特征提取方法，它借鉴了文本分类的思路（Bag of Words），从图像抽象出很多具有代表性的「关键词」，形成一个字典，再统计每张图片中出现的「关键词」数量，得到图片的特征向量。 Bag of Words 模型 要了解「Bag of Feature」，首先要知道「Bag of Words」。 「Bag of Words」 是文本分类中一种通俗易懂的策略。一般来讲，如果我们要了解一段文本的主要内容，最行之有效的策略是抓取文本中的关键词，根据关键词出现的频率确定这段文本的中心思想。比如：如果一则新闻中经常出现「iraq」、「terrorists」，那么，我们可以认为这则新闻应该跟伊拉克的恐怖主义有关。而如果一则新闻中出现较多的关键词是「soviet」、「cuba」，我们又可以猜测这则新闻是关于冷战的（见下图）。 这里所说的关键词，就是「Bag of words」中的 words ，它们是区分度较高的单词。根据这些 words ，我们可以很快地识别出文章的内容，并快速地对文章进行分类。 而「Bag of Feature」也是借鉴了这种思路，只不过在图像中，我们抽出的不再是一个个「word」，而是图像的关键特征「Feature」，所以研究人员将它更名为「Bag of Feature」。 Bag of Feature 算法 从上面的讨论中，我们不难发现，「Bag of Feature」的本质是提出一种图像的特征表示方法。 按照「Bag of Feature」算法的思想，首先我们要找到图像中的关键词，而且这些关键词必须具备较高的区分度。实际过程中，通常会采用「SIFT」特征。 有了特征之后，我们会将这些特征通过聚类算法得出很多聚类中心。这些聚类中心通常具有较高的代表性，比如，对于人脸来说，虽然不同人的眼睛、鼻子等特征都不尽相同，但它们往往具有共性，而这些聚类中心就代表了这类共性。我们将这些聚类中心组合在一起，形成一部字典（CodeBook）。 对于图像中的每个「SIFT」特征，我们能够在字典中找到最相似的聚类中心，统计这些聚类中心出现的次数，可以得到一个向量表示（有些文章称之为「直方图」），如本文开篇的图片所示。这些向量就是所谓的「Bag」。这样，对于不同类别的图片，这个向量应该具有较大的区分度，基于此，我们可以训练出一些分类模型（SVM等），并用其对图片进行分类。 Bag of Feature 算法过程 「Bag of Feature」大概分为四步： 提取图像特征； 对特征进行聚类，得到一部字典（ visual vocabulary ）； 根据字典将图片表示成向量（直方图）； 训练分类器或者用 KNN 进行检索（这一步严格来讲不属于「Bag of Feature」的范畴）。 下面，我们简单分析一下每一步的实现过程。 提取图像特征 特征必须具有较高的区分度，而且要满足旋转不变性以及尺寸不变性等，因此，我们通常都会采用「SIFT」特征（有时为了降低计算量，也会采用其他特征，如：SURF ）。「SIFT」会从图片上提取出很多特征点，每个特征点都是 128 维的向量，因此，如果图片足够多的话，我们会提取出一个巨大的特征向量库。 训练字典（ visual vocabulary ） 提取完特征后，我们会采用一些聚类算法对这些特征向量进行聚类。最常用的聚类算法是 k-means。至于 k-means 中的 k 如何取，要根据具体情况来确定。另外，由于特征的数量可能非常庞大，这个聚类的过程也会非常漫长。 &lt;img src=“/images/2017-4-29/visual vocabulary.png” width=“400px” 聚类完成后，我们就得到了这 k 个向量组成的字典，这 k 个向量有一个通用的表达，叫 visual word。 图片直方图表示 上一步训练得到的字典，是为了这一步对图像特征进行量化。对于一幅图像而言，我们可以提取出大量的「SIFT」特征点，但这些特征点仍然属于一种浅层（low level）的表达，缺乏代表性。因此，这一步的目标，是根据字典重新提取图像的高层特征。 具体做法是，对于图像中的每一个「SIFT」特征，都可以在字典中找到一个最相似的 visual word，这样，我们可以统计一个 k 维的直方图，代表该图像的「SIFT」特征在字典中的相似度频率。 例如：对于上图这辆车的图片，我们匹配图片的「SIFT」向量与字典中的 visual word，统计出最相似的向量出现的次数，最后得到这幅图片的直方图向量。 训练分类器 当我们得到每幅图片的直方图向量后，剩下的这一步跟以往的步骤是一样的。无非是根据数据库图片的向量以及图片的标签，训练分类器模型。然后对需要预测的图片，我们仍然按照上述方法，提取「SIFT」特征，再根据字典量化直方图向量，用分类器模型对直方图向量进行分类。当然，也可以直接根据 KNN 算法对直方图向量做相似性判断。 Bag of Feature 在检索中的应用 「Bag of Feature」虽然是针对图像分类提出的算法，但它同样可以用到图像检索中。检索和分类本质上是同样的，但在细节上会有不同，事实上，我更愿意把检索当成一种精细分类，即得到图片的大致分类后，再在这个分类中找出最相似的图片。 「Bag of Feature」在检索中的算法流程和分类几乎完全一样，唯一的区别在于，对于原始的 BOF 特征，也就是直方图向量，我们引入 TF-IDF 权值。 TF-IDF 对 TF-IDF 的了解，我参考了吴军的《数学之美》一书。下面的解释，基本也是 copy 了书上的内容。 TF-IDF 最早是在文献检索领域中被提出的，下面我们就用一个文本检索的例子来了解 TF-IDF。 假设我们要检索关于「原子能的应用」的文章，最简单的做法就是将查询分解为「原子能」、「的」、「应用」，然后统计每篇文章中这三个词出现的频率。比如，如果一篇文章的总词数是 1000 ，其中「原子能」、「的」、「应用」分别出现了 2 次、35 次和 5 次，那么它们的词频就分别是 0.002、0.035、0.005。将这三个数相加，总和 0.042 就是该文章关于「原子能的应用」的「词频」。一般来说，词频越高，文章的相关性就越强。TF-IDF 中的 TF 也就是词频（Term Frequency）的意思。 但这种方法有一个明显的漏洞，就是一些跟主题不相关的词可能占据较大的比重。比如上面例子中的「的」一词，占据了总词频的 80% 以上，而这个词对主题的检索几乎没有作用。这种词我们称为「停止词（Stop Word）」，表明在度量相关性时不考虑它们的频率。忽略「的」之后，我们的词频变为 0.007，其中「原子能」贡献了 0.002，「应用」贡献了 0.007。 除此以外，这个优化后的结果还存在另一点不足。在汉语中，「应用」是个很通用的词，「原子能」是专业性很强的词，而后者对主题的检索比前者作用更大。 综合以上两点不足，我们需要对每一个词给一个权重。而且这个权重必须满足以下两个条件： 一个词对主题预测能力越强，权重越大； 停止词权重为 0； 观察一下我们就会发现，如果一个关键词只在很少的文章中出现，通过它就容易锁定搜索目标，它的权重就应该更大。反之，如果一个词在大量文章中频繁出现，看到它仍然不清楚要找什么内容，它的权重就应该小。 概括地讲，假定一个关键词 \\(w\\) 在 \\(D_w\\) 篇文章中出现过，那么 \\(D_w\\) 越大，\\(w\\) 的权重越小，反之亦然。在信息检索中，使用最多的权重是「逆文本频率指数」，也就是 TF-IDF 中的 IDF（Inverse Document Frequency）。它的公式为 \\(log(\\frac{D}{D_w})\\)，其中，\\(D\\) 是全部文章数。假定文章总数是 \\(D=10\\) 亿，停止词「的」在所有网页中都出现过，即 \\(D_w=10\\)亿，那么 它的 IDF = log(10亿 / 10亿) = log(1) = 0。假如「原子能」在 200万 篇文章中出现过，即 \\(D_w=200\\)万，那么它的 IDF = log(500) = 8.96。又假定通用词「应用」出现在 5亿 篇文章中，它的权重 IDF = log(2) = 1。 利用 IDF，我们得到一个更加合理的相关性计算公式： \\[ TF_1 * IDF_1+TF_2 * IDF_2+...+TF_N * IDF_N \\] 加权 BOF TF-IDF 是通过增加权重的方法，凸显出重要的关键信息。同样的，在图像检索中，为了更精确地度量相似性，我们也在原来直方图向量的基础上，为向量的每一项增加权重。 具体的，按照上面信息检索的方法，我们需要给字典里的每个向量（visual word）设置权重。权重的计算方法如出一辙：IDF = \\(log(\\frac{N}{f_j})\\)，其中，\\(N\\) 是图片总数，\\(f_j\\) 表示字典向量 j 在 多少张图片上出现过。仿照上面的例子，我们可以这样理解：假设我们要检索汽车图片，而汽车一般是放在地面上的，也就是说，在众多类似图片中，地面对应的 visual word 应该会经常出现，而这种特征对于我们检索汽车而言是没有帮助的，所以，用 IDF 公式，我们可以把这个权重减小到忽略不计的地步，这样就把汽车本身的特征凸显出来。 假设我们按照前面 BOF 算法的过程已经得到一张图片的直方图向量 \\(\\mathbf h = {h_j} (j = 0, 1, …, k)\\)，那么，加权 BOF 的计算公式为：\\(h_j = (h_j / \\sum_{i}{h_i}) log(\\frac{N}{f_j})\\)。公式右边后一部分就是上面所讲到的 IDF，而 \\((h_j / \\sum_{i}{h_i})\\) 就是词频 TF。 相似性度量方法 前面对 TF-IDF 的介绍，我们得到一个相对完善的向量表示方法。最后，再简单提一下如何根据这个向量确定图片之间的相似度。 关于向量相似度测量的方法有很多，最常见的是计算向量之间的欧几里得距离或者曼哈顿距离等。但在图像检索中，我们采用向量之间的夹角作为相似性度量方法。因为我们得到的向量是各个 visual word 综合作用的结果，对于同一类图片，它们可能受几个相同的 visual word 的影响较大，这样它们的特征向量大体上都会指向一个方向。而夹角越小的，证明向量之间应该越相似。 计算向量夹角的方法非常简单，可以直接采用余弦定理： \\[ s(\\mathbf h, \\overline{ \\mathbf h}) =\\frac{ &lt;\\mathbf h, \\overline{\\mathbf h}&gt; }{ ||\\mathbf h||\\ ||\\overline{\\mathbf h}||} \\] 等式右边，分子表示向量内积，分母是向量模的乘积。 由于向量中的每一个变量都是正数，因此余弦值的取值在 0 和 1 之间。如果余弦值为 0，证明向量夹角为 90 度，则这两个向量的相关性很低，图片基本不相似。如果余弦值靠近 1，证明两个向量的夹角靠近 0 度，则两个向量的相关性很高，图片很相似。 更多关于向量相似性度量的方法，请参考其他文章。吴军老师的《数学之美》中也有更加详细的介绍。 Bag of Feature 的缺点 Bag of Feature 在提取特征时不需要相关的 label 进行学习，因此是一种弱监督的学习方法。当然，没有什么方法会是十全十美的，Bag of Feature 也存在一个明显的不足，那就是它完全没有考虑到特征之间的位置关系，而位置信息对于人理解图片来说，作用是很明显的。有不少学者也提出了针对该缺点的改进，关于改进的方法，这里就不再介绍了。 参考 Bag-of-words model in computer vision Bag of Features (BOF)图像检索算法 数学之美","raw":null,"content":null,"categories":[{"name":"计算机视觉","slug":"计算机视觉","permalink":"https://jermmy.github.io/categories/计算机视觉/"}],"tags":[{"name":"计算机视觉","slug":"计算机视觉","permalink":"https://jermmy.github.io/tags/计算机视觉/"},{"name":"机器学习","slug":"机器学习","permalink":"https://jermmy.github.io/tags/机器学习/"}]},{"title":"cmake学习笔记(3)——编译和引用链接库","slug":"2017-4-27-learn-cmake-3","date":"2017-04-26T07:44:39.000Z","updated":"2017-06-04T17:04:19.000Z","comments":true,"path":"2017/04/26/2017-4-27-learn-cmake-3/","link":"","permalink":"https://jermmy.github.io/2017/04/26/2017-4-27-learn-cmake-3/","excerpt":"学习目标\n上一篇文章我们介绍了如何用 cmake 去 build 一个小型的工程项目。在实际开发中，我们有时候只是想编译生成一些链接库，而不是编译一个完整的项目。今天，我们来学习如何用 cmake 构建链接库。","text":"学习目标 上一篇文章我们介绍了如何用 cmake 去 build 一个小型的工程项目。在实际开发中，我们有时候只是想编译生成一些链接库，而不是编译一个完整的项目。今天，我们来学习如何用 cmake 构建链接库。 编译动态链接库(.so/dylib/dll) 接下来我们沿用上一篇文章的例子，编写一个 HelloWorld 类，并将这个类编译成动态链接库。 项目目录如下： 12345678910xyzdeMacBook-Pro:my_cmake_code xyz$ tree.├── CMakeLists.txt├── build├── include│ └── helloworld.h└── src └── helloworld.cpp3 directories, 3 files helloworld.* 文件的代码和之前的教程一样。 下面，我们还是先编写 CMakeList.txt 文件： 12345678910111213141516cmake_minimum_required(VERSION 2.8.9)project (hello)set(CMAKE_BUILD_TYPE Release)# Bring the headers, such as helloworld.h into the projectinclude_directories(include)# However, the file(GLOB...) allows for wildcard additions:file(GLOB SOURCES \"src/*.cpp\")# Generate the shared library from the sourcesadd_library(hello SHARED $&#123;SOURCES&#125;)# Set the location for library installation -- i.e., /usr/local/lib# not really necessary in this example. Use \"sudo make install\" to applyinstall(TARGETS hello DESTINATION /usr/local/lib) 下面介绍几条新命令的含义： set(CMAKE_BUILD_TYPE Release) 表示此次编译为正式版本的编译，一般来说照抄即可； add_library(hello SHARED ${SOURCES}) 这条命令是本文的重点。它表示 ${SOURCES} 变量中包含的源代码会被编译成动态链接库（另外两个选项是 STATIC 和 MODULE ）。这个链接库的名称是 hello； install(TARGETS hello DESTINATION /usr/local/lib) 这条命令表示会将动态链接库 hello 安装到 /usr/local/lib 这个目录下。我们需要使用 sudo make install 来激发这个 TARGET。 之后，我们按照之前的方式执行 cmake 和 make： 123cd buildcmake ..make make 完成后，你会在 build 文件夹下看到一个 libhello.dylib 文件（不同系统命名可能不同，Linux 下的后缀名是.so，Windows 下是.dll ）。 而如果要把链接库安装到系统中，还需要执行一步 sudo make install。你会在 shell 里面看到安装位置： 12345xyzdeMacBook-Pro:build xyz$ make install[100%] Built target helloInstall the project...-- Install configuration: \"\"-- Installing: /usr/local/lib/libhello.dylib 如果还不放心，可以亲自到 /usr/local/lib 目录下查看是否有 libhello 这个链接库。 在类 Unix 系统中，只要链接库在 /usr/local/lib 或 /usr/lib 目录下，我们就可以在链接时用 -l 来链接这些库了。 当然，上面的程序很简单，没有用到一些依赖库。如果动态库本身需要依赖其他外部库，那我们需要将这些依赖库的路径信息包含进去。 cmake 提供了一个 find_package() 指令来帮助搜寻这些依赖库。比如，如果我们需要引用到 OpenGL，可以加入以下几条命令： 12find_package(OpenGL REQUIRED)include_directories($&#123;OPENGL_INCLUDE_DIRS&#125;) 然后在 add_library() 之后链接库文件： 1target_link_libraries(hello $&#123;OPENGL_LIBRARIES&#125;) 不过，find_package()并不保证一定能找到这个依赖库，这个时候就需要手动添加文件路径了，比如，如果需要用到 freeglut 作为依赖，我们可以设置 freeglut 的库文件位置（类 Unix 系统一般都在 /usr/local/include 和 /usr/local/lib下），其他操作和 find_package() 一样： 1234include_directories(/usr/local/include)link_directories(/usr/local/lib)...target_link_libraries(hello glut) 编译静态链接库(.a) 讲完如何构建动态链接库，下面依葫芦画瓢，看看如何构建静态链接库。 不同于动态链接库的是，静态链接库是在编译的时候被「链接」进编译器的。换句话说，静态链接库中包含所有需要用到的源代码（当然是被编译器处理过的源代码），所以它的体积比动态链接库要大许多。但也由于它包含的是高级的代码形式，所以跨平台性比动态链接库好，而且也没有运行时加载库文件的时间开销。不过，由于动态链接库中的代码冗余更少，而且可以在不编译整个工程的情况下动态更新代码，因此动态链接库的使用场景更广。 编译静态链接库的方式和动态链接库几乎一模一样，唯一的区别是把 add_library(hello SHARED ${SOURCES}) 中的 SHARED 改成 STATIC 。然后按照之前的步骤执行 cmake 和 make，就可以在 build 文件夹下看到 libhello.a 文件。 引用链接库 好了，前面虽然扯了这么多，但我们还没验证生成的库文件是否正确。所以，接下来，我们再用 cmake 来引用我们生成的链接库。 为了调用 HelloWorld 函数库，我们先创建一个 main 函数的代码文件（app.cpp）： 1234567#include \"helloworld.h\"int main(int argc, char const *argv[]) &#123; HelloWorld h; h.print(); return 0;&#125; 然后我们把 helloworld.cpp 文件删掉。这样，我们的项目就变成下面这个样子： 1234567891011xyzdeMacBook-Pro:my_cmake_code xyz$ tree.├── CMakeLists.txt├── build│ └── libhello.dylib├── include│ └── helloworld.h└── src └── app.cpp3 directories, 4 files 现在，helloworld.h 文件的实现就以 libhello.dylib 的形式存在了。之后我们要重新编写 CMakeList.txt 文件，让 app.cpp 可以引用到 libhello.dylib 链接库。 123456789101112131415cmake_minimum_required(VERSION 2.8.9)project (hello)# For the shared library:set(PROJECT_LINK_LIBS libhello.dylib)link_directories(build)# For the static library:#set(PROJECT_LINK_LIBS libhello.a)#link_directories(build)include_directories(include)file(GLOB SOURCES src/*cpp)add_executable(hello $&#123;SOURCES&#125;)target_link_libraries(hello $&#123;PROJECT_LINK_LIBS&#125;) 同样的，我们要分析一下上面的命令是如何链接到 libhello.dylib 的（注释中提供了静态链接库的链接代码，和动态链接库的几乎一样，不再赘述）： set(PROJECT_LINK_LIBS libhello.dylib) 这条命令定义了一个 PROJECT_LINK_LIBS 变量，该变量表示我们要链接的库为 libhello.dylib； link_directories(build) 这条命令表示我们的链接库在 build 文件夹下； target_link_libraries(hello ${PROJECT_LINK_LIBS}) 这条命令会将我们的程序和库函数链接起来。 然后，我们执行 cmake 和 make 程序： 123cd buildcmake ..make 前面的步骤指令都正确的话，我们会在 build 文件夹下看到一个 hello 程序。执行这个程序，如果输出如下结果，证明链接成功了： 12xyzdeMacBook-Pro:build xyz$ ./helloHello World! 总结 这一篇教程我们主要介绍了四条新命令： 12345add_library(hello SHARED(STATIC) $&#123;SOURCES&#125;)install(TARGETS hello DESTINATION /usr/local/lib)link_directories(build)target_link_libraries(hello $&#123;PROJECT_LINK_LIBS&#125;) 前两条用于生成和安装动/静态链接库，后两条用于链接这些库文件。 参考 Introduction to CMake by Example","raw":null,"content":null,"categories":[{"name":"工具","slug":"工具","permalink":"https://jermmy.github.io/categories/工具/"}],"tags":[{"name":"工具","slug":"工具","permalink":"https://jermmy.github.io/tags/工具/"},{"name":"cmake","slug":"cmake","permalink":"https://jermmy.github.io/tags/cmake/"}]},{"title":"cmake学习笔记(2)——引用文件夹","slug":"2017-4-26-learn-cmake-2","date":"2017-04-26T02:27:00.000Z","updated":"2017-04-29T06:42:36.000Z","comments":true,"path":"2017/04/26/2017-4-26-learn-cmake-2/","link":"","permalink":"https://jermmy.github.io/2017/04/26/2017-4-26-learn-cmake-2/","excerpt":"学习目标\n之前的文章中，我们介绍了 cmake 的 HelloWorld 程序，并了解了执行 cmake 的一般套路。今天，我们稍微深入一下，学习如何引用多个文件夹下的代码。\n引用文件夹\n随着功能的增加，我们的项目也变得越来越大了。这个时候，为了更好地管理代码，我们将工程拆分成各个模块，每个模块的代码文件单独放在一个文件夹下以便管理。在这种情况下，cmake 将带来更多的便利。","text":"学习目标 之前的文章中，我们介绍了 cmake 的 HelloWorld 程序，并了解了执行 cmake 的一般套路。今天，我们稍微深入一下，学习如何引用多个文件夹下的代码。 引用文件夹 随着功能的增加，我们的项目也变得越来越大了。这个时候，为了更好地管理代码，我们将工程拆分成各个模块，每个模块的代码文件单独放在一个文件夹下以便管理。在这种情况下，cmake 将带来更多的便利。 现在，我们将 HelloWorld 单独封装成一个类，并把头文件放到 include 文件夹，源代码放在 src 文件夹，同时将 main 函数单独放在 app.cpp 文件中作为应用程序的入口。工程目录如下： 1234567891011xyzdeMacBook-Pro:my_cmake_code xyz$ tree.├── CMakeLists.txt├── build├── include│ └── helloworld.h└── src ├── app.cpp └── helloworld.cpp3 directories, 4 files 注意，我在项目根目录下新建了一个 build 文件夹。这个文件夹用来存放 cmake 产生的临时文件以及最终的可执行程序。 下面，我们看看 CMakeList.txt 怎么写： 12345678910111213141516cmake_minimum_required(VERSION 2.8.9)project (hello)# Bring the headers, such as helloworld.h into the projectinclude_directories(include)# Can manually add the sources using the set command as follows:# set(SOURCES src/app.cpp src/helloworld.cpp)# Add all the file in directory# aux_source_directory(src SOURCES)# However, the file(GLOB...) allows for wildcard additions:file(GLOB SOURCES \"src/*.cpp\")add_executable(hello $&#123;SOURCES&#125;) 我们分析一下几条重要的命令： include_directories(include) 这条命令将 include 文件夹下的头文件添加到编译环境中； set(SOURCES src/app.cpp src/helloworld.cpp) 这条命令的作用跟接下来两条命令相同，在文件中被我注释掉了，但在 cmake 中是很常用的命令。set 函数用于设置变量，在本例中，我们定义一个 SOURCES 变量（变量名可以随便起），并将 src/app.cpp、 src/helloworld.cpp 两个文件路径添加到 SOURCES 变量中； aux_source_directory(src SOURCE) 这条命令会将 src 文件夹下的所有文件都放到 SOURCES 变量中，在本例中，它的作用和下一条 file 一致； file(GLOB SOURCES &quot;src/*.cpp&quot;) 这条命令会将后面匹配到的所有文件 src/*.cpp 交给 GLOB 子命令，由后者生成一个文件列表，并将列表赋给 SOURCES 变量。由于这条命令可以帮助我们自动引用所有的源文件，而 set 命令需要我们一个一个地添加文件，所以这里使用 file 更加省事； add_executable(hello ${SOURCES}) 这条命令在之前的教程中已经介绍过了，不过这一次我们传入一个 SOURCES 变量，这个变量包含所有源文件的路径。 写完 CMakeList.txt 后，我们同样要用 cmake 来执行。如果你执行过上一篇文章的 cmake 程序，你会发现 cmake 输出了很多临时文件。所以，为了方便管理，我们这次要将所有 cmake 临时文件输出到 build 文件夹。具体做法如下： 123cd buildcmake ..make 首先我们先进入 build 文件夹，然后执行 cmake 程序，不过，由于 CMakeList.txt 文件在 build 目录上一层，所以需要执行 cmake ..，之后，cmake 会在当前文件夹下生成临时文件以及 Makefile，然后我们 make 一下就可以了，你会看到 build 目录下生成的可执行文件。 如果我们想重新执行 cmake，只需要删掉 build 下的所有文件，然后再重复之前的操作即可。 总结 这一篇教程主要介绍了三个新命令： 1234567891011# Bring the headers, such as helloworld.h into the projectinclude_directories(include)# Can manually add the sources using the set command as follows:set(SOURCES src/app.cpp src/helloworld.cpp)# Add all the file in directoryaux_source_directory(src SOURCES)# However, the file(GLOB...) allows for wildcard additions:file(GLOB SOURCES \"src/*.cpp\") 第一条 include_directories() 命令用于引用头文件的目录，我们可以添加多个参数，中间用空格隔开； 第二条 set() 在 cmake 中很常见，通常用于定义和设置变量。； 第三条 aux_source_directory() 可以添加指定文件夹下所有的文件； 第四条 file() 可以批量引用代码文件，并通过子命令 GLOB 转成文件列表存到变量中。 参考 Introduction to CMake by Example","raw":null,"content":null,"categories":[{"name":"工具","slug":"工具","permalink":"https://jermmy.github.io/categories/工具/"}],"tags":[{"name":"工具","slug":"工具","permalink":"https://jermmy.github.io/tags/工具/"},{"name":"cmake","slug":"cmake","permalink":"https://jermmy.github.io/tags/cmake/"}]},{"title":"cmake学习笔记(1)——HelloWorld","slug":"2017-4-25-learn-cmake-1","date":"2017-04-25T14:26:01.000Z","updated":"2017-04-29T06:42:29.000Z","comments":true,"path":"2017/04/25/2017-4-25-learn-cmake-1/","link":"","permalink":"https://jermmy.github.io/2017/04/25/2017-4-25-learn-cmake-1/","excerpt":"什么是 cmake\ncmake 是一个跨平台的自动化构建系统，它可以产生不同系统平台的构建文件（例如：类 Unix 系统的 Makefile，Windows 系统的 .vcproj ）。开发者只要根据 cmake 的命令规则编写好 CMakeLists.txt 文件，就可以用对应平台的 cmake 程序生成相应的构建文件，再根据构建文件编译代码。它的好处是，开发者只要学会 cmake 自身的语法规则即可，至于平台本身的项目构建文件，则交由 cmake 开发人员处理。总而言之，cmake 是高级版的 Makefile，是优秀的 C/C++ 程序员必不可少的技能（当然后面这句是我自己说的）。\n下面，我们就一步一步地来学习 cmake 吧🤓。本系列教程基于 cmake 3.8 版本。\n（注：本教程大部分内容取自 Introduction to CMake by Example ）","text":"什么是 cmake cmake 是一个跨平台的自动化构建系统，它可以产生不同系统平台的构建文件（例如：类 Unix 系统的 Makefile，Windows 系统的 .vcproj ）。开发者只要根据 cmake 的命令规则编写好 CMakeLists.txt 文件，就可以用对应平台的 cmake 程序生成相应的构建文件，再根据构建文件编译代码。它的好处是，开发者只要学会 cmake 自身的语法规则即可，至于平台本身的项目构建文件，则交由 cmake 开发人员处理。总而言之，cmake 是高级版的 Makefile，是优秀的 C/C++ 程序员必不可少的技能（当然后面这句是我自己说的）。 下面，我们就一步一步地来学习 cmake 吧🤓。本系列教程基于 cmake 3.8 版本。 （注：本教程大部分内容取自 Introduction to CMake by Example ） HelloWorld 按照程序员的规矩，我们要先讲讲 cmake 的 HelloWorld 程序。 首先，我们先创建一个 helloworld.cpp 文件，并编写你最熟悉的 helloworld 代码： 123456#include &lt;iostream&gt; int main(int argc, char *argv[]) &#123; std::cout &lt;&lt; \"Hello World!\" &lt;&lt; std::endl; return 0;&#125; 之后，在同一个目录下创建一个 CMakeLists.txt 文件。CMakeLists.txt 是默认存放 cmake 命令的文件，如同 make 的 Makefile。我们在这个文件上输入以下命令： 1234cmake_minimum_required(VERSION 2.8.9)project (hello)# add source file to compileadd_executable(hello helloworld.cpp) 作为第一个 cmake 程序，我们要分析一下上面三条简单的命令： 第一条命令 cmake_minimum_required() 指明 cmake 的最低版本，这个命令的功能不言而喻，一般来说照抄就可以了； 第二条命令 project() 设置了项目的名字； 第三条命令比较重要。add_executable() 命令的第一个参数表示我们最终编译出来的程序的名称，第二个参数是编译所需要的代码文件。注意，参数之间有空格隔开。 由于 CMakeList.txt 也属于脚本文件，所以打注释的方式和 Makefile 一样，在命令前加上「#」。 下面，我们用 cmake 来编译这个 HelloWorld 程序。 在 CMakeLists.txt 同一个目录下，执行以下命令（不要忘了点）： 1cmake . cmake 会执行当前目录的 CMakeLists.txt ，生成需要的构建文件。 你会看到类似输出： 123456789-- The C compiler identification is AppleClang 8.0.0.8000042-- The CXX compiler identification is AppleClang 8.0.0.8000042-- Check for working C compiler: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/cc-- Check for working C compiler: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/cc -- works-- Detecting C compiler ABI info-- Detecting C compiler ABI info - done-- Detecting C compile features-- Detecting C compile features - done...... 不同的系统输出会有不同。cmake 在执行过程中会自动寻找系统的 C/C++ 编译器，将它们和其他配置信息一起写入 Makefile 文件。 之后，你会在同一个目录下看到一个 Makefile 文件，这个文件是 cmake 自动生成的，不要去修改它。然后，我们就可以用 make 来生成可执行程序了。 1make 然后就是一般的 make 执行过程，最终我们会在同一个目录下看到 hello 程序。 总结 第一个教程虽然简单，但它已经包含了 cmake 的基本使用流程。首先，我们创建一个 CMakeList.txt 文件，在上面输入 cmake 命令，再让 cmake 来执行这个文件，得到特定平台的 Makefile 文件，最后再 make 一下便可以生成可执行程序了。 之后的教程会继续介绍其他高级的 cmake 指令。 参考 cmake wikipedia Introduction to CMake by Example cmake tutorial","raw":null,"content":null,"categories":[{"name":"工具","slug":"工具","permalink":"https://jermmy.github.io/categories/工具/"}],"tags":[{"name":"工具","slug":"工具","permalink":"https://jermmy.github.io/tags/工具/"},{"name":"cmake","slug":"cmake","permalink":"https://jermmy.github.io/tags/cmake/"}]},{"title":"TensorFlow学习笔记：保存和读取模型","slug":"2017-4-23-learn-tensorflow-save-restore-model","date":"2017-04-22T16:00:00.000Z","updated":"2018-03-20T10:25:51.000Z","comments":true,"path":"2017/04/23/2017-4-23-learn-tensorflow-save-restore-model/","link":"","permalink":"https://jermmy.github.io/2017/04/23/2017-4-23-learn-tensorflow-save-restore-model/","excerpt":"TensorFlow 更新频率实在太快，从 1.0 版本正式发布后，很多 API 接口就发生了改变。今天用 TF 训练了一个 CNN 模型，结果在保存模型的时候居然遇到各种问题。Google 搜出来的答案也是莫衷一是，有些回答对 1.0 版本的已经不适用了。后来实在没办法，就翻了墙去官网看了下，结果分分钟就搞定了～囧～。\n这篇文章内容不多，主要讲讲 TF v1.0 版本中保存和读取模型的最简单用法，其实就是对官网教程的简要翻译摘抄。","text":"TensorFlow 更新频率实在太快，从 1.0 版本正式发布后，很多 API 接口就发生了改变。今天用 TF 训练了一个 CNN 模型，结果在保存模型的时候居然遇到各种问题。Google 搜出来的答案也是莫衷一是，有些回答对 1.0 版本的已经不适用了。后来实在没办法，就翻了墙去官网看了下，结果分分钟就搞定了～囧～。 这篇文章内容不多，主要讲讲 TF v1.0 版本中保存和读取模型的最简单用法，其实就是对官网教程的简要翻译摘抄。 保存和恢复 在 TensorFlow 中，保存和恢复模型最简单的方法就是使用 tf.train.Saver 类。这个类会将变量的保存和恢复操作添加到 TF 的图（graph）中。 Checkpoint 文件 TF 将变量保存在二进制文件中，这个文件包含一个从变量名到 tensor 值的映射。当我们创建一个 Saver 对象的时候，我们可以指定 checkpoint 文件中的变量名。默认会使用变量的 Variable.name 属性。 这一段读起来比较生涩难懂，具体看下面的例子。 保存变量 可以通过创建 Saver 来管理模型内的所有变量。 12345678910111213141516171819# Create some variables.v1 = tf.Variable(..., name=\"v1\")v2 = tf.Variable(..., name=\"v2\")...# Add an op to initialize the variables.init_op = tf.global_variables_initializer()# Add ops to save and restore all the variables.saver = tf.train.Saver()# Later, launch the model, initialize the variables, do some work, save the# variables to disk.with tf.Session() as sess: sess.run(init_op) # Do some work with the model. .. # Save the variables to disk. save_path = saver.save(sess, \"/tmp/model.ckpt\") print(\"Model saved in file: %s\" % save_path) 恢复变量 可以通过同一个 Saver 对象（指定相同的保存路径）来恢复变量。这种情况下，我们不需要事先初始化变量（即无需调用 tf.global_variables_initializer()） 123456789101112131415# Create some variables.v1 = tf.Variable(..., name=\"v1\")v2 = tf.Variable(..., name=\"v2\")...# Add ops to save and restore all the variables.saver = tf.train.Saver()# Later, launch the model, use the saver to restore variables from disk, and# do some work with the model.with tf.Session() as sess: # Restore variables from disk. saver.restore(sess, \"/tmp/model.ckpt\") print(\"Model restored.\") # Do some work with the model ... 例子 下面用我自己的例子解释一下。 首先，我们先定义一个图模型（只截选出变量部分）： 1234567891011121314151617181920212223242526272829303132graph = tf.Graph()with graph.as_default(): # Input data # ....省略代码若干 # Variables layer1_weights = tf.Variable(tf.truncated_normal( [patch_size, patch_size, image_channels, depth], stddev=0.1), name=\"layer1_weights\") layer1_biases = tf.Variable(tf.zeros([depth]), name=\"layer1_biases\") layer2_weights = tf.Variable(tf.truncated_normal( [image_size // 4 * image_size // 4 * depth, num_hidden], stddev=0.1, name=\"layer2_weights\") ) layer2_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]), name=\"layer2_biases\") layer3_weights = tf.Variable(tf.truncated_normal( [num_hidden, num_labels], stddev=0.1, name=\"layer3_weights\"), ) layer3_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]), name=\"layer3_biases\") def model(data): # ....省略代码若干 return tf.matmul(fc1, layer3_weights) + layer3_biases # Training computation # ....省略代码若干 # Optimizer optimizer = tf.train.GradientDescentOptimizer(0.05).minimize(loss) 这个模型里的变量其实只有三个网络层的参数：layer1_weights，layer1_biases，layer2_weights，layer2_biases，layer3_weights，layer3_biases。 然后就是启动会话进行训练： 12345678910111213141516with tf.Session(graph=graph) as session: saver = tf.train.Saver() if loading_model: saver.restore(session, model_folder + \"/\" + model_file) print(\"Model restored\") else: tf.global_variables_initializer().run() print(\"Initialized\") for step in range(num_steps): # ....省略训练模型的代码 print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels)) save_path = saver.save(session, model_folder + \"/\" + model_file) print(\"Model saved in file: \", save_path) 这段代码是本文的关键，我们先通过 tf.train.Saver() 构造一个 Saver 对象。注意，这一步执行前要保证我们已经定义好了变量（例如：例子中的用 tf.Variable 定义的 layer1_weights 等），否则会抛异常 ValueError(&quot;No variables to save&quot;)。 通过 Saver，我们可以在模型训练完之后，将参数保存下来。Saver 保存数据的方法十分简单，只要将 session 和文件路径传入 save 函数即可：saver.save(session, model_folder + &quot;/&quot; + model_file)。 如果我们一开始想载入本地的模型文件，而不是让 TF 自动初始化训练，则可以通过 Saver 的 restore 函数读取模型文件，文件路径需要和之前保存的文件路径一致。注意，如果是通过这种方式初始化变量，则不能再调用 tf.global_variables_initializer() 函数。之后，训练或预测的代码不需要改变，TensorFlow 会自动根据模型文件，将你的模型参数初始化。 当然啦，以上都是最基础的用法，只是简单地将所有参数保存下来。更高级的用法，之后如果使用到再继续总结。 参考 TensorFlow官方教程","raw":null,"content":null,"categories":[{"name":"TensorFlow","slug":"TensorFlow","permalink":"https://jermmy.github.io/categories/TensorFlow/"}],"tags":[{"name":"深度学习","slug":"深度学习","permalink":"https://jermmy.github.io/tags/深度学习/"},{"name":"TensorFlow","slug":"TensorFlow","permalink":"https://jermmy.github.io/tags/TensorFlow/"}]},{"title":"翻墙神器SwitchHosts","slug":"2017-4-22-tool-switch-hosts","date":"2017-04-22T03:18:55.000Z","updated":"2017-04-22T06:27:21.000Z","comments":true,"path":"2017/04/22/2017-4-22-tool-switch-hosts/","link":"","permalink":"https://jermmy.github.io/2017/04/22/2017-4-22-tool-switch-hosts/","excerpt":"今天要介绍一个翻墙工具 SwitchHosts。\n说真的，这个工具我已经听说很久了，但由于我一直都用着 Shadowsocks 纸飞机翻墙，就没怎么理睬它。直到今天，不得不去 TensorFlow 官网查点资料，却发现纸飞机也飞不到那里去，因此尝试了下 SwitchHosts，结果真是太感人了。\nSwitchHosts\nSwitchHosts 的原理其实很简单，就是通过修改 host 文件，指定 IP 地址。只要 GFW 没有对 IP 进行封锁，那么我们就可以直接通过 IP 访问服务器，避开敏感词检测。\nSwitchHosts 提供多种平台的版本，详情请移步官网。","text":"今天要介绍一个翻墙工具 SwitchHosts。 说真的，这个工具我已经听说很久了，但由于我一直都用着 Shadowsocks 纸飞机翻墙，就没怎么理睬它。直到今天，不得不去 TensorFlow 官网查点资料，却发现纸飞机也飞不到那里去，因此尝试了下 SwitchHosts，结果真是太感人了。 SwitchHosts SwitchHosts 的原理其实很简单，就是通过修改 host 文件，指定 IP 地址。只要 GFW 没有对 IP 进行封锁，那么我们就可以直接通过 IP 访问服务器，避开敏感词检测。 SwitchHosts 提供多种平台的版本，详情请移步官网。 下面上个结果图，以示喜悦之情。 switchhosts tensorflow 有人可能会说，既然 SwitchHosts，那还要 Shadowsocks 等付费产品干嘛。其实这是两种不同的翻墙工具。SwitchHosts 属于「穿墙」的思路，而 Shadowsocks 则属于借助代理的「翻墙」思路。前者虽然速度更快，但毕竟你没法掌握世界上所有站点的 IP，因此更适合特殊网站的翻墙，比如 TensorFlow 这样的。而且，由于 GFW 会对某些 IP 进行封锁（e.g. Google, Facebook），并不是每个网站都能通过修改 host 穿出去。Shadowsocks 是通过加密混淆来防止被 GFW 识别，虽然速度上偏慢，但对大多数网站适用。因此，两者相互结合，才是翻墙的最佳姿势。 参考 SwitchHosts官网 目前热门科学上网方式介绍及优缺点简评","raw":null,"content":null,"categories":[{"name":"工具","slug":"工具","permalink":"https://jermmy.github.io/categories/工具/"}],"tags":[{"name":"工具","slug":"工具","permalink":"https://jermmy.github.io/tags/工具/"},{"name":"TensorFlow","slug":"TensorFlow","permalink":"https://jermmy.github.io/tags/TensorFlow/"}]},{"title":"如何「优雅」地标数据","slug":"2017-4-19-how-to-label-images","date":"2017-04-19T12:56:02.000Z","updated":"2017-04-23T05:36:40.000Z","comments":true,"path":"2017/04/19/2017-4-19-how-to-label-images/","link":"","permalink":"https://jermmy.github.io/2017/04/19/2017-4-19-how-to-label-images/","excerpt":"最近想做一个识别验证码的程序。目标其实很简单，就是识别出某网站验证码的字母和数字。\n\n验证码\n\n这种类型的验证码已经被做烂了，相应的破解程序也很多。但我只是想学习消遣一下。\n我已经通过爬虫收集了某网站的大量验证码图片，并通过图像处理的方法把字母和数字分割出来（好在这类验证码比较简单，切割工作相对容易）。之后，便是要对这些图片进行标记并训练。我总共爬了 20000 张，每张上面有四个数字或字母，相当于要对 80000 张图片做标记分类。嗯，这很有趣！","text":"最近想做一个识别验证码的程序。目标其实很简单，就是识别出某网站验证码的字母和数字。 验证码 这种类型的验证码已经被做烂了，相应的破解程序也很多。但我只是想学习消遣一下。 我已经通过爬虫收集了某网站的大量验证码图片，并通过图像处理的方法把字母和数字分割出来（好在这类验证码比较简单，切割工作相对容易）。之后，便是要对这些图片进行标记并训练。我总共爬了 20000 张，每张上面有四个数字或字母，相当于要对 80000 张图片做标记分类。嗯，这很有趣！ 需求分析 通过对原图进行处理分割后，我已经得到如下的图片数据（图片尺寸 32 * 32，除了灰度图，最好保留对应的原图）： image set 现在，要将这些图片分门别类。数字和字母，最多可以组合出 10 + 26 = 36 类，但仔细观察数据后，我发现有很多数字和字母压根没出现。通过粗略地扫描一下数据，我统计出这个网站的验证码总共只使用了 23 类数字和字母。于是，我按照如下规则对图片做了分类： 1image_tag = &#123;0: '3', 1: '5', 2: '6', 3: '7', 4: '8', 5: 'a', 6: 'c', 7: 'e', 8: 'f', 9: 'g', 10: 'h', 11: 'j', 12: 'k', 13: 'm', 14: 'n', 15: 'p', 16: 'r', 17: 's', 18: 't', 19: 'v', 20: 'w', 21: 'x', 22: 'y'&#125; 将出现的数字和字母分为 23 类。然后，接下来的目标，就是把图片分到如下 23 个文件夹中： tag folder 实现思路 很多人都觉得标数据这种事情很没技术含量，纯属「dirty work」。如果你只是单纯地用肉眼把一张张图片分到这些目录里面，当然显得很「笨拙」。而且，仔细想想，80000 张图片的分类，（一个人）几乎是不可能人工完成的。我们要用优雅的方法来归类。 这个优雅的方法其实也很简单。分以下几步进行： 先人工挑出几个或十几个样本，训练一个分类器出来，这个分类器准确率会很低，但不要紧； 再从原图片中，选出几十上百张，用刚才的分类器对它们进行分类。由于分类器精度有限，需要从分类后的结果中挑出分错的样本，然后人工将它们分到正确的目录（这个工作比你自己去对上百张图片做分类真的要轻松好多）； 用已经分好类的数据继续训练一个新的分类器，重复第 2 步直到数据都分类完（随着分类器精度提高，可以逐步增加待分类图片的数量）； 这个方法虽然还是需要不少人工辅助，但总体来说，比人工手动分类的效率实在高太多了。 具体实现 人工选取小样本 要训练分类器，挑选样本是必须的，我从分割的图片中，随机挑出一两百张，将它们分类到相应的目录内： 屏幕快照 2017-04-19 下午10.29.55 然后，我需要一个函数来读取这些文件夹的数据，方便之后继续训练。 123456789101112131415161718192021222324252627282930'''读取图片数据文件，转换成numpy格式，并保存'''def maybe_pickle_data(all_image_folder, dest_folder, pickle_file, force=False): if os.path.exists(pickle_file) and force==False: print(\"data already pickled, pass\") return image_folders = os.listdir(all_image_folder) train_image_data = [] train_image_label = [] for folder in image_folders: image_folder = os.path.join(all_image_folder, folder) if os.path.isdir(image_folder): print(image_folder) train_image_data.append(load_letter(image_folder)) train_image_label.append(int(folder)) # merge all the train data to ndarray train_dataset, train_label = merge_datasets(train_image_data, train_image_label) # randomize dataset and label train_dataset, train_label = randomize(train_dataset, train_label) # write to file with open(pickle_file, 'wb') as f: save = &#123; 'train_dataset': train_dataset, 'train_labels': train_label, &#125; pickle.dump(save, f, pickle.HIGHEST_PROTOCOL) 这个函数的主要工作是循环每一个目录文件夹里的文件，将它们依次读入，变成矩阵形式方便处理，并通过 Pickle 保存成文件。 这里主要用了其他几个函数的功能： 1load_letter(image_folder) # 读取一个tag文件夹里的推按文件，并返回所有图片数据的矩阵 1merge_datasets(train_image_data, train_image_label) # 将所有类别的图片数据合并成一个大的矩阵样本数据 1randomize(train_dataset, train_label) # 打乱训练数据 &lt;br&gt; 下面放点关键函数的代码。 load_letter() 函数代码如下，对图片的读取用了 opencv： 12345678910111213141516171819'''读取同种类别的图片转换成numpy数组'''def load_letter(folder): image_files = os.listdir(folder) # image_size 为 32 dataset = np.ndarray(shape=(len(image_files), image_size, image_size), dtype=np.float32) num_images = 0 for image in image_files: image_file = os.path.join(folder, image) image_data = cv2.imread(image_file, cv2.IMREAD_GRAYSCALE) if image_data is None: continue if image_data.shape != (image_size, image_size): raise Exception(\"%s Unexpected image size: %s\" % image_file, str(image_data.shape)) dataset[num_images, :, :] = image_data num_images = num_images + 1 dataset = dataset[0:num_images, :, :] return dataset 代码比较简单，就不多解释了。 &lt;br&gt; merge_datasets() 函数代码： 1234567891011121314151617def merge_datasets(train_image_data, train_image_label): image_number = 0 for image_datas in train_image_data: image_number = image_number + len(image_datas) #print(image_number) train_dataset, train_labels = make_array(image_number, image_size) image_number = 0 # train_image_data 是所有图片矩阵的list，list每个元素对应每个tag图片的矩阵数据 for label, image_datas in enumerate(train_image_data): for image_data in image_datas: train_dataset[image_number, :, :] = image_data train_labels[image_number] = train_image_label[label] image_number = image_number + 1 #print(train_labels) return train_dataset, train_labels &lt;br&gt; 训练分类器 好了，准备好数据，我们需要训练一个分类器。简单起见，这里选择用 SVM，并选用 sklearn 函数库。 其实，可以直接把图片矩阵转换成一个向量进行训练（32 * 32 —&gt; 1 * 1024），但我们拥有的数据量太少，这样效果较差。所以，我们先提取图片的 HOG 特征再进行训练： 123456789101112bin_n = 16 # Number of binsdef hog(image): gx = cv2.Sobel(image, cv2.CV_32F, 1, 0) gy = cv2.Sobel(image, cv2.CV_32F, 0, 1) mag, ang = cv2.cartToPolar(gx, gy) bins = np.int32(bin_n*ang/(2*np.pi)) # quantizing binvalues in (0...16) bin_cells = bins[:16,:16], bins[16:,:16], bins[:16,16:], bins[16:,16:] mag_cells = mag[:16,:16], mag[16:,:16], mag[:16,16:], mag[16:,16:] hists = [np.bincount(b.ravel(), m.ravel(), bin_n) for b, m in zip(bin_cells, mag_cells)] hist = np.hstack(hists) # hist is a 64 bit vector return hist 这个函数代码摘自 opencv3 的文档，想了解代码，请自行去官网阅读文档。 有了特征之后，我们可以正式用 SVM 进行训练了： 123456789101112def train_svm(train_datasets, train_labels): x = np.ndarray(shape=(len(train_datasets), 64)) y = np.ndarray(shape=(len(train_datasets)), dtype=np.int32) for index, image in enumerate(train_datasets): hist = np.float32(hog(image)).reshape(-1, 64) x[index] = hist y[index] = train_labels[index] model = svm.LinearSVC(C=1.0, multi_class='ovr', max_iter=1000) model.fit(x, y) return model 这个函数代码一样很简单，如果看不懂，证明你需要熟悉 numpy 和 sklearn 函数库的用法。 然后，我们需要选取图片进行预测分类。可以人工挑出个几百上千张，放在一个预测目录内。同时再开一个目录文件夹如下： test folder 这个 test 文件夹和先前人工分类的文件夹要分开，因为之后还要人工对这里面的图片除杂。最后，我们遍历预测目录内的图片，用 SVM 做预测，并将图片放到预测结果对应的文件夹里。 测试函数代码如下： 1234567891011121314def test_image(image_folder, result_folder, model): image_files = os.listdir(image_folder) for image in image_files: image_file = os.path.join(image_folder, image) image_data = cv2.imread(image_file, cv2.IMREAD_GRAYSCALE) if image_data is None: continue hist = np.float32(hog(image_data)).reshape(-1, 64) pred = model.predict(hist) shutil.copy(image_file, os.path.join(result_folder+\"/\"+str(int(pred)), image)) &lt;b&gt; 做完这一步，我们最关键，同时也是最优雅的一步就完成了。之后，SVM 也帮不了你了。你需要依次打开每个文件夹，看看里面的图片有没有分错的，然后人工矫正它们，最后把它们归类到我们一开始挑选样本分好类的文件夹里，后者这个文件夹的数据表示已经分类好的。 如果运气好的，这个初步训练好的 SVM 已经稍微有点「聪明」了。看看我得到的分类结果： good result 这个准确率我已经很欣慰了，基本上人工挑出几张分错的，剩下的都是同一类了。 当然，肯定有分的不好的情况： bad result 对于这种，就是发挥你眼力的时候了。基本上，之后所有的工作都是在这一堆类似的图片里面找不同。当然，你要相信这种情况会越来越少，因为随着训练样本逐渐增多，SVM 的训练效果会越来越好。如果越到后面效果越差，程序员，请你不要怀疑，一定是你的代码出问题了。 &lt;br&gt; 接下来我给出整个程序的主体部分： 12345678910111213141516171819if __name__ == '__main__': maybe_create_directory_1(image_real_tag_folder) maybe_create_directory_1(image_test_folder) maybe_pickle_data(image_real_tag_folder, image_dataset_folder, image_dataset_folder + \"/data.pickle\", force=True) f = open(image_dataset_folder + \"/data.pickle\", 'rb') data = pickle.load(f) train_datasets = data['train_dataset'] train_labels = data['train_labels'] model = train_svm(train_datasets, train_labels) print(\"remove data in \" + image_src_folder) remove_files(image_src_folder) print(\"copy data to \" + image_src_folder + \"...\") copy_src_to_test(original_image_folder, image_src_folder) test_image(image_src_folder, image_test_folder, model) main 函数就是上面几个函数的结合。之后，我们就是不断地 run 一遍代码，人工除杂精分类，再 run 一遍代码，再人工……循环往复直到数据分类完为止。 总结 这个方法可以节省你大量的体力活动，有助于提高逼格。虽然如此，这 80000 个样本我还是生生花了一天半时间才分完，工作量还是稍微超出预期。如果有小伙伴有逼格更高，更能提高生产效率的方法，望不吝赐教！ 参考 opencv3 svm教程","raw":null,"content":null,"categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://jermmy.github.io/categories/机器学习/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"https://jermmy.github.io/tags/机器学习/"},{"name":"标数据","slug":"标数据","permalink":"https://jermmy.github.io/tags/标数据/"}]},{"title":"python numpy 三行代码打乱训练数据","slug":"2017-4-18-python-numpy-randomize","date":"2017-04-18T02:39:00.000Z","updated":"2017-04-20T07:04:42.000Z","comments":true,"path":"2017/04/18/2017-4-18-python-numpy-randomize/","link":"","permalink":"https://jermmy.github.io/2017/04/18/2017-4-18-python-numpy-randomize/","excerpt":"今天发现一个用 numpy 随机化数组的技巧。\n需求\n我有两个数组（ ndarray ）：train_datasets 和 train_labels。其中，train_datasets 的每一行和 train_labels 是一一对应的。现在我要将数组打乱并用于训练，打乱后要求两者的行与行之间必须保持原来的对应关系。","text":"今天发现一个用 numpy 随机化数组的技巧。 需求 我有两个数组（ ndarray ）：train_datasets 和 train_labels。其中，train_datasets 的每一行和 train_labels 是一一对应的。现在我要将数组打乱并用于训练，打乱后要求两者的行与行之间必须保持原来的对应关系。 实现 一般的实现思路，应该是先将 train_datasets（或 train_labels ）打乱，并记录被打乱的行号，再通过行号调整 train_labels （或 train_datasets ）的行次序，这样两者的对应关系能保持一致。但代码实现起来会很繁琐，而如果用上 numpy 的话，可以三行代码搞定。 首先，假设我们用如下训练数据（训练数据和标签都是三个）： 12345678910&gt;&gt;&gt; train_data = np.ndarray(shape=(3,1,2), dtype=np.int32, buffer=np.asarray((1,2,3,4,5,6), dtype=np.int32))&gt;&gt;&gt; train_label = np.ndarray(shape=(3,), dtype=np.int32, buffer=np.asarray((1,2,3), dtype=np.int32))&gt;&gt;&gt; train_dataarray([[[1, 2]], [[3, 4]], [[5, 6]]], dtype=int32)&gt;&gt;&gt; train_labelarray([1, 2, 3], dtype=int32) 下面，我们用三行代码打乱样本数据： 123&gt;&gt;&gt; permutation = np.random.permutation(train_label.shape[0])&gt;&gt;&gt; shuffled_dataset = train_data[permutation, :, :]&gt;&gt;&gt; shuffled_labels = train_label[permutation] 稍微解释一下代码： 利用 np.random.permutation 函数，我们可以获得打乱后的行号，输出permutation 为：array([2, 1, 0])。 然后，利用 numpy array 内置的操作 train_data[permutation, :, :] ，我们可以获得打乱行号后的新的训练数据。 我们看看训练数据和标签是不是对应的： 12345678&gt;&gt;&gt; shuffled_datasetarray([[[5, 6]], [[3, 4]], [[1, 2]]], dtype=int32)&gt;&gt;&gt; shuffled_labelsarray([3, 2, 1], dtype=int32) 没错，完全按照 permutation [2, 1, 0] 的顺序重新调整了。 学会这种技巧，妈妈再也不担心我加班了🤓","raw":null,"content":null,"categories":[{"name":"Python","slug":"Python","permalink":"https://jermmy.github.io/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://jermmy.github.io/tags/Python/"},{"name":"numpy","slug":"numpy","permalink":"https://jermmy.github.io/tags/numpy/"}]},{"title":"决策树生成算法","slug":"2017-4-13-decision-tree","date":"2017-04-13T04:10:03.000Z","updated":"2019-05-04T08:05:12.000Z","comments":true,"path":"2017/04/13/2017-4-13-decision-tree/","link":"","permalink":"https://jermmy.github.io/2017/04/13/2017-4-13-decision-tree/","excerpt":"关于决策树，想必大部分人都已经耳熟能详了，这是一种用来预测行为的树状分叉结构。本文主要想总结一下最常用的决策树生成算法：ID3，C4.5 以及 CART。\n构造的原则\n熟悉决策树的你一定记得，决策树每个非叶子结点对应的其实是一个属性。比方说，想判断一个男生是不是 gay，我们首先需要判断他的性别是不是男的，是的话继续判断他的性取向，之后继续判断他的其他行为……这里的「性别」，「性取向」就是属性，而决策树的生成其实是依次挑选这些属性组成自己的节点，到最终可以明确得出结论的时候（也就是叶子结点），整棵树便生成了。所以，我们的目标就是按照某种方法依次挑选出这些属性。\n我们挑选的原则是：每次选出这个属性后，可以最大限度地减小分类的可能性。回到 gay 这个问题，如果摆在我们眼前的属性有：「性取向」，「是否喜欢日漫」，「是否长发披肩」，那么，选择「性取向」这个属性，对我们之后的判断，帮助无疑是最大的。因为得知「性取向」之后，基本也就得到结论了。所以，对这个例子而言，「性取向」是我们优先挑选的属性。\n那么，我们如何衡量这种帮助的大小呢？请往下看👇。","text":"关于决策树，想必大部分人都已经耳熟能详了，这是一种用来预测行为的树状分叉结构。本文主要想总结一下最常用的决策树生成算法：ID3，C4.5 以及 CART。 构造的原则 熟悉决策树的你一定记得，决策树每个非叶子结点对应的其实是一个属性。比方说，想判断一个男生是不是 gay，我们首先需要判断他的性别是不是男的，是的话继续判断他的性取向，之后继续判断他的其他行为……这里的「性别」，「性取向」就是属性，而决策树的生成其实是依次挑选这些属性组成自己的节点，到最终可以明确得出结论的时候（也就是叶子结点），整棵树便生成了。所以，我们的目标就是按照某种方法依次挑选出这些属性。 我们挑选的原则是：每次选出这个属性后，可以最大限度地减小分类的可能性。回到 gay 这个问题，如果摆在我们眼前的属性有：「性取向」，「是否喜欢日漫」，「是否长发披肩」，那么，选择「性取向」这个属性，对我们之后的判断，帮助无疑是最大的。因为得知「性取向」之后，基本也就得到结论了。所以，对这个例子而言，「性取向」是我们优先挑选的属性。 那么，我们如何衡量这种帮助的大小呢？请往下看👇。 ID3 算法 ID3 算法归根到底就是提出一种合理的选择属性的方法。 （注意，决策树是一种知识学习算法，只有从众多样本中才能得出哪个属性最好，所以，构造决策树的前提是有大量的样本可供学习） 下面，为了方便讲解，我们需要引入信息学中「熵」的概念。 熵（entropy） 第一次接触熵的概念是在学高中化学的时候，课本告诉我们：一堆整齐有序的分子，最终都会演变成一个混乱复杂的群体，也就是，这个系统的熵值会逐渐变大。因此，简单整齐的系统，熵越小，越混乱的系统，熵越大。接下来，让我们回顾一下分子的布朗运动…… 开个玩笑啦🤗。 同化学里的熵一样，信息学的熵也有类似的作用。在信息学中，如果熵越大，证明掌握的信息越少，事情越不确定。看到这里，你有没有觉得，熵的定义和我们前面提出的挑选属性的原则有点类似。是的，ID3 的精髓也就是在这，它通过计算属性的熵，来得出一个属性对事情的确定性能产生多大的影响，从而选出最好的属性。 那么熵该如何度量呢？ 著名的信息论创始人「香农」提出一个度量熵的方法：假设有一堆样本 D，那么 D 的熵可以这样计算： \\[ H(D)=-\\sum_{i=1}^{m}{p_ilog_2(p_i)} \\] 其中，\\(p_i\\) 表示第 i 个类别在整个样本中出现的概率。 举个例子吧。假设我们投掷 10 枚硬币，其中，5 枚正面朝上，5 枚正面朝下，那么我们总共得到 10 个样本，这堆样本的熵为： \\(H(D)=-(\\frac{5}{10}log_2{\\frac{5}{10}} + \\frac{5}{10}log_2{\\frac{5}{10}})=1\\) 反之，如果只有 1 枚硬币正面朝上，9 枚硬币正面朝下，那么熵为： \\(H(D)=-(\\frac{1}{10}log_2{\\frac{1}{10}} + \\frac{9}{10}log_2{\\frac{9}{10}})=0.469\\) 如果全部硬币正面朝上，你应该可以算出来，熵为 0。 举这个例子是想说明：当熵的值越大的时候，事情会更加难以确定，如果你知道 10 次实验中，正面朝上的为 5 次，朝下的也为 5 次，那么下一次哪一面朝上，你是不是很难确定。相反，如果熵的值越小，事情就越明朗。当熵为 0，也就是 10 次都正面朝上的时候，下一次你会不会觉得正面朝上的概率会大很多（请忘掉你的传统思维，我没说这是一枚正常的硬币）。 选择属性 好了，有了熵的概念以及度量方法，下面我们可以正式地走一遍 ID3 的流程了。 同样的，假设我们有一堆数据 D，我们先计算出这堆样本的熵\\(H(D)\\)，接下来，我们要对每个属性对信息的价值进行评估。假设我们挑选出 A 属性，那么，根据 A 属性的类别，我们可以把这堆样本分成几个子样本，每个子样本都对应 A 属性中的一类。我们继续按照熵的定义计算这几个子样本的熵，由于它们的熵是挑选出 A 后剩下的，因此我们定义为： \\[ Remainder(A)=\\sum_{j=1}^{v}\\frac{|D_j|}{|D|}H(D_j) \\] 这个公式其实和之前的是一个道理，我们通过 A 将 D 分成几个子集 \\(D_j\\)，这个时候，我们仍然需要计算这堆样本的熵，因此，先分别计算出每个 \\(D_j\\) 的熵，然后乘以这个 \\(D_j\\) 样本占所有数据集的比重，最后将全部子集的熵加起来即可。前面说了，这个熵是挑选 A 后剩下的，那么很自然的，我们想知道 A 到底帮助消减了多少熵，于是，我们定义最后一个公式，即信息增益： \\[ Gain(A)=H(D)-Remainder(A) \\] 之前对熵的说明告诉我们，熵越大，信息越少，反之，信息越多。\\(Gain(A)\\)其实就是 A 对信息的确定作用，它是我们选出 A 属性后，信息的混乱程度减少的量。 好了，到这里，ID3 的关键部分已经讲完了。其实，每次挑选属性的时候，我们都是计算出所有属性的信息增益，选择最大的作为分裂的属性，将样本分成几个子集，然后，对每个子集，继续选出最好的属性，继续分裂，直到所有样本都属于同一类为止（都是 gay 或者都是正面朝上） 举个例子 下面用的这个例子摘自文末的参考博客算法杂货铺——分类算法之决策树(Decision tree)。 假设我们有以下这堆 SNS 社区的资料，我们想确定一个账号是否是真实。 其中，s 、m 和 l 分别表示小、中和大。 我们先计算出这堆样本的熵： \\(H(D)=-(0.7*log_2{0.7}+0.3*log_2{0.3}) = 0.879\\) 然后，我们计算每个属性的信息增益： \\[ \\begin{align} Remainder(L)=&amp;0.3*(-\\frac{0}{3}log_2{0}{3}-\\frac{3}{3}log_2{\\frac{3}{3}})\\\\&amp;+0.4*(-\\frac{1}{4}log_2\\frac{1}{4}-\\frac{3}{4}log_2\\frac{3}{4})\\\\&amp;+0.3*(-\\frac{1}{3}log_2\\frac{1}{3})=0.603 \\end{align} \\] \\[ Gain(L)=0.879-0.603=0.276 \\] 同样的道理： \\[Gain(F)=0.553\\] \\[Gain(H)=0.033\\] 经过比较，我们发现 F 的增益最高，于是选出 F 作为节点，构造出如下决策树： 注意，F 属性有三个类别，对应三个分支，其中，l 和 m 两个分支的数据都是同一类（账号真实性要么都是 no 要么都是 yes），因此这两个分支没法再分了，而 s 属性的分支，剩下一个四个样本的子集，我们之后的任务，是对这个子集继续分割，直到没法再分为止。 接下来要考虑 L 和 H 属性，同样的道理，我们继续计算增益，只不过这一次我们是在这个子集上计算。 \\[H(D)=-(\\frac{3}{4}*log_2{\\frac{3}{4}}+\\frac{1}{4}*log_2{\\frac{1}{4}})=0.811\\] \\[Remainder(L)=\\frac{1}{2}*(0)+\\frac{1}{2}(-\\frac{1}{2}log_2{\\frac{1}{2}}-\\frac{1}{2}log_2{\\frac{1}{2}})=0.5\\] \\[Remainder(H)=\\frac{3}{4}*[-\\frac{2}{3}log_2(\\frac{2}{3})-\\frac{1}{3}log_2(\\frac{1}{3})]+\\frac{1}{4}*0=0.689\\] \\[Gain(L)=0.811-0.5=0.311\\] \\[Gain(H)=0.811-0.689=0.122\\] 这一次，我们选择 L 属性进行分裂： 剩下的只有 H 属性，因此最后加上 H 节点。由于剩下的样本中只有 H=no 的数据，因此 yes 节点的数据没法判断（这种情况在数据量很大的时候一般不会遇到，因为数据量越大，涵盖的情况会更多），而剩下的两个样本存在 yes 和 no 两种情况，因此 no 节点往下也只能随机选择一种类别进行判断（这种情况一般是根据进行「多数表决」，即选择出现次数最多的类别作为最终类别，在数据量很大的情况下，出现次数一样多的情况几乎不会发生）。 属性为连续值的情况 上面给出的例子中，样本的特征都是离散值（e.g. s，m，l），而 ID3 算法确实也只对离散值起作用。如果遇到特征为连续值的情况，一般需要先将其离散化，例如：可以选定几个阈值\\(a_1\\)，\\(a_2\\)，\\(a_3\\)（\\(a_1&lt;a_2&lt;a_3\\)），根据这些阈值，将样本特征分为四类：\\(f &lt; a_1\\)，\\(a_1 &lt; f &lt; a_2\\)，\\(a_2 &lt; f &lt; a_3\\)，\\(f &gt; a_3\\)。然后，便可以按照一般的思路构建决策树了。 C4.5算法 C4.5 算法主要对 ID3 进行了改进，用「增益率」来衡量属性的信息增益效率。算法中定义了「分裂信息」： \\(SplitInfo(A)=-\\sum_{j=1}^{v}{\\frac{|D_j|}{|D|}log_2{\\frac{|D_j|}{|D|}}}\\) 然后，通过该信息，定义增益率公式为： \\(GainRatio(A)=\\frac{Gain(A)}{SplitInfo(A)}\\) C4.5选择具有最大「增益率」的属性作为分裂属性，而其余步骤，和 ID3 完全一致。 CART CART 指的是分类回归树（Classification And Regression Tree）。顾名思义，这是一棵既可以用于分类，也可以用于回归的树。不同于上面的两种树，CART 每一个非叶子节点只有有两个分支，所以 CART 是一棵二叉树。下面我们按照分类和回归两个用途分别介绍 CART 的构建。 分类树的生成 CART 在选择分裂节点的时候，用「基尼指数（Gini）」来挑选最合适的特征进行分裂。所谓「基尼指数」，其实和 ID3 中熵的作用类似。假设我们有一个数据集 D，其中包含 N 个类别，那么「基尼指数」为： \\[ Gini(D) = 1 - \\sum_{j=1}^{N}{P_j^2} \\] 其中，\\(P_j\\)表示每个类别出现的概率。 同熵一样，「基尼系数」的值越小，样本越纯，分类越容易。 我们根据 Gini 选择一个最合适的特征作为 CART 的分裂节点。注意，与 ID3 不同的是，如果特征的类别超过两类，CART 不会根据特征的所有类别分出子树，而是选择其中的一个类别，根据是否属于这个类别分成两棵子树。假设 A 特征中有三种类别（s、m、l），我们需要分别按照「是否属于 s 」、「是否属于 m 」、「是否属于 l 」将样本分为两类，根据 Gini 值最小的情况挑选出分裂的特征类别（比如：按照「是否属于 s 」将样本分为两类）。对于分裂后的样本的 Gini 值，我们按照如下公式计算： \\[ Gini(D, A) = \\sum_{j}^{k}{\\frac{|D_j|}{|D|}} \\] 其中，k 表示分裂的子集数目。事实上，在 CART 中，k 的取值为 2。 然后，选择 \\(Gini(D, A)\\) 最小的特征 A 作为分裂的特征即可。 这里还需要注意一点，在 ID3 中，已经选择过的特征是没法在之后的节点分裂中被选上的，即每个特征只能被挑选一次。但 CART 没有这种限制，每次都是将所有特征放在一起，通过「基尼系数」选出最好的，哪怕这个特征已经在之前被挑选过了。有学者认为，ID3 这种只挑选一次的做法容易导致 overfitting 问题，所以相信 CART 的这种做法能使模型的泛化能力更强。 CART 按照这样的方式，不断挑选特征分裂子数，直到剩下的子样本都属于同一类别，或者特征没法再分为止，这个停止条件可以参考 ID3 ，这里就不再举例说明了。 回归树的生成 回归树相对来说比较难理解，我自己也花了较长时间咀嚼，其中还有一些不明白的地方，日后有了新的想法会继续补充修正。 为了更好地说明回归树的构建流程，我们假设有以下训练数据： \\(X\\) \\(Y\\) (\\(x_{11}\\)，\\(x_{12}\\)，\\(x_{13}\\)) \\(y_1\\) (\\(x_{21}\\)，\\(x_{22}\\)，\\(x_{23}\\)) \\(y_2\\) (\\(x_{31}\\)，\\(x_{32}\\)，\\(x_{33}\\)) \\(y_3\\) 上面的表中一共有三个样本，每个样本有三个特征，为了解说方便，我们分别命名为特征 1、特征 2、特征 3（比如：\\(x_{11}\\)，\\(x_{21}\\)，\\(x_{31}\\) 就属于特征 1）。 与分类树类似，回归树每次也需要从样本 \\(X\\) 选取特征，并根据特征值将数据集切分为两份。在分类树中，我们是用「熵」或者「基尼系数」来确定分裂特征，但回归树中的 \\(X\\) 和 \\(Y\\) 都是连续值，因此需要采用新的特征挑选方式。 首先，我们先简单明确一下回归树的分裂原则：每次分裂后，每个样本集合内的数据要尽可能相似。这一点与分类树是不谋而合的，尽可能相似就是说，类别上要尽量统一。而为了做到这一点，最常用的方法便是「最小二乘法」。下面，我们定义「最小二乘法」的代价函数（可以简单认为就是回归树的信息熵）： \\[ \\min_{j,s}[\\min_{c_1}\\sum_{x_i\\ \\in\\ {R_1\\ (j,s)}}{(y_i-c_1)}^2+\\min_{c_2}\\sum_{x_i\\ \\in\\ {R_2\\ (j,s)}}{(y_i-c_2)}^2] \\] 其中， \\(j\\) 表示样本的特征，上面例子中的 \\(x_{11}\\)，\\(x_{21}\\) 就属于同一个特征。 \\(s\\) 表示特征的分裂值，如果 \\(s=x_{11}\\)，就表示所有样本以特征 1 为基准，按照 \\(&gt;=x_{11}\\) 和 \\(&lt;x_{11}\\) 分为两类。 \\(R_1\\) 表示分裂后的第一个样本集，\\(R_2\\) 表示分裂后的第二个样本集。 \\(c_1\\)、\\(c_2\\) 分别表示 \\(R_1\\)、\\(R_2\\) 的固定输出值。简单点说，它们是最能代表 \\(R_1\\)，\\(R_2\\) 内所有样本的值。 如果我们进一步对 \\(\\sum_{x_i\\ \\in\\ {R_1\\ (j,s)}}{(y_i-c_1)}^2\\) 求导的话，就会发现，要使这个式子最小，\\(c\\) 必须取 \\(y_i\\) 的平均值（ \\(y_i \\in R\\) ）。因此，我们对原公式进行简化： \\[ \\min_{j,s}[\\sum_{x_i\\ \\in\\ {R_1\\ (j,s)}}{(y_i-c_1)}^2+\\sum_{x_i\\ \\in\\ {R_2\\ (j,s)}}{(y_i-c_2)}^2] \\] 其中，\\(c_1\\)、\\(c_2\\) 分别是 \\(R_1\\)、\\(R_2\\) 两个集合中 \\(y\\) 的平均值。 （希望上面对符号的说明能减少读者对公式的畏惧🤒） 这个公式的做法其实很简单，就是枚举所有特征以及特征值，挑选出最好的特征以及特征值作为分裂点，将样本分为两部分，其中，每一部分内的样本值 \\(y\\) 的平方差最小。平方差最小，意味着这个样本内的数据是最相近的，可以认为属于同一类。 至此，回归树的精髓部分就介绍完了。下面顺藤摸瓜讲一下回归树的构建过程。 最小二乘回归树生成算法： 依次遍历每个特征 j，根据所有样本中特征 j 的取值 s，我们按照上面的公式计算代价函数，这样便可以得到每对 ( \\(j\\)，\\(s\\) ) 的代价函数，我们选择函数值最小的作为切分点； 使用上一步的切分点将数据分为两份； 重复第 1、2 步，直到样本的平方差小于阈值或样本数目小于阈值为止。此时，叶子节点的数据就是该样本空间 \\(R_m\\) 的平均值 \\(c_m\\)； 根据第 3 步构造的各个样本空间 \\(R_m\\)，生成回归树。 在本文参考的博文 CART之回归树构建内有一个构建回归树的简单例子，虽然计算方式略有不同，但和本文陈述的方法应该大同小异，我这里偷个懒，就不再举例了。 参考 算法杂货铺——分类算法之决策树(Decision tree) ID3 algorithm C4.5 algorithm CART算法学习及实现 分类与回归树（classification and regression tree，CART） CART分类与回归树 学习笔记 CART之回归树构建","raw":null,"content":null,"categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://jermmy.github.io/categories/机器学习/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"https://jermmy.github.io/tags/机器学习/"},{"name":"决策树","slug":"决策树","permalink":"https://jermmy.github.io/tags/决策树/"}]},{"title":"Bagging, Boosting, Bootstrap","slug":"2017-4-12-bagging-boosting-bootstrap","date":"2017-04-12T02:19:08.000Z","updated":"2017-04-23T05:37:06.000Z","comments":true,"path":"2017/04/12/2017-4-12-bagging-boosting-bootstrap/","link":"","permalink":"https://jermmy.github.io/2017/04/12/2017-4-12-bagging-boosting-bootstrap/","excerpt":"Bagging 和 Boosting 都属于机器学习中的元算法（meta-algorithms）。所谓元算法，简单来讲，就是将几个较弱的机器学习算法综合起来，构成一个更强的机器学习模型。这种「三个臭皮匠，赛过诸葛亮」的做法，可以帮助减小方差（over-fitting）和偏差（under-fitting），提高准确率。\n狭义的理解：Bagging，Boosting 为这种元算法的训练提供了一种采样的思路。","text":"Bagging 和 Boosting 都属于机器学习中的元算法（meta-algorithms）。所谓元算法，简单来讲，就是将几个较弱的机器学习算法综合起来，构成一个更强的机器学习模型。这种「三个臭皮匠，赛过诸葛亮」的做法，可以帮助减小方差（over-fitting）和偏差（under-fitting），提高准确率。 狭义的理解：Bagging，Boosting 为这种元算法的训练提供了一种采样的思路。 Boosting Boosting 最著名的实现版本应该是 AdaBoost 了。 Boosting 的流程一般为： 从数据集 D 中，无放回地、随机地挑选出一个子集 d1，训练一个弱的分类器 C1； 从数据集 D 中，无放回地、随机地挑选出一个子集 d2，再加上一部分上一步被错分类的样本，训练一个弱分类器 C2； 重复步骤 2，直到所有分类器都训练完毕； 综合所有的弱分类器，并为每个分类器赋予一个权值。 Bagging 采用 Bagging 原理的机器学习算法，代表的有 Random Forest（有些许改进）。 理解 Bagging 之前，需要先简单了解一下 Bootstrap 的概念。Bootstrap 是一种有放回的随机采样过程（注意，Boosting 是无放回的）。 Bagging 指的其实是 Bootstrap AGGregatING，「aggregating」是聚合的意思，也就是说，Bagging 是 Bootstrap 的增强版。 Bagging 的流程一般为： 根据 bootstrap 方法，生成 n 个不同的子集； 在每个子集上，单独地训练弱分类器（或者说，子机器学习模型）； 预测时，将每个子模型的预测结果平均一下，作为最终的预测结果。 Bagging 和 Boosting 对比 Bagging 这种有放回的采样策略，可以减少 over-fitting，而 Boosting 会修正那些错分类的样本，因此能提高准确率（但也可能导致 overfitting ）。 Bagging 由于样本之间没有关联，因此它的训练是可以并行的，比如 Random Forest 中，每一棵决策树都是可以同时训练的。Boosting 由于需要考虑上一步错分类的样本，因此需要顺序进行。 参考 What’s the difference between boosting and bagging? Bagging, boosting and stacking in machine learning bootstrap, boosting, bagging 几种方法的联系","raw":null,"content":null,"categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://jermmy.github.io/categories/机器学习/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"https://jermmy.github.io/tags/机器学习/"}]},{"title":"C++ offsetof","slug":"2017-4-8-c-plus-plus-offsetof","date":"2017-04-08T03:49:40.000Z","updated":"2017-04-22T05:04:41.000Z","comments":true,"path":"2017/04/08/2017-4-8-c-plus-plus-offsetof/","link":"","permalink":"https://jermmy.github.io/2017/04/08/2017-4-8-c-plus-plus-offsetof/","excerpt":"今天写 OpenGL 时，发现一个 C/C++ 中可能很实用但出现频率较低的函数： offsetof(type, member)。\n严格来说，这只是一个宏定义。其中的 type 表示一个数据结构（ struct 或者 class ），member 表示这个结构的成员。这个函数会返回一个 size_t 类型的数值，表示这个 member 在这个数据结构中的位置（以字节为单位）。","text":"今天写 OpenGL 时，发现一个 C/C++ 中可能很实用但出现频率较低的函数： offsetof(type, member)。 严格来说，这只是一个宏定义。其中的 type 表示一个数据结构（ struct 或者 class ），member 表示这个结构的成员。这个函数会返回一个 size_t 类型的数值，表示这个 member 在这个数据结构中的位置（以字节为单位）。 看个例子（改自 C++官网 ）： 12345678910111213141516171819#include &lt;iostream&gt;using namespace std;struct foo &#123; char a; char b[10]; int c; char d;&#125;;int main(int argc, const char * argv[]) &#123; cout &lt;&lt; offsetof(foo, a) &lt;&lt; endl; cout &lt;&lt; offsetof(foo, b) &lt;&lt; endl; cout &lt;&lt; offsetof(foo, c) &lt;&lt; endl; cout &lt;&lt; offsetof(foo, d) &lt;&lt; endl; return 0;&#125; 输出： 1234011216 这个例子简单明了。 在结构体 foo 中，成员 a 处于第一位的位置，所以它的偏移是 0 。 成员 b 处于第二位的位置，它的偏移就是 a 所占的空间大小，所以是 1 ，注意 offsetof 以字节为单位返回偏移量，而 a 是 char 类型，刚好占一个字节的空间。 成员 c 的偏移是成员 a 和 成员 b 所占空间之和。由于 b 是一个 10 字节的数组，所以 c 最终的偏移是 11。可等等！上面的结果却是 12 ！这并不是你的编译器在调皮，而是它悄悄帮你做了「对齐」的工作（关于「对齐」的知识，欢迎参考文末链接C语言字节对齐问题详解）。这里简单提一下为什么结果会是 12。从前面的分析我们知道，a 和 b 总共占了 11 个字节的内容，c 作为一个 int 变量，讲道理应该占据 11，12，13，14 这几个位置。可是，为了满足对齐的规则，int 变量必须从一个能满足被 4 整除的地址空间开始存放。所以，gcc/g++ 会跳过 11 这个位置，从 12 开始为 c 分配空间。而原来 11 这个位置，则会自行填充其他值（一般是 0 ）。 成员 d 在 c 之后，因此它的偏移位置就是 12 + 4 = 16。由于 d 是 char 类型，因此只要起始位置能被 1 整除就可以，不用对齐。 参考 C++官网 C语言字节对齐问题详解","raw":null,"content":null,"categories":[{"name":"C++","slug":"C","permalink":"https://jermmy.github.io/categories/C/"}],"tags":[{"name":"C++","slug":"C","permalink":"https://jermmy.github.io/tags/C/"}]},{"title":"翻译：什么是当下最有价值的编程技能？","slug":"2017-4-4-translate-what-is-the-most-valuable-programming-skill-at-the-moment","date":"2017-04-03T16:00:00.000Z","updated":"2019-02-16T15:51:08.000Z","comments":true,"path":"2017/04/04/2017-4-4-translate-what-is-the-most-valuable-programming-skill-at-the-moment/","link":"","permalink":"https://jermmy.github.io/2017/04/04/2017-4-4-translate-what-is-the-most-valuable-programming-skill-at-the-moment/","excerpt":"本文翻译自 What is the most valuable programming skill at the moment? 中的一个回答，其中谈及的对编程能力的看法很有借鉴意义，希望自己未来的时间能够静下心来，不忘初心，持续积累。由于这是我第一次翻译英文文章，其中难免有翻译不恰当的地方，还望指正😁。\n什么是当下最有价值的编程技能？\n最具价值的技能，你可能都不会想到\n\n\n\n（图片来自 collider.com）","text":"本文翻译自 What is the most valuable programming skill at the moment? 中的一个回答，其中谈及的对编程能力的看法很有借鉴意义，希望自己未来的时间能够静下心来，不忘初心，持续积累。由于这是我第一次翻译英文文章，其中难免有翻译不恰当的地方，还望指正😁。 什么是当下最有价值的编程技能？ 最具价值的技能，你可能都不会想到 （图片来自 collider.com） 就在某天，我正和我的一个学生在 Google Hangout 上谈论这个话题。 在谈话结束时，他跟我说： “Ken，这是我收到过的编程建议中最最无聊的一条。但我还是很开心你告诉了我。” 以下是我讲的主要内容： 正如其他领域的人一样，每一个编程人员都喜欢聊最新的东西。我有很多话题都是类似这样的内容： AI 是否是编程的未来？ 我应该学习 VR 吗？ React／Elixir／Websockets 是否会成为未来五年内最主要的技术？ 所有这些聊起来都会很有趣。但如果你只是关注前沿技术，你将完全忽略了要点。 编程一直都跟一项主要的技能有关： 你需要擅长于快速掌握要领。（译者注：原文是 You need to be really good at figuring things out on the fly.） 技术会不断革新。新的编程语言和框架将会一直出现。你不需要花费时间追逐新潮流。 相反，你应该花时间成为一个自主开发者上，在新事物出现时也能学会。 为什么？ 因为时代变了。没人能够确切地知道编程领域下一个重大事件会是什么。 不变的是，时代总是对那些人群有需求，那些： 能有效地使用 Google 搜索的人 能解决他们遇到的问题的人 能快速弄清事物的人 将自己变成能看清事物本质的人，那么你将永远为人所需。 如果你想成为一个被人所需的开发者，但不确定从哪开始，请查看 The Coder’s Compass，这是一个我帮助构建的工具，以帮助有抱负的开发者找到他们的道路。 原文：What is the most valuable programming skill at the moment? 帮助更正翻译的人：好同学桦姐","raw":null,"content":null,"categories":[{"name":"杂感","slug":"杂感","permalink":"https://jermmy.github.io/categories/杂感/"}],"tags":[{"name":"杂感","slug":"杂感","permalink":"https://jermmy.github.io/tags/杂感/"},{"name":"翻译","slug":"翻译","permalink":"https://jermmy.github.io/tags/翻译/"}]},{"title":"线性代数中的重要矩阵","slug":"2017-4-3-important-matrix-in-linear-algebra","date":"2017-04-02T16:00:00.000Z","updated":"2017-09-03T13:48:36.000Z","comments":true,"path":"2017/04/03/2017-4-3-important-matrix-in-linear-algebra/","link":"","permalink":"https://jermmy.github.io/2017/04/03/2017-4-3-important-matrix-in-linear-algebra/","excerpt":"老实说，我觉得线性代数可能是大学里最重要的数学，没有之一。无论是机器学习、计算机视觉，抑或是计算机图形学等等，都需要靠线性代数这门工具作支撑。这篇文章主要总结一下线性代数中那些很重要的矩阵们。","text":"老实说，我觉得线性代数可能是大学里最重要的数学，没有之一。无论是机器学习、计算机视觉，抑或是计算机图形学等等，都需要靠线性代数这门工具作支撑。这篇文章主要总结一下线性代数中那些很重要的矩阵们。 单位正交矩阵(orthonormal matrix) 单位正交矩阵，顾名思义，就是矩阵的列由两两相互正交的单位向量组成。用数学语言表达为（以 3 * 3 得矩阵为例）： \\[ U=\\begin{bmatrix} \\mathbf u1 &amp; \\mathbf u2 &amp; \\mathbf u_3 \\end{bmatrix} \\] 其中，\\(\\mathbf u_1^T\\mathbf u_2=\\mathbf u_2^T\\mathbf u_1=0\\)，\\(\\mathbf u_1^T\\mathbf u_3=\\mathbf u_3^T\\mathbf u_1=0\\)，\\(\\mathbf u_2^T\\mathbf u_3=\\mathbf u_3^T\\mathbf u_2=0\\)， 并且，\\(\\mathbf u_1^T\\mathbf u_1=1, \\mathbf u_2^T\\mathbf u_2=1, \\mathbf u_3^T\\mathbf u_3=1\\)。 （如果观察细致，你就会发现，这个矩阵的列向量其实是可以张成一个 \\(R^3\\) 空间的基。） 这个矩阵有什么用处呢？它隐藏着一个很重要的性质：\\(U^TU=I\\)。这个性质的证明也很简单，如下所示： \\(U^TU=\\begin{bmatrix} \\mathbf u_1^T \\\\\\\\ \\mathbf u_2^T \\\\\\\\ \\mathbf u_3^T \\end{bmatrix} \\begin{bmatrix} \\mathbf u_1 &amp; \\mathbf u_2 &amp; \\mathbf u_3 \\end{bmatrix}\\) \\(=\\begin{bmatrix} \\mathbf u_1^T\\mathbf u_1 &amp; \\mathbf u_1^T\\mathbf u_2 &amp; \\mathbf u_1^T\\mathbf u_3 \\\\\\\\ \\mathbf u_2^T\\mathbf u_1 &amp; \\mathbf u_2^T\\mathbf u_2 &amp; \\mathbf u_2^T\\mathbf u_3 \\\\\\\\ \\mathbf u_3^T\\mathbf u_1 &amp; \\mathbf u_3^T\\mathbf u_2 &amp; \\mathbf u_3^T\\mathbf u_3 \\end{bmatrix}\\) 结合前面的数学定义，很容易得到：\\(U^TU=I\\) 这个性质和我们熟悉的可逆矩阵的性质很类似。事实上，如果 \\(U\\) 同时是个方阵，那么，由可逆矩阵的性质，我们可以断定：\\(U\\) 是可逆的，并且 \\(U^{-1}=U^T\\)（由 \\(U^TU=I=U^{-1}U\\) 也可以间接推出来）。而且另一个很重要的性质是：在这种情况下，\\(U\\) 的行向量也是单位正交向量。 这个性质对我们来说很有帮助，因为求逆矩阵是一个计算量较大的工作，而求矩阵的转置就容易得多。因此，如果我们知道矩阵是一个单位正交矩阵，就可以利用它的转置轻松求出矩阵的逆。","raw":null,"content":null,"categories":[{"name":"线性代数","slug":"线性代数","permalink":"https://jermmy.github.io/categories/线性代数/"}],"tags":[{"name":"线性代数","slug":"线性代数","permalink":"https://jermmy.github.io/tags/线性代数/"}]},{"title":"3D中的旋转变换","slug":"2017-3-28-rotate-in-3D","date":"2017-03-28T15:24:08.000Z","updated":"2019-05-02T07:21:58.000Z","comments":true,"path":"2017/03/28/2017-3-28-rotate-in-3D/","link":"","permalink":"https://jermmy.github.io/2017/03/28/2017-3-28-rotate-in-3D/","excerpt":"相比 2D 中的旋转变换，3D 中的旋转变换复杂了很多。关于 2D 空间的旋转，可以看这篇文章。本文主要粗略地探讨一下 3D 空间中的旋转。\n旋转的要素\n所谓旋转要素就是说，我们只有知道了这些条件，才知道怎么旋转一个物体。回忆 2D 空间中的旋转，我们需要确定旋转中心、旋转角以及旋转方向才能旋转一个图形。以此类推，到了 3D 空间，我们仍然需要确定三个要素：一个旋转轴、旋转角以及旋转方向。 下面，为了讲解的方便，旋转方向默认为：正对旋转轴正方向，按逆时针方向为旋转正方向，反之为旋转负方向。","text":"相比 2D 中的旋转变换，3D 中的旋转变换复杂了很多。关于 2D 空间的旋转，可以看这篇文章。本文主要粗略地探讨一下 3D 空间中的旋转。 旋转的要素 所谓旋转要素就是说，我们只有知道了这些条件，才知道怎么旋转一个物体。回忆 2D 空间中的旋转，我们需要确定旋转中心、旋转角以及旋转方向才能旋转一个图形。以此类推，到了 3D 空间，我们仍然需要确定三个要素：一个旋转轴、旋转角以及旋转方向。 下面，为了讲解的方便，旋转方向默认为：正对旋转轴正方向，按逆时针方向为旋转正方向，反之为旋转负方向。 旋转的几种情况 3D 中的旋转本质上可以分为下面三类情况： 1. 绕 x / y / z 轴旋转； 2. 绕通过原点的直线旋转； 3. 绕不通过原点的直线旋转。 可能有同学不理解为什么要分这么多情况讨论，其实这是一个将复杂的问题简单化的过程。在旋转 2D 空间中的物体时，我们也只是计算出绕原点旋转的公式，然后将旋转点平移到跟原点重合，再根据公式旋转物体，最后再平移回去。其实完全可以计算出一个绕任意轴旋转的通用公式，但那样会导致计算量更大。 绕 x / y / z 轴旋转 这是最简单的旋转情况，只要把 2D 中的旋转延伸到 3D 空间就可以了。 绕 x 轴旋转 上图是一个绕 x 轴旋转的图示。假设我们需要从点(\\(x, y, z\\))绕 x 轴旋转 \\(\\theta\\) 角到点 (\\(x^{&#39;}, y^{&#39;}, z^{&#39;}\\))，那么，旋转过程中，x 的坐标值始终都是固定不变的，因此，我们可以把它当作是在\\(x=x^{&#39;}\\)这个平面上进行旋转，从而退化成一个 2D 旋转的问题。 上图右边的两个矩阵，上面那个是 2D 旋转矩阵，而底下那个只是把该矩阵延伸到 3D 空间而已（为了将平移也纳入矩阵运算，3D 的变换都是采用齐次坐标）。因为 x 轴是旋转轴，因此实际上是在 yoz 平面上做 2D 旋转。只要你知道 2D 空间那个旋转矩阵怎么计算，3D 的变换只是依葫芦画瓢而已。 绕 y 轴旋转 同理，这里不再赘述。 绕 z 轴旋转 同理，这里不再赘述。 绕通过原点的直线旋转 以下所引用的例子来自文末链接三维空间中的旋转：旋转矩阵、欧拉角 现在，假设我们要绕旋转轴 \\(P\\) 旋转 \\(\\theta\\) 角（如下图所示），那又该如何？ 目前我们已有的工具只是绕 x / y / z 轴旋转的矩阵而已。回想 2D 中绕任意点旋转的情况，我们是将任意点变换到原点，绕原点旋转后，再变换回原来的位置。所以，同样的道理，这次我们也将绕 \\(P\\) 轴的旋转分解为三步（跟原文例子的解释稍有不同，但本质上是一样的）: 将 \\(P\\) 轴旋转到与 z 轴重合，此时物体跟着旋转到新位置； 让物体绕 z 轴旋转 \\(\\theta\\) 角（可以直接套用之前的矩阵）； 将物体逆向旋转回原来的位置。 下面就针对这三步，解释一下具体的操作。 (1) 首先是将旋转轴旋转到与 z 轴重合。为此，我们需要将 \\(P\\) 轴绕 z 轴旋转 \\(\\psi\\) 角（根据前面的声明，这里是正方向）。因此，需要乘以矩阵： \\[ R_z(\\psi)=\\begin{bmatrix} cos\\psi &amp; -sin\\psi &amp; 0 &amp; 0 \\\\ sin\\psi &amp; cos\\psi &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 \\end{bmatrix} \\] 旋转完后，\\(P\\) 轴落入 xoz 平面，然后，按照同样的思路，绕 y 轴旋转 \\(\\phi\\) 角，再乘以矩阵： \\[ R_y(\\phi)=\\begin{bmatrix} cos\\phi &amp; 0 &amp; -sin\\phi &amp; 0 \\\\ 0 &amp; 1 &amp; 0 &amp; 0 \\\\ sin\\phi &amp; 0 &amp; cos\\phi &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 \\end{bmatrix} \\] 这时，\\(P\\) 轴与 z 轴已经重合了。 (2) 然后我们让物体绕 z 轴旋转 \\(\\theta\\) 角： \\[ R_z(\\theta)=\\begin{bmatrix} cos\\theta &amp; -sin\\theta &amp; 0 &amp; 0 \\\\ sin\\theta &amp; cos\\theta &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 \\end{bmatrix} \\] (3) 最后，将物体旋转回之前的位置。具体做法是乘以之前矩阵的逆矩阵。至此，我们得到物体旋转所需要的最终矩阵： \\[ R(\\theta)=R_z(-\\psi)R_y(-\\phi)R_z(\\theta)R_y(\\phi)R_z(\\psi) \\] 利用旋转矩阵的性质：\\(R(-\\alpha)=R^{-1}(\\alpha)=R^T(\\alpha)\\)，我们也可以写成： \\[ R(\\theta)=R_z^T(\\psi)R_y^T(\\phi)R_z(\\theta)R_y(\\phi)R_z(\\psi) \\] 绕不通过原点的直线旋转 有了上面的基础作铺垫，这种情况将变得十分简单。只要将旋转轴平移到经过原点的位置，那么问题就转换成上面的情况，最后再平移回去就可以了。因此，变换矩阵只是在上一种情况的基础上，乘上平移矩阵： \\[ R(\\theta)=T(x_1, y_1, z_1)R_z^T(\\psi)R_y^T(\\phi)R_z(\\theta)R_y(\\phi)R_z(\\psi)T(-x_1, -y_1, -z_1) \\] 参考 Interactive Computer Graphics - A Top-Down Approach 6e By Edward Angel and Dave Shreiner (Pearson, 2012) 三维空间中的旋转：旋转矩阵、欧拉角","raw":null,"content":null,"categories":[{"name":"计算机图形学","slug":"计算机图形学","permalink":"https://jermmy.github.io/categories/计算机图形学/"}],"tags":[{"name":"计算机图形学","slug":"计算机图形学","permalink":"https://jermmy.github.io/tags/计算机图形学/"},{"name":"线性代数","slug":"线性代数","permalink":"https://jermmy.github.io/tags/线性代数/"}]},{"title":"转载：每个人都需要的中文排版指南","slug":"2017-3-26-writing-style-for-blog","date":"2017-03-26T02:21:06.000Z","updated":"2018-12-02T12:07:01.000Z","comments":true,"path":"2017/03/26/2017-3-26-writing-style-for-blog/","link":"","permalink":"https://jermmy.github.io/2017/03/26/2017-3-26-writing-style-for-blog/","excerpt":"（本文转载自「stormzhang」在微信公众号「AndroidDeveloper」发表的关于写作排版的文章。看完后，再对比「Google黑板报」以及其他一些大V的文章排版后，发现原来好排版的套路都是一样的，在此记录一波。）","text":"（本文转载自「stormzhang」在微信公众号「AndroidDeveloper」发表的关于写作排版的文章。看完后，再对比「Google黑板报」以及其他一些大V的文章排版后，发现原来好排版的套路都是一样的，在此记录一波。） 之前有不少人在我公众号留言，说看我的排版很舒服，求指导。 我不是一个专业的写字人，我是程序员出身，在写代码的时候有点小洁癖，尽量会把每个细节做到更好，所以我在公众号写作的时候一样会注重每个细节，我认为排版是对写作最基本的要求，所以当我决定要在公众号写作的时候，就非常在意排版，并且一直在改进， 可惜的是，我看到无数不管是写博客还是写公众号的作者，从不会注意自己的排版，有时候看到一篇好的文章，如果排版很烂，会觉得很可惜。 公众号「小道消息」的作者 Fenng 曾不止一次强调排版的重要性，对此我也非常认同，任何说不拘小节，注重内容就行了的反驳都是借口，我自己在我的小密圈中也一直强调排版的重要性，很多圈友发布的动态无一排版都太烂，为此，我在小密圈中写下了这么一篇排版指南，真心希望所有的写字人都能注意下自己的排版。 这篇文章适用于所有编辑、作家、撰稿人、公众号作者、博主等，甚至适合所有人，因为人的一生难免避免不了写作。 1 空格 我每次看到网页上的中文字和英文、数字、符号挤在一起，就会坐立不安，忍不住想在他们之间加个空格。 「有研究显示，打字的时候不喜欢在中文和英文之间加空格的人，感情路都走得很辛苦，有七成的比例会在 34 岁的时候跟自己不爱的人结婚，而其余三成的人最后只能把遗产留给自己的猫。毕竟爱情跟书写都需要适时地留白。」 中英文之间需要加空格 eg. 大家可以搜索公众号 AndroidDeveloper 或者 googdev 关注我。 中文与数字之间需要加空格 eg. 不知不觉，我都快 30 岁了。 数字与单位之间需要加空格 eg. 我有一块 20 TB 的硬盘，鬼知道我是怎么用完的。 完整的英文整句时标点与单词之间需要加空格 eg. Stay hungry, stay foolish. 但是也有一些例外。 度的标志、百分号不加空格 eg. 今天气温有 30° 的高温。 eg. 据统计，关注公众号 AndroidDeveloper 的读者中有 80% 是长得好看的。 全角标点与其他字符之间不加空格 eg. 大家好，我是 stormzhang，请多多关照。 2 标点 说到标点，不得不说下全角和半角，很多人可能不了解全角和半角的概念，这里顺便介绍下。 全角和半角是英文和中文的编码规范不同遗留下的问题，简单来说，全角占两个字节，半角占一个字节，你可以理解成中文汉字是全角，英文字母是半角，不过半角全角主要是针对标点符号来说的，中文标点占两个字节，英文标点占一个字节。体现在排版上的差异就是，全角字符屏幕打印宽度是两个，而半角字符屏幕打印宽度是一个，如中文逗号和英文逗号他们的显示分别是「，」和「,」。 使用全角中文标点 中文排版中所有的标点都应该使用中文全角中文标点 eg. 大家好，我是 stormzhang。 遇到英文整句、特殊名词时使用半角标点。 eg. 乔布斯说过：「Stay hungry, stay foolish.」 eg. Facebook, Inc. 使用直角引号 我国国家标准要求弯引号，但是个人建议在新媒体排版时使用直角引号。 eg. 你竟然不知道「帅比张」？ 如果引号中使用引号使用直角双引号 eg. 我质问他，「你难道忘记『帅比张』了么？」 （这里插一句：想在电脑上打出直角引号，只需要切换到中文输入法，按 shitf + [ 就可以了，直角双引号在直角单引号内按 shift + [ 会自动识别并转换） 3 其他 除以上之外，还有一些其他标准推荐大家使用的。 英文名词首字母尽量大写 eg. Google、Android、Facebook。 专有名词使用正确的大小写 eg. GitHub、iOS、iPhone 6S、MacBook Pro。 首行不要缩进 这点可能有人不同意，但是我要在这里解释下，在说明之前我们必须弄明白「首行缩进」的目的是什么。 「每段之前空两格」是我们从小学写作文就养成的习惯，也是正式文体的格式要求，其目的是为了区分自然段。 但是像我们现在接触的阅读，都是没有固定的格式要求的，如微信公众号、电子文档等，所以大家一般都采用「空出一行」进行自然段与自然段之间的区分，这种写作方式非常省事，而且很整齐。 所以，我认为这种应该是最科学的方式，只要没有明确的格式要求，写作的排版无须首行缩进。 适当的间距、空行 如果段落太长，可以使用适当的空行分段，比如这篇文章，这样不会给读者造成阅读压力，提升阅读体验。 最后，你可能会问，以上这些原则是业界标准么？我不敢保证，但是你不妨看下 「Apple 中国官网」、「Microsoft 中国官网」、公众号「小道消息」、「MacTalk」、「可能吧」等的排版，基本都是使用以上排版标准。 转载自：每个人都需要的中文排版指南","raw":null,"content":null,"categories":[{"name":"写作","slug":"写作","permalink":"https://jermmy.github.io/categories/写作/"}],"tags":[{"name":"写作","slug":"写作","permalink":"https://jermmy.github.io/tags/写作/"},{"name":"转载","slug":"转载","permalink":"https://jermmy.github.io/tags/转载/"}]},{"title":"PCA算法浅析","slug":"2017-3-25-understand-PCA","date":"2017-03-25T01:27:35.000Z","updated":"2019-05-02T03:35:51.000Z","comments":true,"path":"2017/03/25/2017-3-25-understand-PCA/","link":"","permalink":"https://jermmy.github.io/2017/03/25/2017-3-25-understand-PCA/","excerpt":"最近频繁在论文中看到「PCA」的影子，所以今天决定好好把「PCA」的原理和算法过程弄清楚。\n「PCA」是什么\nPCA，又称主成分分析，英文全称「Principal Components Analysis」。维基百科上的解释是：「PCA」是一种分析、简化数据集的技术，经常用于减少数据集的维数，同时保持数据集中对方差贡献最大的特征。说得通俗一点，就是把数据集中重要的特征保存下来，把不重要的特征去除掉。","text":"最近频繁在论文中看到「PCA」的影子，所以今天决定好好把「PCA」的原理和算法过程弄清楚。 「PCA」是什么 PCA，又称主成分分析，英文全称「Principal Components Analysis」。维基百科上的解释是：「PCA」是一种分析、简化数据集的技术，经常用于减少数据集的维数，同时保持数据集中对方差贡献最大的特征。说得通俗一点，就是把数据集中重要的特征保存下来，把不重要的特征去除掉。 为什么要做这种事情呢？特征越多不是对分析问题更有帮助吗？确实，特征越多，涵盖的信息理论上会越多。但要注意一点，这个假设成立的前提是特征本身都彼此无关，并能描述事物的某些属性。如果由特征 A 可以推出特征 B，或者特征 B 所描述的属性与问题本身无关，或者特征 A 和特征 B 描述的是同一个特征，那么特征 B 是没有价值的。在机器学习里面，特征越多意味着需要更多的训练样本（否则容易 overfit ），但真实情况是，训练数据并不容易获取。基于以上种种，如果能构提取出特征的主要部分，那么，不仅数据将变得简单清晰，训练过程也会更快，效果理论上也会更好才是。 「PCA」算法过程 「PCA」的目标是把原来 n 维特征的数据压缩成 k 维特征。 接下来用一个例子说明「PCA」的执行流程（这个例子参考自文末链接主成分分析（Principal components analysis）-最大方差解释）。 假设我们得到 2 维数据如下： \\[ Data=\\begin{array}{c|lcr} x &amp; y \\\\ \\hline 2.5 &amp; 2.4 \\\\ 0.5 &amp; 0.7 \\\\ 2.2 &amp; 2.9 \\\\ 1.9 &amp; 2.2 \\\\ 3.1 &amp; 3.0 \\\\ 2.3 &amp; 2.7 \\\\ 2 &amp; 1.6 \\\\ 1 &amp; 1.1 \\\\ 1.5 &amp; 1.6 \\\\ 1.1 &amp; 0.9 \\\\ \\end{array} \\] 行代表样例，列代表特征，这里有 10 个样例，每个样例有两个特征。 step1 归一化样本数据 分别求出 x 和 y 的平均值，对于所有样本，都减去平均值。这里 x 的均值为 1.81，y 的均值为 1.91。归一化后得到： \\[ Data=\\begin{array}{c|lcr} x &amp; y \\\\ \\hline 0.69 &amp; 0.49 \\\\ -1.31 &amp; -1.21 \\\\ 0.39 &amp; 0.99 \\\\ 0.09 &amp; 0.29 \\\\ 1.29 &amp; 1.09 \\\\ 0.49 &amp; 0.79 \\\\ 0.19 &amp; -0.31 \\\\ -0.81 &amp; -0.81 \\\\ -0.31 &amp; -0.31 \\\\ -0.71 &amp; -1.01 \\\\ \\end{array} \\] step2 求协方差矩阵 由于原样本中的特征是 2 维的，所以得到的协方差矩阵的维度是 \\(2*2\\) （关于协方差矩阵的求法，参见上一篇文章协方差矩阵）。这里省略过程，直接给出结果： \\(cov=\\begin{bmatrix} 0.616555556 &amp; 0.615444444 \\\\ 0.615444444 &amp; 0.716555556 \\end{bmatrix}\\) step3 求协方差矩阵的特征值和特征向量 由于协方差矩阵是对称矩阵，因此在数学计算上更加方便。关于矩阵如何求特征值和特征向量，下次再写一篇相关的文章。这里假设我们已经求出了特征值和特征向量： \\[ eigenvalues=\\left( \\begin{matrix} 0.490833989 \\\\ 1.28402771 \\end{matrix} \\right) \\] \\[ eigenvectors=\\left( \\begin{matrix} -0.735178656 &amp; -0.677873399 \\\\ 0.677873399 &amp; -0.735178656 \\end{matrix} \\right) \\] 上面是两个特征值，下面是对应的特征向量，特征值 0.0490833989 对应的特征向量为 \\((-0.735178656， 0.677873399)^T\\)，这里的特征向量都为归一化后的向量。 step4 选取特征向量矩阵 将特征值按照从大到小的顺序排列，选择其中最大的 k 个，然后将其对应的 k 个特征向量分别作为列向量组成特征向量矩阵。由于这里特征值只有两个，我们选择较大的那个，也就是 1.28402771，得到的特征向量矩阵为： \\[ \\begin{bmatrix} -0.677873399 \\\\ -0.735178656 \\end{bmatrix} \\] step5 将样本投影到特征矩阵上 假设样本数为 m，特征数为 n，归一化后的样本矩阵为 DataAdjust(m * n)，协方差矩阵为 n * n，选取的 k 个特征向量组成的矩阵为 EigenVectors(n * k)，则投影后得到的最终数据为： FinalData(m * k) = DataAdjust(m * n) * EigenVectors(n * k) 在本例中， \\[ FinalData(m*k)=\\begin{array}{c|lcr} x \\\\ \\hline -0.82797 \\\\ 1.77758 \\\\ -0.992197494 \\\\ -0.27421 \\\\ -1.67580 \\\\ -0.91294 \\\\ 0.09910 \\\\ 1.14457 \\\\ 0.43804 \\\\ 1.22382 \\\\ \\end{array} \\] 这样，我们就将原始的 n 维特征数据变成了 k 维（本例中，是 2 维到 1 维）。 总结 讲到这，「PCA」的流程就基本结束了。 不过，总结之前还有个细节要交代。虽然 step1 的时候我们已经对数据做了归一化，让数据的特征值都平移到原点附近，但对不同特征之间方差差异较大的问题仍然没解决（比如：样本第一个特征是汽车速度「0～100」，第二个特征是汽车座位数「2～6」，那么很明显，第一个特征的方差比第二个大很多）。这个问题会导致方差较大的特征对样本起到主导作用。为了消除这种差异的方法，除了减去均值外，还需要除以各自的标准差，流程归纳如下： Let \\(\\mu = \\frac{1}{m} \\sum_{i=1}^m{x^{(i)}}\\) Replace each \\(x^{(i)} \\text{with} \\ x^{(i)}-\\mu\\) Let \\(\\sigma_{j}^2 = \\frac{1}{m} \\sum_{i}{({x_{j}^{(i)}})^2}\\) Replace each \\(x_j^{(i)} \\text{with} \\ \\frac{x_j^{(i)}}{\\sigma_j}\\) （注：\\(x^{(i)}\\) 代指每个样例） PCA的总体流程为： 1. 样本归一化； 2. 求样本特征的协方差矩阵； 3. 选取 k 个最大的特征值； 4. 组成特征向量矩阵； 5. 将样本数据投影到特征向量矩阵上。 实际应用 在机器学习中，我们通过训练集计算出特征向量矩阵并降维，然后在降维后的训练集上进行训练。预测的时候，我们同样需要对测试集进行降维（要保证数据模型的统一），降维的方法是用训练集计算出来的特征向量矩阵与测试集的数据相乘，然后再对降维的测试数据进行预测评估。 参考 维基百科：主成分分析 主成分分析（Principal components analysis）-最大方差解释","raw":null,"content":null,"categories":[{"name":"机器学习","slug":"机器学习","permalink":"https://jermmy.github.io/categories/机器学习/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"https://jermmy.github.io/tags/机器学习/"},{"name":"线性代数","slug":"线性代数","permalink":"https://jermmy.github.io/tags/线性代数/"}]},{"title":"协方差矩阵","slug":"2017-3-19-covariance-matrix","date":"2017-03-19T01:41:39.000Z","updated":"2019-03-30T14:01:10.000Z","comments":true,"path":"2017/03/19/2017-3-19-covariance-matrix/","link":"","permalink":"https://jermmy.github.io/2017/03/19/2017-3-19-covariance-matrix/","excerpt":"概念\n协方差（Covariance）在概率论和统计学中用于衡量两个变量的总体误差。而方差是协方差的一种特殊情况，即当两个变量是相同的情况。 这个解释摘自维基百科，看起来很是抽象，不好理解。其实简单来讲，协方差就是衡量两个变量相关性的变量。当协方差为正时，两个变量呈正相关关系（同增同减）；当协方差为负时，两个变量呈负相关关系（一增一减）。 而协方差矩阵，只是将所有变量的协方差关系用矩阵的形式表现出来而已。通过矩阵这一工具，可以更方便地进行数学运算。","text":"概念 协方差（Covariance）在概率论和统计学中用于衡量两个变量的总体误差。而方差是协方差的一种特殊情况，即当两个变量是相同的情况。 这个解释摘自维基百科，看起来很是抽象，不好理解。其实简单来讲，协方差就是衡量两个变量相关性的变量。当协方差为正时，两个变量呈正相关关系（同增同减）；当协方差为负时，两个变量呈负相关关系（一增一减）。 而协方差矩阵，只是将所有变量的协方差关系用矩阵的形式表现出来而已。通过矩阵这一工具，可以更方便地进行数学运算。 数学定义 回想概率统计里面关于方差的数学定义： \\[ Var(X)=\\frac{\\sum_{i=1}^n{(x_i-\\overline x)(x_i-\\overline x)}}{n-1} \\] 协方差的数学定义异曲同工： \\[ Cov(X,Y)=\\frac{\\sum_{i=1}^n{(x_i-\\overline x)(y_i-\\overline y)}}{n-1} \\] 这里的 \\(X\\)，\\(Y\\) 表示两个变量空间。用机器学习的话讲，就是样本有 \\(x\\) 和 \\(y\\) 两种特征，而 \\(X\\) 就是包含所有样本的 \\(x\\) 特征的集合，\\(Y\\) 就是包含所有样本的 \\(y\\) 特征的集合。 协方差矩阵 两个变量的协方差矩阵 有了上面的数学定义后，我们可以来讨论协方差矩阵了。当然，协方差本身就能够处理二维问题，两个变量的协方差矩阵并没有实际意义，不过为了方便后面多维的推广，我们还是从二维开始。 用一个例子来解释会更加形象。 假设我们有 4 个样本，每个样本都有两个变量，也就是两个特征，它们表示如下： \\(x_1=(1,2)\\)，\\(x_2=(3,6)\\)，\\(x_3=(4,2)\\)，\\(x_4=(5,2)\\) 用一个矩阵表示为： \\[ Z=\\begin{bmatrix} 1 &amp; 2 \\\\ 3 &amp; 6 \\\\ 4 &amp; 2 \\\\ 5 &amp; 2 \\end{bmatrix} \\] 现在，我们用两个变量空间 \\(X\\)，\\(Y\\) 来表示这两个特征： \\[ X=\\begin{bmatrix} 1 \\\\ 3 \\\\ 4 \\\\ 5 \\end{bmatrix}, \\ \\ \\ Y=\\begin{bmatrix} 2 \\\\ 6 \\\\ 2 \\\\ 2 \\end{bmatrix} \\] 由于协方差反应的是两个变量之间的相关性，因此，协方差矩阵表示的是所有变量之间两两相关的关系，具体来讲，一个包含两个特征的矩阵，其协方差矩阵应该有 \\(2 \\times 2\\) 大小： \\[ Cov(Z)=\\begin{bmatrix} Cov(X,X) &amp; Cov(X,Y) \\\\ Cov(Y,X) &amp; Cov(Y,Y) \\end{bmatrix} \\] 接下来，就来逐一计算 \\(Cov(Z)\\) 的值。 首先，我们需要先计算出 \\(X\\)，\\(Y\\) 两个特征空间的平均值：\\(\\overline x=3.25\\)，\\(\\overline y=3\\)。 然后，根据协方差的数学定义，计算协方差矩阵的每个元素： \\(Cov(X,X)=\\frac{(1-3.25)^2+(3-3.25)^2+(4-3.25)^2+(5-3.25)^2}{4-1}=2.9167\\) \\(Cov(X,Y)=\\frac{(1-3.25)(2-3)+(3-3.25)(6-3)+(4-3.25)(2-3)+(5-3.25)(2-3)}{4-1}=-0.3333\\) \\(Cov(Y,X)=\\frac{(2-3)(1-3.25)+(6-3)(3-3.25)+(2-3)(4-3.25)+(2-3)(5-3.25)}{4-1}=-0.3333\\) \\(Cov(Y,Y)=\\frac{(2-3)^2+(6-3)^2+(2-3)^2+(2-3)^2}{4-1}=4\\) 所以协方差矩阵 \\(Cov(Z)=\\begin{bmatrix} 2.9167 &amp; -0.3333 \\\\ -0.3333 &amp; 4.000 \\end{bmatrix}\\) 好了，虽然这只是一个二维特征的例子，但我们已经可以从中总结出协方差矩阵 \\(\\Sigma​\\) 的「计算套路」： \\(\\Sigma_{ij}=\\frac{(样本矩阵第i列-第i列均值)^T(样本矩阵第j列-第j列均值)}{样本数-1}\\) 这里所说的样本矩阵可以参考上面例子中的 \\(Z\\)。 多个变量的协方差矩阵 接下来，就用上面推出的计算协方差矩阵的「普世规律」。 假设我们有三个样本： \\(x_1=(1,2,3,4)^T\\)， \\(x_2=(3,4,1,2)^T\\)， \\(x_3=(2,3,1,4)^T\\)。 同理我们将它们表示成样本矩阵： \\(Z=\\begin{bmatrix} 1 &amp; 2 &amp; 3 &amp; 4 \\\\ 3 &amp; 4 &amp; 1 &amp; 2 \\\\ 2 &amp; 3 &amp; 1 &amp; 4 \\end{bmatrix}​\\) 按照上面给出的计算套路，我们需要先计算出矩阵每一列的均值，从左到右分别为：2、3、1.67、3.33。 然后按照上面讲到的公式，计算矩阵每个元素的值，对了，四个变量的协方差矩阵，大小为 \\(4 \\times 4\\) ： \\(\\Sigma_{11}=\\frac{(第1列-第1列的均值)^T*(第1列-第1列的均值)}{样本数-1} \\\\=\\frac{(-1,1,0)^T*(-1,1,0)}{2}=1​\\) （后面的依此类推……） 独立变量的协方差 以上的讨论都是针对一般情况进行计算的，毕竟变量互相独立的情况较少。 不过，如果两个变量 \\(X\\), \\(Y\\) 独立，那么它们的协方差 \\(Cov(X,Y) = 0\\)。简要证明如下（简单起见，假设变量是离散的）： 由于 \\(X, Y\\) 独立，所以它们的概率密度函数满足：\\(p(x,y)=p_x(x)p_y(y)\\)。 求出期望： \\[ \\begin{eqnarray} E(XY) &amp; = &amp;\\sum_x \\sum_y {x \\times y \\times p(x,y)} \\notag \\\\ &amp; = &amp;\\sum_x \\sum_y x \\times y \\times p_x(x) \\times p_y(y) \\notag \\\\ &amp; = &amp;\\sum_x{x \\times p_x(x)}\\sum_y{y \\times p_y(y)} \\notag \\\\ &amp; = &amp;E(X)E(Y) \\notag \\end{eqnarray} \\] 利用协方差的另一个公式：\\(Cov(X,Y)=E(X,Y)-E(X)E(Y)\\)，可以推出，当 \\(X, Y\\) 相互独立时，\\(Cov(X, Y)=0\\)。 这时，协方差矩阵就变成一个对角矩阵了： \\[ Cov(Z)=\\begin{bmatrix} Cov(X,X) &amp; 0\\\\ 0 &amp; Cov(Y,Y) \\end{bmatrix} \\] 协方差矩阵的作用 虽然我们已经知道协方差矩阵的计算方法了，但还有一个更重要的问题：协方差矩阵有什么作用？作为一种数学工具，协方差矩阵经常被用来计算特征之间的某种联系。在机器学习的论文中，协方差矩阵的出现概率还是很高的，用于降维的主成分分析法（PCA）就用到了协方差矩阵。另外，由于协方差矩阵是一个对称矩阵，因此它包含了很多很有用的性质，这也导致它受青睐的程度较高。 参考 详解协方差与协方差矩阵 浅谈协方差矩阵 协方差","raw":null,"content":null,"categories":[{"name":"线性代数","slug":"线性代数","permalink":"https://jermmy.github.io/categories/线性代数/"}],"tags":[{"name":"线性代数","slug":"线性代数","permalink":"https://jermmy.github.io/tags/线性代数/"},{"name":"概率统计","slug":"概率统计","permalink":"https://jermmy.github.io/tags/概率统计/"}]},{"title":"用C++写一个文件分割器","slug":"2017-3-18-a-file-segmentation-in-c-plus-plus","date":"2017-03-18T07:48:21.000Z","updated":"2017-06-04T15:27:48.000Z","comments":true,"path":"2017/03/18/2017-3-18-a-file-segmentation-in-c-plus-plus/","link":"","permalink":"https://jermmy.github.io/2017/03/18/2017-3-18-a-file-segmentation-in-c-plus-plus/","excerpt":"在成功将 mac 由 10.10 升级到 10.12 后，我发现除了新增一个并不怎么好用的 Siri 外，原来支持 NTFS 硬盘的驱动居然也成功失效了。我那块 500 GB 的东芝硬盘，虽不至于成砖，但一块只能读不能写的硬盘，实在让人欲哭无泪。巧的是，最近需要频繁地将一些数据文件( GB 级别)拷贝到其他电脑，而手头又仅剩一些小容量 U 盘。于是，我突然萌生了写一个文件分割器的想法，将大的压缩文件分片后，再用这些小 U 盘搬到到其他电脑上去。\n有人会问，这样的软件明明网上有的是，何必自己写呢？没错，我就是这么无聊的人。","text":"在成功将 mac 由 10.10 升级到 10.12 后，我发现除了新增一个并不怎么好用的 Siri 外，原来支持 NTFS 硬盘的驱动居然也成功失效了。我那块 500 GB 的东芝硬盘，虽不至于成砖，但一块只能读不能写的硬盘，实在让人欲哭无泪。巧的是，最近需要频繁地将一些数据文件( GB 级别)拷贝到其他电脑，而手头又仅剩一些小容量 U 盘。于是，我突然萌生了写一个文件分割器的想法，将大的压缩文件分片后，再用这些小 U 盘搬到到其他电脑上去。 有人会问，这样的软件明明网上有的是，何必自己写呢？没错，我就是这么无聊的人。 需求分析 其实也不用怎么分析，功能非常简单。我需要两个函数（分别用于分割和合成），分割函数的输入是：一个文件、分片数量，输出是：分片文件、一个配置文件（记录分片文件的顺序）；合成函数的输入是：配置文件，输出是：完整的数据文件（根据配置，程序会寻找分片文件用于合成）。 基于此，其实要实现的是两个这样的函数： 1234// 分割文件的函数，第三个参数指定配置文件名称void segment(string file_name, int segment_num, string json_file);// 合成文件的函数，参数为分割时生成的配置文件void merge(string json_file); 配置文件的格式，我使用了 json（其实用简单的字符串记录一下也是可以的）。 另外，为了方便使用，最好再用一个类将两个方法封装一下。 难点分析 这么小的程序会有难点？！其实还是有一丢丢，就是切割文件的时候，由于文件可能太大，因此不能一口气读入内存中，所以这里采用分块的方法，读一小块写一小块。当然啦，速度方面的优化，这里先不考虑了。 程序实现 首先，我们把所有功能放在一个类FileSegment里面实现，对外只暴露上面的两个函数接口。 segment 上面的难度分析已经指出，我们需要分块读取文件，然后分块写入。 首先需要定义分块大小：const int FileSegment::kBlockSize = 1024 * 1024; ，这里设定一个块大小为1 MB。 我们再定义两个辅助函数，用来分块读文件、写文件： 12345678// 从input流中读取size(默认大小kBlockSize)大小的字节到data里面inline void read_file_in_block(char* data, ifstream &amp;input, int size=kBlockSize) &#123; input.read(data, size);&#125;// 从data中将size(默认大小kBlockSize)大小的字节写入到output流inline void write_file_in_block(char* data, ofstream &amp;output, int size=kBlockSize) &#123; output.write(data, size);&#125; 这两个函数因为要经常用到，所以把它们作为内联函数使用。 综合这两个辅助函数，我们定义另一个辅助函数，用于从输入文件中将大批量的数据写入到输出文件中： 12345678910111213141516171819// 将input流中读取input_size大小的字节内容，写入到output流中void FileSegment::copy_file(ifstream &amp;input, ofstream &amp;output, size_t input_size) &#123; char* data = new char[kBlockSize]; for (size_t block = 0; block &lt; input_size / kBlockSize; block++) &#123; read_file_in_block(data, input); write_file_in_block(data, output); &#125; // 读取剩余的字节 size_t left_size = input_size % kBlockSize; if (left_size != 0) &#123; read_file_in_block(data, input, left_size); write_file_in_block(data, output, left_size); &#125; delete [] data; data = nullptr;&#125; 有了上面的辅助函数后，我们可以聚焦于segment()函数的核心代码部分了。 我们只需要利用copy_file()函数，将源文件分片写入到几个分片文件中即可。 1234567891011121314151617181920212223242526// 分片文件名vector&lt;string&gt; segment_files;for (int i = 0; i &lt; segment_num; i++) &#123; segment_files.push_back(file_name + to_string(i+1) + \".tmp\"); cout &lt;&lt; \"segment_file --- \" &lt;&lt; segment_files[i] &lt;&lt; endl;&#125;ifstream src_file_input(file_name);// 输入文件大小size_t src_file_size = file_size(src_file_input);// 分片文件大小size_t segment_size = src_file_size / segment_num;// 分片输出文件for (int i = 0; i &lt; segment_num; i++) &#123; ofstream segment_file_output(segment_files[i]); if (i == segment_num-1) &#123; // 最后一次，要将剩余文件片全部写入 size_t left_size = src_file_size % segment_size; copy_file(src_file_input, segment_file_output, segment_size + left_size); &#125; else &#123; copy_file(src_file_input, segment_file_output, segment_size); &#125; segment_file_output.close();&#125;src_file_input.close(); 另外，我们需要将分片文件的文件名和分割顺序等信息写入配置文件中，这里使用json格式，并用这个第三方库来操纵json对象。 1234567891011const string FileSegment::kSegmentFileNum = \"SegmentNum\";const string FileSegment::kSourceFileName = \"SourceFileName\";const string FileSegment::kSegmentFiles = \"SegmentFiles\";ofstream json_output(json_file);json j;j[kSegmentFileNum] = segment_num;j[kSourceFileName] = file_name;j[kSegmentFiles] = segment_files; // 这里segment_files是vector对象json_output &lt;&lt; j;json_output.close(); 下面给出segment()函数的完整代码： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849void FileSegment::segment(string file_name, int segment_num, string json_file) &#123; // 检查源文件是否存在 if (!exist(file_name)) &#123; cout &lt;&lt; \"file [\" &lt;&lt; file_name &lt;&lt; \"] doesn't exist!\" &lt;&lt; endl; return; &#125; // 检查分片数量是否大于0 if (segment_num &lt;= 0) &#123; cout &lt;&lt; \"segment number should be greater than 0!\" &lt;&lt; endl; return; &#125; // 分片文件名 vector&lt;string&gt; segment_files; for (int i = 0; i &lt; segment_num; i++) &#123; segment_files.push_back(file_name + to_string(i+1) + \".tmp\"); cout &lt;&lt; \"segment_file --- \" &lt;&lt; segment_files[i] &lt;&lt; endl; &#125; ifstream src_file_input(file_name); // 输入文件大小 size_t src_file_size = file_size(src_file_input); // 分片文件大小 size_t segment_size = src_file_size / segment_num; // 分片输出文件 for (int i = 0; i &lt; segment_num; i++) &#123; ofstream segment_file_output(segment_files[i]); if (i == segment_num-1) &#123; // 最后一次，要将剩余文件片全部写入 size_t left_size = src_file_size % segment_size; copy_file(src_file_input, segment_file_output, segment_size + left_size); &#125; else &#123; copy_file(src_file_input, segment_file_output, segment_size); &#125; segment_file_output.close(); &#125; src_file_input.close(); ofstream json_output(json_file); json j; j[kSegmentFileNum] = segment_num; j[kSourceFileName] = file_name; j[kSegmentFiles] = segment_files; json_output &lt;&lt; j; json_output.close();&#125; merge 有了前面的辅助函数后，merge()函数的实现基本是依葫芦画瓢。首先需要从配置文件中读取出json对象，根据配置信息去合成文件： 1234567891011121314151617181920212223json j;if (!exist(json_file)) &#123; cout &lt;&lt; \"json file [\" &lt;&lt; json_file &lt;&lt; \"] doesn't exist!\" &lt;&lt; endl; return;&#125;ifstream json_input(json_file);json_input &gt;&gt; j;// 源文件名string src_file = j[kSourceFileName];// 检查源文件是否已经存在if (exist(src_file)) &#123; src_file += \".copy\";&#125;ofstream result(src_file);// 文件分片数量int segment_num = j[kSegmentFileNum];// 分片文件名vector&lt;string&gt; segment_files = j[kSegmentFiles]; 之后，根据分片文件来合成大文件： 12345678// 合并文件for (auto it = segment_files.begin(); it != segment_files.end(); it++) &#123; cout &lt;&lt; \"copy file [\" &lt;&lt; *it &lt;&lt; \"]\" &lt;&lt; endl; ifstream seg_input(*it); size_t seg_input_size = file_size(seg_input); // 计算分片文件大小 copy_file(seg_input, result, seg_input_size); seg_input.close();&#125; 接下来照例给出merge()函数完整实现： 123456789101112131415161718192021222324252627282930313233343536373839404142434445void FileSegment::merge(string json_file) &#123; json j; if (!exist(json_file)) &#123; cout &lt;&lt; \"json file [\" &lt;&lt; json_file &lt;&lt; \"] doesn't exist!\" &lt;&lt; endl; return; &#125; ifstream json_input(json_file); json_input &gt;&gt; j; // 源文件名 string src_file = j[kSourceFileName]; // 检查源文件是否已经存在 if (exist(src_file)) &#123; src_file += \".copy\"; &#125; ofstream result(src_file); // 文件分片数量 int segment_num = j[kSegmentFileNum]; // 分片文件名 vector&lt;string&gt; segment_files = j[kSegmentFiles]; // 检查文件分片是否齐全 for (auto it = segment_files.begin(); it != segment_files.end(); ++it) &#123; if (!exist(*it)) &#123; cout &lt;&lt; \"segment file [\" &lt;&lt; *it &lt;&lt; \"] doesn't exist!\" &lt;&lt; endl; return; &#125; &#125; // 合并文件 for (auto it = segment_files.begin(); it != segment_files.end(); it++) &#123; cout &lt;&lt; \"copy file [\" &lt;&lt; *it &lt;&lt; \"]\" &lt;&lt; endl; ifstream seg_input(*it); size_t seg_input_size = file_size(seg_input); copy_file(seg_input, result, seg_input_size); seg_input.close(); &#125; json_input.close(); result.close();&#125; &lt;br&gt; main 在main()中，直接实例化FileSegment类，通过segment()和merge()函数分割或者合成文件。 1234567int main(int argc, char const *argv[]) &#123; FileSegment fs; // 分割data.zip文件，分为4片 fs.segment(\"data.zip\", 4, \"config.json\"); // 根据config.json文件合成最终文件 fs.merge(\"config.json\");&#125; 另外，为了方便使用，我特意写了一个解析命令的类InputParser，然后，我们可以按照如下方式使用该程序： 分割文件 ./main -s data.zip 4 config.json 合成文件 ./main -m config.json 完整工程代码，请看：https://github.com/Jermmy/file_segmentation 测试结果 在我的 mac (双核，2.7 GHz Intel Core i5) 上，将一个 7.35 G 的 zip 文件分割为 10 片，所用时间为 37.7 s。 同样的机器，将上面的 10 片文件合成原来的大文件，所用时间为 31.8 s。","raw":null,"content":null,"categories":[{"name":"C++","slug":"C","permalink":"https://jermmy.github.io/categories/C/"}],"tags":[{"name":"C++","slug":"C","permalink":"https://jermmy.github.io/tags/C/"}]},{"title":"坐标系统之间的转换","slug":"2017-3-14-learning-cg-coordinate-transformation","date":"2017-03-13T16:00:00.000Z","updated":"2018-01-04T11:48:01.000Z","comments":true,"path":"2017/03/14/2017-3-14-learning-cg-coordinate-transformation/","link":"","permalink":"https://jermmy.github.io/2017/03/14/2017-3-14-learning-cg-coordinate-transformation/","excerpt":"概要\n这篇文章中，我们来聊聊 OpenGL 中的坐标系统以及它们之间的转换。\n（⚠️阅读本文需要有线性代数基础。）\n坐标变换原理\n首先，我们需要运用一点线性代数的知识，了解不同坐标系统变换的原理。 由于本文针对的是三维坐标，所以讨论的空间是 \\(R^3\\) 空间。","text":"概要 这篇文章中，我们来聊聊 OpenGL 中的坐标系统以及它们之间的转换。 （⚠️阅读本文需要有线性代数基础。） 坐标变换原理 首先，我们需要运用一点线性代数的知识，了解不同坐标系统变换的原理。 由于本文针对的是三维坐标，所以讨论的空间是 \\(R^3\\) 空间。 在标准三维坐标系中，我们通常用一个向量 v=[x, y, z] 来表示一个点的位置。这里的 x、y、z 分别对应 x 轴、y 轴以及 z 轴三个方向的偏移，而标准三维坐标空间的基采用的是三个互相垂直的向量 \\(\\mathbf e_1=[1,0,0]\\), \\(\\mathbf e_2=[0,1,0]\\), \\(\\mathbf e_3=[0,0,1]\\)。但根据线性无关等知识，我们完全可以找出另外三个向量作为三维空间的基，只要这三个向量线性无关，同样能够张成 \\(R^3\\) 空间。 现在，假设坐标系 A 采用的基向量是 {\\(\\mathbf v_1, \\mathbf v_2, \\mathbf v_3\\)}，坐标系 B 采用的是{\\(\\mathbf u_1\\), \\(\\mathbf u_2\\), \\(\\mathbf u_3\\)}。 那么，根据线性无关性，我们可以得到线性方程组： \\[ {\\mathbf u_1 = \\gamma_{11}\\mathbf v1+\\gamma_{12}\\mathbf v2+\\gamma_{13}\\mathbf v_3} \\] \\[ \\mathbf u_2 = \\gamma_{21}\\mathbf v1+\\gamma_{22}\\mathbf v2+\\gamma_{23}\\mathbf v_3 \\] \\[ \\mathbf u_3 = \\gamma_{31}\\mathbf v1+\\gamma_{32}\\mathbf v2+\\gamma_{33}\\mathbf v_3 \\] 用矩阵方程的形式表示为： \\[\\mathbf u = \\mathbf M \\mathbf v\\] 由于 \\(\\mathbf u\\), \\(\\mathbf v\\) 都是三维空间的基，因此，对于三维空间内任意一个向量 \\(\\mathbf w\\)，\\(\\mathbf u\\)、\\(\\mathbf v\\)都可以通过线性组合的方式表示出 \\(\\mathbf w\\)： \\(\\mathbf w = \\mathbf a^T \\mathbf v = \\mathbf b^T \\mathbf u\\)（这里的\\(\\mathbf a^T\\), \\(\\mathbf b^T\\)分别表示不同坐标空间的标量）。 结合前面 \\(\\mathbf u = \\mathbf M \\mathbf v\\)，进一步得到：\\(\\mathbf w = \\mathbf b^T \\mathbf u = \\mathbf b^T \\mathbf M \\mathbf v=\\mathbf a^T \\mathbf v\\)， 继而 ：\\(\\mathbf a = \\mathbf M^T \\mathbf b\\)，\\(\\mathbf b = (\\mathbf M^T)^{-1} \\mathbf a\\)。 好了，到这里，关键的东西就讲完了。所以坐标系统的变换很简单有木有！如果你在B坐标系（基向量为{\\(\\mathbf u_1\\), \\(\\mathbf u_2\\), \\(\\mathbf u_3\\)}）中有个向量 \\(\\mathbf w\\)，沿用上面的假设，\\(\\mathbf w\\) 的坐标为 \\(\\mathbf b\\)（即 \\(\\mathbf w = \\mathbf b^T \\mathbf u\\)），这个时候，我们想求出它在 A 坐标系（基向量为{\\(\\mathbf v_1, \\mathbf v_2, \\mathbf v_3\\)}）的坐标表示（假设为\\(\\mathbf a\\)），我们只需要求出矩阵 \\(\\mathbf M\\)，则：\\(\\mathbf a = \\mathbf M^T \\mathbf b\\)。 反之同理。 这个时候，有同学可能会问矩阵 \\(\\mathbf M\\) 怎么求？ 假设一个坐标系统的基向量为{\\(\\mathbf u_1\\), \\(\\mathbf u_2\\), \\(\\mathbf u_3\\)}，而另一个系统采用标准向量{\\(\\mathbf e_1\\), \\(\\mathbf e_2\\), \\(\\mathbf e_3\\)}，假设存在关系：\\(\\mathbf u = \\mathbf M^T \\mathbf e\\)（这个式子的理解是：如果 \\(\\mathbf M\\) 是两个坐标系统的变换矩阵，那么两个系统内的任意向量可以通过这个矩阵相互转换，基向量只不过是特殊的向量，一样可以通过 \\(\\mathbf M\\) 进行转换），那矩阵 \\(\\mathbf M\\) 其实可以表示为 [\\(\\mathbf u_1, \\mathbf u_2, \\mathbf u_3\\)]。这个结果其实很好理解，只要换种写法： \\(\\mathbf u = \\begin{bmatrix} \\mathbf u_1 \\\\ \\mathbf u_2 \\\\ \\mathbf u_3 \\\\ \\end{bmatrix}\\)，\\(\\mathbf e = \\begin{bmatrix} \\mathbf e_1 \\\\ \\mathbf e_2 \\\\ \\mathbf e_3 \\\\ \\end{bmatrix}\\)，可以发现，\\(\\mathbf e\\)其实是一个单位矩阵。 而如果是非标准坐标系统之间的变换，则需要解一个线性方程组：\\(\\mathbf u = \\mathbf M^T \\mathbf v\\)，而且可以肯定，这个解存在且唯一。 尽管从上面的推论中我们可以得出，不同坐标系统可以通过一个唯一的 3*3 矩阵 \\(\\mathbf M\\) 来变换，但都是基于坐标原点相同的前提。如果原点也发生变化，这时就必须引入第四个维度来表示平移的偏移量，也就是常说的齐次坐标。 引入第四维后，\\(\\mathbf u=[\\mathbf u_1,\\mathbf u_2,\\mathbf u_3,\\mathbf p]\\)，\\(\\mathbf v=[\\mathbf v_1,\\mathbf v_2,\\mathbf v_3,\\mathbf q]\\)，我们再次用一个矩阵 \\(\\mathbf M\\) 来转换这两个坐标系统，不同的是，这里的 \\(\\mathbf M\\) 是一个 4*4 的矩阵： \\[ \\mathbf M= \\begin{bmatrix} \\gamma_{11} &amp; \\gamma_{12} &amp; \\gamma_{13} &amp; 0 \\\\ \\gamma_{21} &amp; \\gamma_{22} &amp; \\gamma_{23} &amp; 0 \\\\ \\gamma_{31} &amp; \\gamma_{32} &amp; \\gamma_{33} &amp; 0 \\\\ \\gamma_{41} &amp; \\gamma_{42} &amp; \\gamma_{43} &amp; 1 \\\\ \\end{bmatrix} \\] \\[ \\mathbf u = \\mathbf M^T * \\mathbf v \\] 除了多出一维外，齐次坐标与上面使用的三维坐标本质上没有区别，计算方法也基本一致，在对应到三维坐标系时，只需要舍弃第四个维度即可。 OpenGL中的坐标系统 OpenGL 的坐标系统有六种： 1. Object (or model) coordinates 2. World coordinates 3. Eye (or camera) coordinates 4. Clip coordinates 5. Normalized device coordinates 6. Window (or screen) coordinates 模型坐标系 (Object coordinates) 是每个模型在制作时特有的，如果要把模型放入世界，就需要将所有模型的坐标系转换成世界坐标系 (World coordinates)。世界中的场景需要通过相机被人眼观察，需要将世界坐标系转换成相机坐标系 (Eye coordinates)。从模型坐标系，到世界坐标系，再到相机坐标系的变换，通常称为 model-view transformation，通过 model-view matrix 来实现。 前三种坐标系统通常是由用户指定的，而后三种坐标系统一般都是在 OpenGL 管道中，由程序自己实现的。 而 OpenGL 中坐标系统转换的原理，其实就是上面所讲的那些，只不过在使用时，我们可以使用一些 API 来简化不少工作。 参考 Interactive Computer Graphics - A Top-Down Approach 6e By Edward Angel and Dave Shreiner (Pearson, 2012)","raw":null,"content":null,"categories":[{"name":"计算机图形学","slug":"计算机图形学","permalink":"https://jermmy.github.io/categories/计算机图形学/"}],"tags":[{"name":"计算机图形学","slug":"计算机图形学","permalink":"https://jermmy.github.io/tags/计算机图形学/"},{"name":"线性代数","slug":"线性代数","permalink":"https://jermmy.github.io/tags/线性代数/"},{"name":"OpenGL","slug":"OpenGL","permalink":"https://jermmy.github.io/tags/OpenGL/"}]},{"title":"TensorFlow学习笔记：搭建CNN模型","slug":"2017-2-16-learn-tensorflow-build-cnn-model","date":"2017-02-16T15:18:54.000Z","updated":"2017-04-22T07:47:20.000Z","comments":true,"path":"2017/02/16/2017-2-16-learn-tensorflow-build-cnn-model/","link":"","permalink":"https://jermmy.github.io/2017/02/16/2017-2-16-learn-tensorflow-build-cnn-model/","excerpt":"最近跟着 Udacity 上的深度学习课程学了一丢丢 TensorFlow，这里记录一下用 TensorFlow搭建简单 CNN 网络的代码模板。","text":"最近跟着 Udacity 上的深度学习课程学了一丢丢 TensorFlow，这里记录一下用 TensorFlow搭建简单 CNN 网络的代码模板。 python代码 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495pickle_file = 'notMNIST.pickle'with open(pickle_file, 'rb') as f: save = pickle.load(f) train_dataset = save['train_dataset'] train_labels = save['train_labels'] valid_dataset = save['valid_dataset'] valid_labels = save['valid_labels'] test_dataset = save['test_dataset'] test_labels = save['test_labels'] del save # hint to help gc free up memory print('Training set', train_dataset.shape, train_labels.shape) print('Validation set', valid_dataset.shape, valid_labels.shape) print('Test set', test_dataset.shape, test_labels.shape) image_size = 28num_labels = 10num_channels = 1 # grayscalebatch_size = 16patch_size = 5depth = 16num_hidden = 64graph = tf.Graph()with graph.as_default(): # Input data. tf_train_dataset = tf.placeholder( tf.float32, shape=(batch_size, image_size, image_size, num_channels)) tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels)) tf_valid_dataset = tf.constant(valid_dataset) tf_test_dataset = tf.constant(test_dataset) # Variables. layer1_weights = tf.Variable(tf.truncated_normal([patch_size, patch_size, num_channels, depth], stddev=0.1)) layer1_biases = tf.Variable(tf.zeros([depth])) layer2_weights = tf.Variable(tf.truncated_normal([patch_size, patch_size, depth, depth], stddev=0.1)) layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth])) layer3_weights = tf.Variable( tf.truncated_normal([image_size // 4 * image_size // 4 * depth, num_hidden], stddev=0.1)) layer3_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden])) layer4_weights = tf.Variable(tf.truncated_normal([num_hidden, num_labels], stddev=0.1)) layer4_biases = tf.Variable(tf.constant(1.0, shape=[num_labels])) # Model. def model(data): conv1 = tf.nn.relu(tf.nn.conv2d(data, layer1_weights, [1, 1, 1, 1], padding='SAME') + layer1_biases) pool1 = tf.nn.max_pool(conv1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME') conv2 = tf.nn.relu(tf.nn.conv2d(pool1, layer2_weights, [1, 1, 1, 1], padding='SAME') + layer2_biases) pool2 = tf.nn.max_pool(conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME') shape = pool2.get_shape().as_list() reshape = tf.reshape(pool2, [shape[0], shape[1] * shape[2] * shape[3]]) fc1 = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases) return tf.matmul(fc1, layer4_weights) + layer4_biases # Training computation. logits = model(tf_train_dataset) loss = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels)) # Optimizer. optimizer = tf.train.GradientDescentOptimizer(0.05).minimize(loss) # Predictions for the training, validation, and test data. train_prediction = tf.nn.softmax(logits) valid_prediction = tf.nn.softmax(model(tf_valid_dataset)) test_prediction = tf.nn.softmax(model(tf_test_dataset))num_steps = 1001with tf.Session(graph=graph) as session: tf.initialize_all_variables().run() print('Initialized') for step in range(num_steps): offset = (step * batch_size) % (train_labels.shape[0] - batch_size) batch_data = train_dataset[offset:(offset + batch_size), :, :, :] batch_labels = train_labels[offset:(offset + batch_size), :] feed_dict = &#123;tf_train_dataset: batch_data, tf_train_labels: batch_labels&#125; _, l, predictions = session.run( [optimizer, loss, train_prediction], feed_dict=feed_dict) if (step % 50 == 0): print('Minibatch loss at step %d: %f' % (step, l)) print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels)) print('Validation accuracy: %.1f%%' % accuracy( valid_prediction.eval(), valid_labels)) print('Max pool Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels)) 卷积网络结构 conv &lt;br&gt;","raw":null,"content":null,"categories":[{"name":"TensorFlow","slug":"TensorFlow","permalink":"https://jermmy.github.io/categories/TensorFlow/"}],"tags":[{"name":"深度学习","slug":"深度学习","permalink":"https://jermmy.github.io/tags/深度学习/"},{"name":"TensorFlow","slug":"TensorFlow","permalink":"https://jermmy.github.io/tags/TensorFlow/"}]},{"title":"Bilateral Filter","slug":"2017-1-7-Bilateral-Filter","date":"2017-01-06T16:00:00.000Z","updated":"2018-02-21T09:50:19.000Z","comments":true,"path":"2017/01/07/2017-1-7-Bilateral-Filter/","link":"","permalink":"https://jermmy.github.io/2017/01/07/2017-1-7-Bilateral-Filter/","excerpt":"最近在看图像风格化的论文的时候，频繁遇到 Bilateral Filter。google 一波后，发现并不是什么不得了的东西，但它的思想却很有借鉴意义。\n简介\nBilateral Filter，中文又称「双边滤波器」。相比以往那些仅仅使用位置信息进行滤波的 filter，Bilateral Filter 还考虑了颜色信息，可以保证边缘部分不会被过滤。\n简单来说，一般的 filter 都是基于这样的公式进行滤波的： \\[\nh(x)=k_{d}^{-1}{(x)}\\iint_\\infty^\\infty{f(\\zeta)c(\\zeta, x)} d\\zeta\n\\]","text":"最近在看图像风格化的论文的时候，频繁遇到 Bilateral Filter。google 一波后，发现并不是什么不得了的东西，但它的思想却很有借鉴意义。 简介 Bilateral Filter，中文又称「双边滤波器」。相比以往那些仅仅使用位置信息进行滤波的 filter，Bilateral Filter 还考虑了颜色信息，可以保证边缘部分不会被过滤。 简单来说，一般的 filter 都是基于这样的公式进行滤波的： \\[ h(x)=k_{d}^{-1}{(x)}\\iint_\\infty^\\infty{f(\\zeta)c(\\zeta, x)} d\\zeta \\] 其中，\\(k_{d}^{-1}{(x)}\\) 是权重之和，\\(f(\\zeta)\\) 可以理解为单个像素，\\(c(\\zeta, x)\\) 可以理解为位置权重。 翻译成程序员可以理解的语言，大概是这样： 1234567for (int i = -r; i &lt;= r; i++) &#123; for (int j = -r; j &lt;= +r; j++) &#123; newpixel += pixel[row+i][col+j] * c[i][j]; k += c[i][j]; &#125;&#125;pixel[row][col] = newPixel / k; 高斯函数也属于这类 filter。 但这种 filter 有一个缺点：各向同性（不知道这个理解对不对）。用这种滤波器，每个点受邻居的影响是一样的，即使它跟邻居像素可能差得比较多，也会被邻居「同化」（举个例子：边缘被「和谐」掉了）。因此，有人提出了 Bilateral Filter。 Bilateral Filter 采用这样的公式： \\[ h(x)=k_{d}^{-1}{(x)}\\iint_\\infty^\\infty{f(\\zeta)c(\\zeta, x)s(f(\\zeta), f(x))} d\\zeta \\] 对比之前的式子，最大的变化无非是权值中增加了一个 \\(s(f(\\zeta), f(x))\\)，这个东西也是权值，不过它不是采用位置信息，而是颜色信息 \\(f(\\zeta)\\)。不管是哪种信息，形势上来看都是一样的，但由于增加了颜色权值，却使滤波的结果有了明显不同，后面会给出效果图。 再次翻译成程序语言： 1234567for (int i = -r; i &lt;= r; i++) &#123; for (int j = -r; j &lt;= +r; j++) &#123; newpixel += pixel[row+i][col+j] * c[i][j] * s(pixel[row][col], pixel[row+i][col+j]); k += c[i][j]*s(pixel[row][col], pixel[row+i][col+j]); &#125;&#125;pixel[row][col] = newPixel / k; s 函数可以借鉴位置权值的思路。例如，可以采用这种方式定义（当然这个是我自己构造的）： 123function s(p1, p2) &#123; return (255-abs(p1-p2)) / 255&#125; 这样，差的越多的颜色，所占权值越小。 如果要追求科学严谨一点，也不妨仿照高斯核函数的定义： \\[ c(\\zeta-x) = e^{-{1\\over2}({ {\\zeta-x} \\over {\\sigma} } )^2} \\\\\\\\\\\\ s(\\zeta-x) = e^{-{1\\over2}({ {f(\\zeta)-f(x)} \\over \\sigma })^2} \\] &lt;br&gt; 代码实现 理解原理后，实现其实也很简单，上面给出的伪代码基本是核心算法了。另外需要注意的是，如果是彩色图的话，需要对每个通道的颜色值进行滤波。 具体实现可以参考这篇博客：图像处理之双边滤波效果(Bilateral Filtering for Gray and Color Image)，或者参考我自己的 demo，当然，我也只是将上面博客的 java 版改成 c++ 而已0。 给出几幅结果图： 原图： 高斯模糊： 仅仅用颜色信息滤波： 双边滤波： 仔细对比一下，双边滤波对边缘的保留效果比高斯滤波好太多了，这一点从第三幅图就可以知晓缘由了。 另外！！如果使用高斯核函数来实现双边滤波，颜色卷积和的 \\(\\sigma\\) 要取大一点的值，比如：50。否则，由于不同颜色的差值往往比位置差值大出许多（举个例子：50 和 60 两种像素值肉眼上看很接近，但却差出 10，平方一下就是 100），可能导致很相近的像素点权值很小，最后跟没滤波的效果一样。 启发 Bilateral Filter 的思想是：在位置信息的基础上加上颜色信息，相当于考虑两个权值。如果还要考虑其他重要因素，是不是可以再加进一个权值，构成一个三边滤波器呢？答案当然是可以的，由此，我们可以把很多简单的滤波器综合起来形成一个更强大的滤波器。 参考 图像处理之双边滤波效果(Bilateral Filtering for Gray and Color Image) 双边滤波器","raw":null,"content":null,"categories":[{"name":"图像处理","slug":"图像处理","permalink":"https://jermmy.github.io/categories/图像处理/"}],"tags":[{"name":"图像处理","slug":"图像处理","permalink":"https://jermmy.github.io/tags/图像处理/"},{"name":"NPR","slug":"NPR","permalink":"https://jermmy.github.io/tags/NPR/"}]},{"title":"numpy常用接口","slug":"2017-1-4-numpy常用接口","date":"2017-01-04T14:27:03.000Z","updated":"2017-09-06T13:13:19.000Z","comments":true,"path":"2017/01/04/2017-1-4-numpy常用接口/","link":"","permalink":"https://jermmy.github.io/2017/01/04/2017-1-4-numpy常用接口/","excerpt":"numpy.ndarray\n\nnumpy.ndarray(shape, dtype=None, buffer=None, offset=0, strides=None, order=None)\n\nThis is an array object represents a multidimensional array of fixed-size items（Fixed-size 意味着数组一旦创建就没法修改了）。\n\nshape: tuple of ints（除了该参数必须之外，其余为可选）；\ndtype: data-type\nbuffer: object exposing buffer interface, used to fill the array with data.\noffset: int, offset of array data in buffer.\nstrides: tuple of ints, strides of data in memory.\norder: {‘C’, ‘F’}, row-major (C-style) or column-major (Fortran-style).\n\n注：\n\n如果 buffer 是 None，则只有 Shape、dtype、 order 是需要的。\n如果 buffer 不是 None，则所有参数都会用到。\nbuffer 的 dtype 和 ndarray 的 dtype 要保持一致。\n\n1234&gt;&gt;&gt; m = np.ndarray(shape=(2, 3), dtype=np.int32, buffer=np.array([1,2,3,4,5,6], dtype=np.int32))&gt;&gt;&gt; m[[1 2 3] [4 5 6]]","text":"numpy.ndarray numpy.ndarray(shape, dtype=None, buffer=None, offset=0, strides=None, order=None) This is an array object represents a multidimensional array of fixed-size items（Fixed-size 意味着数组一旦创建就没法修改了）。 shape: tuple of ints（除了该参数必须之外，其余为可选）； dtype: data-type buffer: object exposing buffer interface, used to fill the array with data. offset: int, offset of array data in buffer. strides: tuple of ints, strides of data in memory. order: {‘C’, ‘F’}, row-major (C-style) or column-major (Fortran-style). 注： 如果 buffer 是 None，则只有 Shape、dtype、 order 是需要的。 如果 buffer 不是 None，则所有参数都会用到。 buffer 的 dtype 和 ndarray 的 dtype 要保持一致。 1234&gt;&gt;&gt; m = np.ndarray(shape=(2, 3), dtype=np.int32, buffer=np.array([1,2,3,4,5,6], dtype=np.int32))&gt;&gt;&gt; m[[1 2 3] [4 5 6]] numpy.array numpy.array(object, dtype=None, copy=True, order=‘K’, subok=False, ndmin=0) 这是 numpy 创建数组最常用的方式了，object 通常传入一个 list 数组，可以是一维或者多维。 e.g. 1234567&gt;&gt;&gt; np.array([1, 2, 3.0])array([ 1., 2., 3.])&gt;&gt;&gt; np.array([[1, 2], [3, 4]])array([[1, 2], [3, 4]])&gt;&gt;&gt; np.array([1, 2, 3], dtype=complex)array([ 1.+0.j, 2.+0.j, 3.+0.j]) numpy.allclose() numpy.allclose(a, b, rtol=1e-05, atol=1e-08, equal_nan=False) Returns True if two arrays are element-wise equal within a tolerance. numpy.arange() numpy.arange([start,] stop, [step, ] dtype=None) Returns an ndarray in range [start, stop) e.g. 12345678&gt;&gt;&gt; np.arange(3)array([0, 1, 2])&gt;&gt;&gt; np.arange(3.0)array([ 0., 1., 2.])&gt;&gt;&gt; np.arange(3,7)array([3, 4, 5, 6])&gt;&gt;&gt; np.arange(3,7,2)array([3, 5]) 与python内置的range和xrange的比较： range()多用于循环，会返回一个list。xrange()也多用于循环，但不返回list，而是类似generator，每次返回一个元素，因此内存开销更小。另外，python3 已经将range()的实现改成xrange()的方式，而xrange()这个接口则已经被取消了。 numpy.asmatrix() numpy.asmatrix(data, dtype=None) Interprete the input as a matrix, does not make a copy if the input is already a matrix or an ndarray. Returns numpy matrix. 123456&gt;&gt;&gt; x = np.array([[1, 2], [3, 4]])&gt;&gt;&gt; m = np.asmatrix(x)&gt;&gt;&gt; x[0,0] = 5&gt;&gt;&gt; mmatrix([[5, 2], [3, 4]]) numpy.random.permutation numpy.random.permutation(x) Randomly permute a sequence, or return a permuted range. Parameters x: int or array_like. If x is an integer, randomly permute np.arange(x). If x is an array, make a copy and shuffle the elements randomly. Returns out: ndarray. Permuted sequence or array range. 123&gt;&gt;&gt; permutation = np.random.permutation(np.array((1,2,3,4,5)))&gt;&gt;&gt; permutationarray([2, 4, 3, 5, 1]) numpy 生成序列中的随机子序列 可以借助 numpy.random.choice 接口。 例如，要从 0 ～ 5 的数字中随机挑出 3 个数字，可以这样写： 12&gt;&gt;&gt; np.random.choice(5, 3) # 等概率随机挑选array([0, 3, 4]) 还可以根据不同的概率挑选： 12&gt;&gt;&gt; np.random.choice(5, 3, p=[0.1, 0, 0.3, 0.6, 0]) # 3 的概率最大，为0.6array([3, 3, 0]) 可以设置不放回地挑选： 12&gt;&gt;&gt; np.random.choice(5, 3, replace=False, p=[0.1, 0, 0.3, 0.6, 0])array([2, 3, 0]) # 数字不会重复 当然，除了数字外，我们可以对其他类型的列表进行随机挑选： 1234&gt;&gt;&gt; aa_milne_arr = ['pooh', 'rabbit', 'piglet', 'Christopher']&gt;&gt;&gt; np.random.choice(aa_milne_arr, 5, p=[0.5, 0.1, 0.1, 0.3])array(['pooh', 'pooh', 'pooh', 'Christopher', 'piglet'], dtype='|S11') numpy.argmax numpy.argmax(a, axis=None, out=None) 返回数组中最大值的下标 参数 a：数组 axis：比较的坐标系或者维度，如果没有提供，则对数组中的所有元素进行比较。 返回一个下标数组 e.g. 123456789101112131415161718&gt;&gt;&gt; a = np.arange(6).reshape(2,3)&gt;&gt;&gt; aarray([[0, 1, 2], [3, 4, 5]])&gt;&gt;&gt; np.argmax(a) # 比较所有元素5&gt;&gt;&gt; np.argmax(a, axis=0) # 比较列array([1, 1, 1])&gt;&gt;&gt; np.argmax(a, axis=1) # 比较行array([2, 2])&gt;&gt;&gt; b = np.arange(6)&gt;&gt;&gt; b[1] = 5&gt;&gt;&gt; barray([0, 5, 2, 3, 4, 5])&gt;&gt;&gt; np.argmax(b) # Only the first occurrence is returned.1 numpy.where numpy .where(condition, x, y) 这个函数会根据 condition，返回合适的下标。 e.g. 1234&gt;&gt;&gt; a = np.array([1,2,3,4])&gt;&gt;&gt; b = np.array([2,1,4,5])&gt;&gt;&gt; np.where(a &gt; b)(array([1]),) # 只有a[1]&gt;b[1]","raw":null,"content":null,"categories":[{"name":"Python","slug":"Python","permalink":"https://jermmy.github.io/categories/Python/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://jermmy.github.io/tags/Python/"}]},{"title":"Mac下的常用命令","slug":"2016-12-15-Mac下的常用命令","date":"2016-12-15T03:45:20.000Z","updated":"2017-01-08T03:09:28.000Z","comments":true,"path":"2016/12/15/2016-12-15-Mac下的常用命令/","link":"","permalink":"https://jermmy.github.io/2016/12/15/2016-12-15-Mac下的常用命令/","excerpt":"","text":"Mac常用的命令行工具： 1. which 查看某个命令行工具的位置，比如： 1which node","raw":null,"content":null,"categories":[{"name":"Mac","slug":"Mac","permalink":"https://jermmy.github.io/categories/Mac/"}],"tags":[{"name":"Mac","slug":"Mac","permalink":"https://jermmy.github.io/tags/Mac/"},{"name":"工具","slug":"工具","permalink":"https://jermmy.github.io/tags/工具/"}]},{"title":"计算机系统","slug":"2016-11-23-深入理解计算机系统","date":"2016-11-23T08:16:43.000Z","updated":"2017-04-23T05:40:27.000Z","comments":true,"path":"2016/11/23/2016-11-23-深入理解计算机系统/","link":"","permalink":"https://jermmy.github.io/2016/11/23/2016-11-23-深入理解计算机系统/","excerpt":"RAM &amp; ROM\nRAM(Read Access Memory)和ROM(Read Only Memory)是很多小白感到困惑的东西，只知道跟主存有关，傻傻分不清。\nRAM分为SRAM和DRAM。DRAM就是通常用作主存的设备，速度快于普通硬盘，访问速度以ns计算，目前已经可以到几十ns。SRAM速度更快，价格更贵，所以通常用作缓存(Cache，一种读写速度比主存更快的设备，用于缓解CPU与主存之间的速度差异，CPU指令周期低于1个ns)。这两种设备都是靠电压驱动的，断电就无法保存数据。\nROM也是一种访问速度很快的设备，它与RAM的区别是，ROM在断电的情况下仍然可以保存数据。ROM分很多类型，比如PROM（可编程的ROM，只能编写一次）、EPROM（可编程1000次）。\nFlash memory是基于EPROM的一种重要存储设备，在手机、电子照相机等设备中大量使用。SSD（固态硬盘）也是基于Flash memory的。\nROM设备中存储的程序通常称为固件（firmware）。计算机启动的时候，会先执行ROM中的固件程序。有些系统会在固件中提供了一系列读写函数（比如BIOS）。","text":"RAM &amp; ROM RAM(Read Access Memory)和ROM(Read Only Memory)是很多小白感到困惑的东西，只知道跟主存有关，傻傻分不清。 RAM分为SRAM和DRAM。DRAM就是通常用作主存的设备，速度快于普通硬盘，访问速度以ns计算，目前已经可以到几十ns。SRAM速度更快，价格更贵，所以通常用作缓存(Cache，一种读写速度比主存更快的设备，用于缓解CPU与主存之间的速度差异，CPU指令周期低于1个ns)。这两种设备都是靠电压驱动的，断电就无法保存数据。 ROM也是一种访问速度很快的设备，它与RAM的区别是，ROM在断电的情况下仍然可以保存数据。ROM分很多类型，比如PROM（可编程的ROM，只能编写一次）、EPROM（可编程1000次）。 Flash memory是基于EPROM的一种重要存储设备，在手机、电子照相机等设备中大量使用。SSD（固态硬盘）也是基于Flash memory的。 ROM设备中存储的程序通常称为固件（firmware）。计算机启动的时候，会先执行ROM中的固件程序。有些系统会在固件中提供了一系列读写函数（比如BIOS）。 &lt;br&gt; GCC工作流程 gcc从编译源代码到生成可执行程序，要经过四个步骤： C preprocessor（cpp）预处理源文件（main.c），生成中间文件（main.i，ASCII字符）：这一过程中，cpp程序会替换文件中定义的宏，并将头文件拷贝到源文件中，最后生成中间文件（main.i）； C compiler（cc1）编译（main.i）生成汇编代码（main.s）； Assembler（as）编译（main.s）生成目标文件（main.o，relocatable object file） Linker program（ld）链接所有的目标文件，生成可执行程序。 &lt;br&gt; Static Linking 静态链接器（Static linker）会将可重定位的目标文件（relocatable object file）链接起来生成可执行文件。这个过程中它要执行两个很重要的任务： 符号表（Symbol resolution）：目标文件中可能有符号引用了其他目标文件定义的内容，链接器要将符号的引用串接起来，并保证定义的唯一性； 重定位（Relocation）：编译器和汇编器生成的代码段和数据段的位置都是从0开始的，链接器需要将这些目标文件组合起来，并重新生成地址。事实上，目标文件可以看作是字节块的集合，Linker只是把这些块重新编排，并分配新的地址给它们。 &lt;br&gt; Object Files 目标文件分为三种类型： 可重定位的目标文件（Relocatable object file）：包括二进制码和数据，可能被链接器链接起来生成可执行程序； 可执行的目标文件（Executable object file）：包括二进制码和数据，可以直接载入内存并执行； 可共享的目标文件（Shared object file）：一种特殊的relocatable object file，可以在加载或运行时被链接到程序中。 Compiler和assembler生成的是第一和第三种文件。 不同系统生成的目标文件的格式是不同的，例如：Linux使用的是ELF格式的目标文件。 &lt;br&gt; Relocatable Object Files 这里以ELF格式为例，说明目标文件包含哪些内容： obj .text：机器码； .rodata：程序中的只读数据； .data：已经初始化的全局变量（c语言）； .bss：未初始化的全局变量，它们在目标文件中没有分配空间，仅仅是一个占位符； .symtab：目标文件中引用到的函数和全局变量； .rel.text：一个包含.text段的地址列表，这些地址在Linker组合目标文件的时候是需要修改的，所以事实上这些地址一般是指向一些外部函数或全局变量； .debug：调试需要的信息，用-g命令生成； .line：调试需要的信息，用-g命令生成； .strtab：字符串符号表，可以认为是.symtab和.debug的辅助表。 Symbols and Symbol Table 每个目标文件（relocatable object file）中都包含一个符号表，记录该文件定义和引用的符号，包括函数和一下变量。具体来说，包括以下两类： Global symbols：目标文件中定义的函数和全局变量（在c里面没有static修饰的符号），目标文件引用的在其他目标文件中定义的函数和全局变量； Local symbols：目标文件中定义的仅在该文件中使用的函数和变量（在c里面用static修饰的符号）。 需要注意的是，local symbols中不包括程序中的局部变量，这些局部变量是在运行的时候在stack中生成的，而symbols中记录的变量是编译时在目标文件的.data或.bss段中生成的。对于前者，编译器可以保证符号名称的唯一性，对于后者，Linker需要保证其在所有目标文件中的唯一性。 &lt;br&gt; Linking with Static Libraries 所谓静态链接库，就是事先将一些目标文件压缩打包而成的*.a文件。在链接的时候，Linker会根据应用程序代码中的引用，把静态链接库中需要用到的object modules拷贝到最终的可执行程序中。 可以用ar工具生成静态链接库，例如： 1unix&gt; ar rcs lib.a lib1.o lib2.o 然后在链接的时候将它们引入： 1unix&gt; gcc -static -o main main.o lib.a Linker在解决链接时的符号引用问题时，采用从左向右扫描的方法，如果扫描一个目标文件时，发现一个未定义的符号，就记录下来（保存到未定义符号集），在之后扫描到的目标文件中查找，找到该符号则将其从未定义符号集中删除，并将对应的目标文件放入目标文件集合。 这种方法需要保证目标文件的输入顺序，如果main.c引用了lib.a中的符号，但在编译的时候，把lib.a放在了main.c前面，这样链接将会出错。 &lt;br&gt; Dynamic Linking with Shared Libraries 动态链接库链接到程序中的方法有两种： 程序被载入内存时，由loader控制dynamic linker加载动态链接库； 在程序运行的时候，由程序自己调用dynamic linker加载并链接库，例如：Linux提供了接口来执行这样的操作 1void *dlopen(const char *filename, int flag); JNI的工作原理与这种思路很类似。 &lt;br&gt; Unix I/O 在unix中，文件就是一个字节序列。 所有的I/O设备，例如：网络、硬盘、终端，都被当作是文件模型。所以，所有的输入输出都被当作是对特定文件的读写（比如键盘输入到终端）。 每次应用程序打开文件的时候，kernel会返回一个descriptor，应用程序根据这个descriptor来跟踪文件状态，而文件的所有信息都是由kernel维护的（比如：改变当前文件的读写偏移位置，关闭文件等）。 共享文件 Unix内核通过三种数据结构来表示打开的文件，从而实现文件共享的目的： Descriptor table：由每个进程单独维护，表中的每个子项指向file table中的一个子项； File table：打开的文件的集合，由所有进程共享。每个子项包含当前文件的偏移位置、引用数目等。如果一个descriptor指向某个File table的子项，这个子项的引用数目会增加1，当引用数目为0的时候，内核会关闭该文件。同时，每个子项也包含一个指向v-node table子项的指针； v-node table：由所有进程共享，每个子项包含文件的绝大部分信息。 具体的，通过三幅图了解其工作原理： noshare 上图中，进程的descriptor table中，fd1和fd4指向两个不同的子项（File table），这两个子项指向两个不同的文件，所以不存在共享。 share 上图中，fd1和fd4同样指向两个子项，但这两个子项却指向同一个v-node table的子项，所以它们实际上使用了同一个文件。由于file table的两个子项维护两个File pos，所以fd1和fd4在读文件的时候是“隔离”的。 process 上图是父进程和子进程的文件共享模型，它们使用的是相同的文件，包括File pos。 &lt;br&gt; Virtual Memory 虚拟内存是位于disk上面的用于模拟物理内存的空间。早期计算机没有虚拟内存，CPU直接通过物理内存地址访问Main Memory。有了虚拟内存后，CPU只拥有Virtual address(VA)，先通过MMU转换成物理内存，再访问Main Memory。 （使用VM） vm （没有使用VM） &lt;br&gt; VM as Tool for Caching vm副本 2 如图，虚拟内存和物理内存都是将空间分成block进行管理的。VM中的块称为virtual page，物理内存上的块称为physical page或page frame。 virtual page有三种类型： Unallocated: 尚未分配空间的虚拟内存； Cached: 已经分配空间且映射到物理内存的页； Uncached: 已经分配空间但还没有映射到物理内存的页（物理内存不够用），当CPU需要访问这块内存时，需要进行页调度。 由于disk跟DRAM的读写速度存在巨大差异，所以VM的主要工作之一是减少页调度。通常来说，virtual page的大小为4KB到2KB（物理内存跟它一致）。 &lt;br&gt; Page Tables Page Tables是MMU的辅助工具，简单来讲，它是用来判断virtual page的状态的。 vm副本 3 Page tables主要包括两个信息：valid bit，n-bit address。前者用来表示这块virtual page是否已经缓存到物理内存，后者表示这块virtual page在disk上对应的空间位置（即用户是否拥有使用权）。 Page tables在CPU寻址的时候发挥作用。CPU访问一块virtual page，通常有三种状态： Page Hits: 这块vp刚好在DRAM中，直接引用； Page Faults: 这块vp不在DRAM中，但用户已经在vm上分配了空间。这时发生页调度算法，在vm上找到这块地址空间，将它拷贝到DRAM，如果DRAM没有位置了，需要置换出一个frame，将这块frame写回vm后，再将新的vp写进去。最后要更新Page tables； Allocating Pages: 这个动作通常是malloc等系统调用导致的。先在vm上分配virtual pages，接下来的步骤跟Page Faults是一致。 要知道，程序的内存调优基本上就是为了防止频繁的页调度。频繁的页调度又称为“抖动”，它受限于disk的读取速度。 &lt;br&gt; VM as a Tool for Memory Management 事实上，操作系统为每个进程单独分配一张Page table，这么做的好处是，对于每个进程而言，它们的内存地址空间可以看作是从0开始的，至于具体对应哪一块物理地址，由MMU决定。 vm副本 4 这种设计有多种好处： 简化linking： 之前学linking的时候，提到linking就是将obj链接起来，为符号分配新的地址，这里的地址其实是相对地址，对于Linux而言，每个进程的起始地址（virtual address）都是一样的（例如：32bit系统是0x08048000），因此只要以这个地址为起点，计算往后的偏移地址，就知道程序运行时真正对应的地址是什么。而这个相同的起始地址其实就是每个进程的virtual address space的起始位置，对应到具体的物理内存肯定是不一样的（参见上图）。这样的工作大大简化了linker的设计。 简化loading： 加载程序的时候，会为目标文件中的变量（.text、.data）分配空间，但实际上，初次加载的时候，系统只是在disk的vm上为这些section分配一个virtual pages（没有复制内容，仅仅分配一大块空间，因此速度较快），然后在进程的Page tables中将这些vp的valid bit设为0（not cached），并将address指向obj files中的真正的section， 只有当程序真正引用到它们时，才会将它们载入内存。 注：这种将连续的vp跟任意文件的任意地址映射起来的技术称为memory mapping。Unix提供了mmap系统调用给开发人员使用。 简化了sharing： 如果两个进程都调用了系统函数，不必要将系统函数的代码都复制到进程空间，而是将它们的vp都指向相同的磁盘空间（空间上有相应的内核函数代码）即可。 简化了memory allocation： 当程序想分配新的heap空间时，系统可以分配连续的vps给进程（对应到进程的page tables），然后将这些vps映射到物理内存，这样就不用管在物理内存上如何分配连续空间了。 &lt;br&gt; VM as a Tool for Memory Protection 虚拟内存机制可以控制vp的访问权限，通过在PTE（Page table entry）中加入权限标识位来实现。 vm副本 5 例如：如果SUP标识位为NO，就表示这个vp内的程序没法访问内核代码。 通过这种方法，可以防止程序在运行时修改代码，或者访问没有权限的区域。","raw":null,"content":null,"categories":[{"name":"计算机系统","slug":"计算机系统","permalink":"https://jermmy.github.io/categories/计算机系统/"}],"tags":[{"name":"计算机系统","slug":"计算机系统","permalink":"https://jermmy.github.io/tags/计算机系统/"}]},{"title":"Thinking in Java — 类型信息","slug":"2016-11-1-Thinking-In-Java-类型信息","date":"2016-11-01T13:33:26.000Z","updated":"2017-03-25T01:32:07.000Z","comments":true,"path":"2016/11/01/2016-11-1-Thinking-In-Java-类型信息/","link":"","permalink":"https://jermmy.github.io/2016/11/01/2016-11-1-Thinking-In-Java-类型信息/","excerpt":"这一章将讨论Java是如何让我们在运行时识别对象和类的信息的。主要有两种方式：1、“传统的”RTTI，它假定我们在编译时已经知道了所有的类型；2、“反射”机制，它允许我们在运行时发现和使用类的信息。","text":"这一章将讨论Java是如何让我们在运行时识别对象和类的信息的。主要有两种方式：1、“传统的”RTTI，它假定我们在编译时已经知道了所有的类型；2、“反射”机制，它允许我们在运行时发现和使用类的信息。 Class对象 要理解RTTI在java中的工作原理，首先必须知道类型信息在运行时是如何表示的。这项工作由称为Class的特殊对象完成。事实上，Class对象就是用来创建类的所有的“常规”对象。 类是程序的一部分，每个类都有一个Class对象。换言之，每当编写一个新类，就会产生一个Class对象（保存在.class文件）。这个工作由被称为“类加载器”（ClassLoader）的子系统完成。 类加载器子系统实际上包含一条类加载器链，但是只有一个原生类加载器，它是JVM实现的一部分。原声类加载器加载所谓的：可信类：，包括Java API类。 所有的类都是在对其第一次使用时，动态加载到JVM的。当程序创建第一个对类的静态成员的引用时，就会加载这个类，这说明构造器也是类的静态方法。 因此，java程序在开始运行之前并非被完全加载，而是按需加载。类加载器首先检查这个类的Class对象是否已经加载。如果尚未加载，默认的类加载器就会根据类名查找.class文件。这个类的字节码被加载时，会接受验证，以确保其没有被破坏，并且没有包括不良Java代码。 &lt;br&gt; Class提供的常用API 方法 说明 Class.forName(&quot;name&quot;) 获得某个类的Class对象的引用 getName() 获得该Class对象的完整名称 getSimpleName() 获得该Class对象的名称（不包括包名） isInterface() 是否是接口 getInterfaces() 获得该Class实现的所有接口的Class对象 getSuperClass() 获得该Class继承的父类的Class对象 &lt;br&gt; 使用类字面常量 Java提供了另一种方法来生成对Class对象的引用，即.class。这样做比使用forName()更加安全，因为它会在编译期受到检查，故不必使用try语句。 为了使用类而做的准备工作实际包含三步： 加载。这是由类加载器完成的。该步骤将查找字节码，并创建一个Class对象； 链接。验证类中的字节码，为静态域分配存储空间。如果必须的话，将解析这个类创建的对其他类的引用； 初始化。如果该类具有超类，则对其初始化。执行静态初始化器和静态初始化块。 有趣的是，使用.class来创建对Class的引用时，不会自动初始化Class对象。 1234567891011121314151617181920212223242526272829303132333435363738394041import java.util.*;class Initable &#123; static final int staticFinal = 47; static final int staticFinal2 = ClassInitialization.rand.nextInt(1000); static &#123; System.out.println(\"Initializing Initable\"); &#125;&#125;class Initable2 &#123; static int staticNonFinal = 147; static &#123; System.out.println(\"Initializing Initable2\"); &#125;&#125;class Initable3 &#123; static int staticNonFinal = 74; static &#123; System.out.println(\"Initializing Initable3\"); &#125;&#125;public class ClassInitialization &#123; public static Random rand = new Random(47); public static void main(String[] args) throws Exception &#123; Class initable = Initable.class; System.out.println(\"After creating Initable ref\"); System.out.println(Initable.staticFinal); System.out.println(Initable.staticFinal2); System.out.println(Initable2.staticNonFinal); Class initable3 = Class.forName(\"Initable3\"); System.out.println(\"After creating Initable3 ref\"); System.out.println(Initable3.staticNonFinal); &#125;&#125; 输出： 123456789After creating Initable ref47Initializing Initable258Initializing Initable2147Initializing Initable3After creating Initable3 ref74 上面这个例子需要注意两点： 在使用Initable.class获得Initable的Class的引用时，并没有输出static静态代码区的内容，也就是说此时Initable类并没有加载； 输出Initable.staticFinal时，静态代码段同样没有执行，因为这个变量是个“编译期常量”，不需要初始化类就可以加载。但输出Initable.staticFinal2时则执行了加载操作，因为这不是一个编译期常量。 &lt;br&gt; 类型转换前先做检查 迄今为止，我们已知的RTTI形式包括： 传统的类型转换，如果执行了一个错误的类型转换，就会抛出一个ClassCastException异常； 代表对象的类型的Class对象。通过查询Class对象可以获取运行时所需的信息。 &lt;br&gt; 反射：运行时的类信息 Java的Class类和java.lang.reflect类库一起对反射的概念进行了支持，该类库包含了Field、Method以及Constructor类（每个类都实现了Member接口）。这些类型的对象是由JVM在运行时创建的，用以表示未知类里对应的成员。 当通过反射与一个未知类型的对象打交道时，JVM只是简单地检查这个对象，看它属于哪个特定的类（就像RTTI）。在用它做其他事情之前必须先加载那个类的Class对象。因此，那个类的.class文件对于JVM来说必须是可获取的：要么在本地机器上，要么可以通过网络取得。所以RTTI和反射之间真正的区别只在于：对RTTI来说，编译器在编译时打开和检查.class文件；对于反射机制来说，.class文件在编译时是不可获取的，所以在运行时打开和检查.class文件。","raw":null,"content":null,"categories":[{"name":"Java","slug":"Java","permalink":"https://jermmy.github.io/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://jermmy.github.io/tags/Java/"}]},{"title":"Android：在SurfaceView上做放大镜效果","slug":"2016-10-23-Android-SurfaceView做放大镜效果","date":"2016-10-23T13:40:17.000Z","updated":"2016-10-24T03:58:20.000Z","comments":true,"path":"2016/10/23/2016-10-23-Android-SurfaceView做放大镜效果/","link":"","permalink":"https://jermmy.github.io/2016/10/23/2016-10-23-Android-SurfaceView做放大镜效果/","excerpt":"一开始遇到这个需求的时候，觉得应该是一个再普通不过的功能，于是上网查了下怎么实现放大镜效果。果不其然，很快就google出一堆如何在ImageView或者其他View上实现放大镜的方法，但当我把同样的思路用在SurfaceView上时，却遇到一个极坑的问题。于是特意写这篇文章记录实现的思路。\n简单起见，我们要实现的是下图展示的功能，当手指触及SurfaceView时，放大手指所指的位置，放大镜出现在手指左上方。\n\nezgif.com-video-to-gif\n\n预备知识\n\nAndroid canvas基本用法，推荐这篇文章：\nhttp://blog.csdn.net/harvic880925/article/details/39080931\nSurfaceView的初级使用\n","text":"一开始遇到这个需求的时候，觉得应该是一个再普通不过的功能，于是上网查了下怎么实现放大镜效果。果不其然，很快就google出一堆如何在ImageView或者其他View上实现放大镜的方法，但当我把同样的思路用在SurfaceView上时，却遇到一个极坑的问题。于是特意写这篇文章记录实现的思路。 简单起见，我们要实现的是下图展示的功能，当手指触及SurfaceView时，放大手指所指的位置，放大镜出现在手指左上方。 ezgif.com-video-to-gif 预备知识 Android canvas基本用法，推荐这篇文章： http://blog.csdn.net/harvic880925/article/details/39080931 SurfaceView的初级使用 &lt;br&gt; 普通View的实现思路 SurfaceView放大镜的实现思路和普通View基本一样，所以有必要了解普通View的放大镜如何实现。 其实归根到底是Canvas的作用。 思路是这样的：当用户手指触碰到View时，捕获指尖位置(onTouch()方法)，然后用Canvas在该位置左上角裁减出一个圆形区域作为放大镜的位置，在该位置画出放大后的图片。当用户移动手指时，就不断刷新View（通过invalidate()方法调用onDraw()），这样就实现了放大镜效果。 接下来细化每一个细节问题。 如何裁出那个圆？ Canvas表示一个图层，在这个图层上可以进行任意的平移旋转等操作，同时可以通过clipXXX()等方面裁减这个图层。因此，我们可以事先定义好一个圆形的Path，并通过clipPath()方法在指尖左上角的位置裁出一个圆形区域。 当然，在这之前，你要把Canvas移动到裁减的位置。Canvas的操作默认都是以(0,0)坐标为起点执行的，对应到手机UI的坐标系，也就是屏幕左上角。而移动的操作可以通过translate()函数来完成。 为了方便理解，我简陋地做了几张图： 屏幕快照 2016-10-24 上午10.29.35 假设上图中，绿色部分代表View，图中那个红点是用户指尖的位置。 接下来，我们要平移Canvas到合适的位置，并裁剪出放大镜的区域。 屏幕快照 2016-10-24 上午10.33.27 上面这张图，假设带虚线的绿色框是移动后的Canvas，至于为什么要移动到这个位置，跟我的Path的设置有关： 12mPath = new Path();mPath.addCircle(RADIUS, RADIUS, RADIUS, Path.Direction.CW); 如果以（0，0）点作为标准，这个Path会以（RADIUS，RADIUS）这个点为圆心，以RADIUS为半径形成一个圆。因此，如果Canvas移动到上图的位置，Path对应的就是那个蓝色圆的位置。 接下来，用Canvas裁剪出这个Path， 1canvas.clipPath(mPath); 这个时候，Canvas会在图层上裁出图中那个蓝色圆。也就是说，下次做画的时候，只有那个蓝色圆的位置会被绘制。 如何制作放大效果？ 这一步使用了Matrix的作用。简单来讲，只要我们将Bitmap绘制成n倍大小，同时保证蓝色圆的圆心与红点对应原Bitmap同一位置即可。后一步保证放大的区域确实是手指触碰的区域。 简单起见，这里以放大两倍为例。 假设Canvas仍然在（0，0）位置，那么放大两倍后的Canvas就如下面的蓝色区域所示，注意原来的红点坐标也被放大了： 屏幕快照 2016-10-24 上午10.52.59 需要注意的是，裁减出来的那个蓝色圆是不会动的（我觉得这个API的设计有点奇怪）。 接下来要做的就是让蓝色区域的红点和蓝色圆的圆心重合，这样当Canvas绘制放大两倍的Bitmap的时候，就相当于把指尖位置的区域放大后，再画到蓝色圆的区域，也就是放大镜的效果。 移动的操作其实很简单，按照上图的标示，横坐标要左移x+RADIUS，纵坐标上移y+RADIUS，换成向量表示就是translate(-x-RADIUS, -y-RADIUS)。 但要注意，我们之前已经平移过Canvas了，所以要把之前平移的距离算上（如下图所示） 屏幕快照 2016-10-24 上午11.05.18 所以最后总的平移向量为：translate(-2*x+RADIUS, -2*y+RADIUS)。 onDraw()函数如下： 123456789101112protected void onDraw(Canvas canvas) &#123; canvas.drawBitmap(bitmap, matrix, null); if (isTouching) &#123; // 剪切出放大区域 canvas.translate(mX - 2 * RADIUS, mY - 2 * RADIUS); canvas.clipPath(mPath); // 画放大后的图 canvas.translate(RADIUS - mX * 2, RADIUS - mY * 2); canvas.drawBitmap(bitmap, scaleMatrix, null); &#125;&#125; &lt;br&gt; SurfaceView的放大镜实现 完成了普通View的放大镜效果后，SurfaceView照理来说应该也就不是问题，毕竟SurfaceView也是View的子类。但真正实现的时候，却遇到一个很大的问题。 仔细观察上面onDraw()的代码，可以发现，放大效果最终是通过Matrix将Bitmap放大后再重绘一遍。但应用到SurfaceView时，我发现根本无法拿到SurfaceView的Bitmap。后来查了各种资料，发现大家普遍遇到这个问题，有人甚至通过Linux底层的驱动来获取这个SurfaceView的Frame，不过这要在手机root的前提下实现。后来我尝试通过截屏的思路获取SurfaceView的截图，却发现截图是一片漆黑。导致该问题的根本原因在于SurfaceView的实现机制与普通的View完全是两码事。于是，只能另辟蹊径去获得这个Bitmap。思路其实也很简单，我们自己实例化一个Canvas和Bitmap，将SurfaceView上绘制的结果重新画一遍，这样就相当于间接获得了SurfaceView的Bitmap，绘制函数的代码如下： 12345678910111213141516171819202122232425262728 private void draw() &#123; try &#123; Canvas canvas = surfaceHolder.lockCanvas(); // 画一层底色,防止SurfaceView闪烁 canvas.drawColor(Color.BLACK); if (bitmap != null) &#123; canvas.drawBitmap(bitmap, matrix, null); &#125; if (isTouching) &#123; // 将SurfaceView上的结果在自己的Bitmap上重新画一遍 drawSurfaceToBitmap(); // 剪切出放大区域 canvas.translate(mX - 2 * RADIUS, mY - 2 * RADIUS); canvas.clipPath(mPath); // 画放大后的图 canvas.translate(RADIUS - mX * 2, RADIUS - mY * 2); // magBitmap是我们自己的Bitmap canvas.drawBitmap(magBitmap, magMatrix, null); &#125; surfaceHolder.unlockCanvasAndPost(canvas); &#125; catch (NullPointerException e) &#123; e.printStackTrace(); &#125;&#125; 当然，由于这个例子里绘制的图片是一张静态的背景图，所以可以直接使用这张背景图的Bitmap，但如果绘制的是一张动图，就只能将所有绘制的操作重新画在自己的Bitmap上了。 另外，如果SurfaceView的绘制流程过于复杂，可能会导致不流畅（毕竟画了两次），需要使用多线程来执行draw()函数，这应该也是使用SurfaceView的正确姿势。 &lt;br&gt; 参考 Android放大镜的实现 自定义控件之绘图篇（四）：canvas变换与操作 setScale,preScale和postScale的区别","raw":null,"content":null,"categories":[{"name":"Android","slug":"Android","permalink":"https://jermmy.github.io/categories/Android/"}],"tags":[{"name":"Android","slug":"Android","permalink":"https://jermmy.github.io/tags/Android/"},{"name":"SurfaceView","slug":"SurfaceView","permalink":"https://jermmy.github.io/tags/SurfaceView/"}]},{"title":"Android生命周期的那些事","slug":"2016-10-15-Android生命周期的那些事","date":"2016-10-15T01:57:37.000Z","updated":"2017-02-16T15:59:22.000Z","comments":true,"path":"2016/10/15/2016-10-15-Android生命周期的那些事/","link":"","permalink":"https://jermmy.github.io/2016/10/15/2016-10-15-Android生命周期的那些事/","excerpt":"这篇文章总结一下Android里面那些常用组件的生命周期，以及在项目中的一些简单应用。\nActivity\nActivity的生命周期\n说起Activity的生命周期，很多人都会马上想起下面这张图：\n\nActivity生命周期\n","text":"这篇文章总结一下Android里面那些常用组件的生命周期，以及在项目中的一些简单应用。 Activity Activity的生命周期 说起Activity的生命周期，很多人都会马上想起下面这张图： Activity生命周期 （不错，这是一个已经烂大街的问题。不过我是总结给自己看的＝。＝） 我写了一个例子观察不同的动作对生命周期的影响： Activity首次启动： LifeCycle1Activity: onCreate LifeCycle1Activity: onStart LifeCycle1Activity: onResume 按下Home键： LifeCycle1Activity: onPause LifeCycle1Activity: onStop 再次打开app： LifeCycle1Activity: onRestart LifeCycle1Activity: onStart LifeCycle1Activity: onResume 跳转到其他Activity: LifeCycle1Activity: onPause LifeCycle2Activity: onCreate LifeCycle2Activity: onStart LifeCycle2Activity: onResume LifeCycle1Activity: onStop 从其他Activity返回： LifeCycle2Activity: onPause LifeCycle1Activity: onRestart LifeCycle1Activity: onStart LifeCycle1Activity: onResume LifeCycle2Activity: onStop LifeCycle2Activity: onDestroy Activity销毁： LifeCycle1Activity: onPause LifeCycle1Activity: onStop LifeCycle1Activity: onDestroy 切换横竖屏： LifeCycle1Activity: onPause LifeCycle1Activity: onStop LifeCycle1Activity: onDestroy LifeCycle1Activity: onCreate LifeCycle1Activity: onStart LifeCycle1Activity: onResume 和生命周期相关的动作基本就是上面这些了。现在对照上面那张图以及这些Log，可以总结出以下几点： 当一个Activity创建并出现在屏幕中时，一定要经历onStart()、onResume()两个函数，只有做完onResume()这个动作，Activity才会可见； 当Activity退到后台时(按home键或切换Activity)，一定要经过onPause()、onStop()两个动作。经过onPause()这个动作，Activity就已经不可见了，这个时候其他组件(如其他Activity)会开始自己的创建绘制工作，等屏幕上面的其他组件绘制完成并覆盖原Activity后，原Activity才会调用onStop()方法； 当Activity从后台变为可见时，一定会经过onRestart()、onStart()和onResume()三个动作，onResume()之后Activity才变得可见； 切换横竖屏会导致Activity销毁并重建。 &lt;br&gt; Activity与Fragment生命周期比较 由于Fragment是要依附Activity才能存在的，所以把Fragment和Activity放在一起对比，下面的View同理。 说实话，Fragment是我不怎么喜欢的组件(因为实在太复杂)，但既然Google推荐，我们还是总结一下： fragment生命周期 有了上面这张图，再结合Activity的生命周期，基本就可以梳理完Fragment的生命周期问题了。 这里主要总结一下同一个Activity内不同Fragment创建或替换时，生命周期的变化。 同一个Activity内同时创建两个Fragment: 12345678910111213141516171819202122public class LifeCycle2Activity extends FragmentActivity &#123; MyFragment1 myFragment1 = new MyFragment1(); MyFragment2 myFragment2 = new MyFragment2(); @Override protected void onCreate(@Nullable Bundle savedInstanceState) &#123; super.onCreate(savedInstanceState); setContentView(R.layout.activity_life_cycle2); Log.i(TAG, \"onCreate\"); FragmentManager fragmentManager = getSupportFragmentManager(); FragmentTransaction ft = fragmentManager.beginTransaction(); ft.add(R.id.container_fragment1, myFragment1); ft.add(R.id.container_fragment2, myFragment2); ft.commit(); &#125; ....&#125; 这里只摘了重要代码片段，打印出的Log如下： LifeCycle2Activity: onCreate MyFragment1: onAttach(Activity) MyFragment1: onAttach(Context) MyFragment1: onCreate MyFragment2: onAttach(Activity) MyFragment2: onAttach(Context) MyFragment2: onCreate MyFragment1: onCreateView MyFragment1: onActivityCreated MyFragment2: onCreateView MyFragment2: onActivityCreated LifeCycle2Activity: onStart LifeCycle2Activity: onResume 从中可以看出，不同Fragment的创建是依次进行的，但它们都遵循上面那张图的规则。 Fragment之间的替换 12345678910111213141516171819202122232425262728293031public class LifeCycle2Activity extends FragmentActivity &#123; private final String TAG = getClass().getName(); MyFragment1 myFragment1 = new MyFragment1(); MyFragment2 myFragment2 = new MyFragment2(); @Override protected void onCreate(@Nullable Bundle savedInstanceState) &#123; super.onCreate(savedInstanceState); setContentView(R.layout.activity_life_cycle2); Log.i(TAG, \"onCreate\"); FragmentManager fragmentManager = getSupportFragmentManager(); FragmentTransaction ft = fragmentManager.beginTransaction(); ft.add(R.id.container_fragment1, myFragment1); ft.commit(); findViewById(R.id.btn).setOnClickListener(new View.OnClickListener() &#123; @Override public void onClick(View v) &#123; getSupportFragmentManager().beginTransaction(). replace(R.id.container_fragment1, myFragment2) .addToBackStack(null) .commit(); &#125; &#125;); &#125; .....&#125; 上面的例子中，Activity先加载MyFragment1，然后在点击按钮时，用MyFragment2替换MyFragment1。打印出的Log如下： MyFragment2: onAttach(Activity) MyFragment2: onAttach(Context) MyFragment2: onCreate MyFragment1: onDestroyView MyFragment2: onCreateView MyFragment2: onActivityCreated 可以发现，原来的Fragment的视图会被销毁，新的Fragment则会重建。但上面的生命周期流程图显示，只有在Activity进入Destroy状态时，Fragment才会执行onDestroyView()动作，Log里面MyFragment1却执行了onDestroyView()方法，有点费解。 如果Activity中包含较多的Fragment，比较好的方式是将它们缓存起来，防止频繁地创建和回收资源造成内存“抖动”现象。Google之所以提倡使用Fragment，是因为Fragment占用的资源比Activity小，这样，如果可以把视图都嵌入到多个Fragment中，而共用同一个Activity，可以更加合理地使用内存和CPU资源。 &lt;br&gt; Activity与View生命周期比较 笔者之前比较关注自定义View的一些知识，但比较忽略View生命周期相关的东西，直到前不久的项目中需要获得View的宽高后才遇到了问题，所以这一块有必要总结一下。 我简单地自定义了一个View，并在一些关键的回调函数里面打了Log： 1234567891011121314151617181920212223242526272829303132333435public class MyView extends View &#123; public static final String TAG = \"MyView\"; public MyView(Context context) &#123; super(context); Log.i(TAG, \"call Constructor\"); &#125; public MyView(Context context, AttributeSet attrs) &#123; super(context, attrs); &#125; public MyView(Context context, AttributeSet attrs, int defStyleAttr) &#123; super(context, attrs, defStyleAttr); &#125; @Override protected void onMeasure(int widthMeasureSpec, int heightMeasureSpec) &#123; super.onMeasure(widthMeasureSpec, heightMeasureSpec); Log.i(TAG, \"call onMeasure\"); &#125; @Override protected void onLayout(boolean changed, int left, int top, int right, int bottom) &#123; super.onLayout(changed, left, top, right, bottom); Log.i(TAG, \"call onLayout\"); &#125; @Override protected void onDraw(Canvas canvas) &#123; super.onDraw(canvas); Log.i(TAG, \"call onDraw\"); &#125;&#125; 将这个View放到Activity的Layout文件中，启动Activity时会打出如下Log： MyView: call Constructor LifeCycle1Activity: onCreate LifeCycle1Activity: onStart LifeCycle1Activity: onResume MyView: call onMeasure MyView: call onMeasure MyView: call onMeasure MyView: call onMeasure MyView: call onLayout MyView: call onMeasure MyView: call onMeasure MyView: call onLayout MyView: call onDraw 结果明确地告诉我们，View只有在Activity执行完onResume()动作后才会开始测量绘制工作，在这之前它仅仅是实例化了自己。相信很多人刚开始接触Android时，都曾经尝试过在onCreate()函数里去获取View的宽高，结果总是得到0。现在看来那是再自然不过的事情了，因为View只有测量绘制后才会有宽高。 另外，View只会测量并布局一次，之后如果Activity从后台重新返回前台，View只会执行onDraw()方法，下面是Activity切换回来后打印的Log： LifeCycle1Activity: onStop LifeCycle2Activity: onPause LifeCycle1Activity: onRestart LifeCycle1Activity: onStart LifeCycle1Activity: onResume MyView: call onDraw LifeCycle2Activity: onStop 显然，Activity执行onResume()后界面需要重绘，所以需要执行onDraw()函数，但View的宽高及摆放位置是一样的，所以onMeasure()、onLayout()便不必再执行。 现在问题来了，如果onResume()之后才开始测量绘制，我们要如何得到View的宽高呢？Google为每一个View提供了一个ViewTreeObserver类来帮我们监听View的一些动作，可以通过注册OnGlobalLayoutListener监听器来获得View的宽高等信息，见下面的代码： 12345678910111213141516171819202122232425@Overrideprotected void onCreate(Bundle savedInstanceState) &#123; super.onCreate(savedInstanceState); setContentView(R.layout.activity_life_cycle1); myView = (MyView) findViewById(R.id.myview); Log.i(TAG, \"onCreate\"); Log.i(TAG, \"onCreate width===&gt;\" + myView.getWidth() + \" height===&gt;\" + myView.getHeight()); myView.getViewTreeObserver().addOnGlobalLayoutListener(new ViewTreeObserver.OnGlobalLayoutListener() &#123; @Override public void onGlobalLayout() &#123; // Ensure you call it only once : if(android.os.Build.VERSION.SDK_INT &gt;= android.os.Build.VERSION_CODES.JELLY_BEAN) &#123; myView.getViewTreeObserver().removeOnGlobalLayoutListener(this); &#125; else &#123; myView.getViewTreeObserver().removeGlobalOnLayoutListener(this); &#125; Log.i(TAG, \"onGlobalLayout width===&gt;\" + myView.getWidth() + \" height===&gt;\" + myView.getHeight()); &#125; &#125;);&#125; 在onCreate函数中，我在myView上注册了OnGlobalLayoutListener监听器，结果打印的Log如下： MyView: call Constructor LifeCycle1Activity: onCreate LifeCycle1Activity: onCreate width===&gt;0 height===&gt;0 LifeCycle1Activity: onStart LifeCycle1Activity: onResume MyView: call onMeasure MyView: call onMeasure MyView: call onMeasure MyView: call onMeasure MyView: call onLayout LifeCycle1Activity: onGlobalLayout width===&gt;350 height===&gt;350 MyView: call onMeasure MyView: call onMeasure MyView: call onLayout MyView: call onDraw 不难看出，这个监听器会在测量得到宽高后被调用，并返回正确的宽高。关于View生命周期更多的东西，可以参考后面提供的链接。 &lt;br&gt; 参考 Android自定义view生命周期 getWidth() and getHeight() of View returns 0","raw":null,"content":null,"categories":[{"name":"Android","slug":"Android","permalink":"https://jermmy.github.io/categories/Android/"}],"tags":[{"name":"Android","slug":"Android","permalink":"https://jermmy.github.io/tags/Android/"}]},{"title":"Android内存泄漏那些事","slug":"2016-10-4-Android-内存泄漏那些事","date":"2016-10-04T10:15:52.000Z","updated":"2016-11-23T02:46:21.000Z","comments":true,"path":"2016/10/04/2016-10-4-Android-内存泄漏那些事/","link":"","permalink":"https://jermmy.github.io/2016/10/04/2016-10-4-Android-内存泄漏那些事/","excerpt":"这篇博文总结一下最近开发中遇到的内存泄漏的场景，并提供一些我所能找到的解决方案。\n静态全局类\n说起静态全局类，在Android里面用的最多的要数全局单例类（单例模式）。大多数人在构造单例类的时候，都会毫不犹豫地使用这种模板：\n123456789101112131415161718192021222324public class Singleton &#123;    private static Singleton instance = null;      private Context mContext;    public static Singleton getInstance(Context context) &#123;        if (instance != null) &#123;            return instance;        &#125;        synchronized (Singleton.class) &#123;            if (instance == null) &#123;                instance = new Singleton(context);            &#125;            return instance;        &#125;    &#125;    private Singleton(Context context) &#123;        this.mContext = context;    &#125;&#125;","text":"这篇博文总结一下最近开发中遇到的内存泄漏的场景，并提供一些我所能找到的解决方案。 静态全局类 说起静态全局类，在Android里面用的最多的要数全局单例类（单例模式）。大多数人在构造单例类的时候，都会毫不犹豫地使用这种模板： 123456789101112131415161718192021222324public class Singleton &#123; private static Singleton instance = null; private Context mContext; public static Singleton getInstance(Context context) &#123; if (instance != null) &#123; return instance; &#125; synchronized (Singleton.class) &#123; if (instance == null) &#123; instance = new Singleton(context); &#125; return instance; &#125; &#125; private Singleton(Context context) &#123; this.mContext = context; &#125;&#125; 这个时候问题来了：类中的Context指的是什么？想想平时用的时候是不是都会毫不犹豫地传入this，就是Activity本身。而对于单例类来说，mContext成员是永远不会被回收的，也就是说，这个引用指向的Activity无法被回收，从而造成内存泄漏。Google在这篇博客http://android-developers.blogspot.com/2009/01/avoiding-memory-leaks.html 中提出了解决办法，用Application代替Activity，因此我们可以这样改进： 123456789101112131415161718192021222324252627public class Singleton &#123; private static Singleton instance = null; private Context mContext; public static Singleton getInstance(Context context) &#123; if (instance != null) &#123; return instance; &#125; synchronized (Singleton.class) &#123; if (instance == null) &#123; instance = new Singleton(context); &#125; return instance; &#125; &#125; private Singleton(Context context) &#123; if (!(context instanceof Application)) &#123; throw new IllegalArgumentException(\"context must be instance of Application, \" + \"try using getApplicationContext()\"); &#125; this.mContext = context; &#125;&#125; 在传入Context的时候检查类型，强制外部传入Application。由于后者的生命周期属于整个应用，所以不存在内存泄漏问题。 如果单例类确实需要Activity的上下文（比方说需要做些UI相关的操作），那么可以提供一个detech()方法： 123public void detech() &#123; this.mContext = null;&#125; 在onDestroy方法中调用单例类的detech()来解除引用关系，防止泄漏。 但通常来说，涉及到UI相关的工作，更提倡用回调来执行，于是就有了更进一步的改进： 12345678910111213141516171819202122232425262728293031323334353637383940public class Singleton &#123; private static Singleton instance = null; public static Singleton getInstance(Context context) &#123; if (instance != null) &#123; return instance; &#125; synchronized (Singleton.class) &#123; if (instance == null) &#123; instance = new Singleton(context); &#125; return instance; &#125; &#125; private Context mContext; private CallBack callBack; private Singleton(Context context) &#123; if (!(context instanceof Application)) &#123; throw new IllegalArgumentException(\"context must be instance of Application, \" + \"try using getApplicationContext()\"); &#125; this.mContext = context; &#125; public void setCallBack(CallBack callBack) &#123; this.callBack = callBack; &#125; public void removeCallback() &#123; this.callBack = null; &#125; public interface CallBack &#123; void doSomethingInUI(); &#125;&#125; 这样，可以在Activity中向单例类传入一个CallBack，并在doSomethingInUI()回调中做一些UI相关的事情。需要注意的是，由于这个CallBack属于Activity的内部类，这个内部类会拥有外部类Activity的引用，所以需要在onDestroy()方法中调用单例类的removeCallback()类来解除引用关系，防止内存泄漏。 &lt;br&gt; WebView WebView一直是我不敢轻易使用的组件（个人感觉越强大的组件，菜鸟用起来越危险）。最近需要重点使用到这个组件，但却遭遇内存泄漏的问题。上网一查才知道，这是Google留给开发者的大坑：WebView自带内存泄漏属性。而且考虑到Android的碎片化情况严重，不同版本的系统泄漏的问题可能还不一样，比如，Android4.4以前的内核采用webkit，而4.4及以后就用chromium内核，所以4.4之前的解决方法可能在4.4及以后的系统不适用。简直蛋疼到极点～囧～ 在实战的时候，我发现WebView会持有原Activity的引用，即使在onDestroy()中将WebView置空也会导致Activity泄漏（大概是jni层有指针没有释放这个Activity），加上这个Activity的内容比较多，稍不留神便OOM。 再经过一天的搜索后，我找到一种可以暂时解决这种问题的方法，关键代码是这样的： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657private WebView webView;private LinearLayout webContainer;@Overrideprotected void onCreate(Bundle savedInstanceState) &#123; super.onCreate(savedInstanceState); setContentView(R.layout.activity_face_webview); initView();&#125;private void initView() &#123; webContainer = (LinearLayout) findViewById(R.id.webview_container); webView = new WebView(getApplicationContext()); webView.setLayoutParams(new RelativeLayout.LayoutParams(ViewGroup.LayoutParams.MATCH_PARENT, ViewGroup.LayoutParams.MATCH_PARENT)); webContainer.addView(webView); // 初始化网页浏览器 webView.requestFocus(); webView.setWebChromeClient(new WebChromeClient()); webView.getSettings().setJavaScriptEnabled(true); webView.getSettings().setDefaultTextEncodingName(\"utf-8\"); webView.setHorizontalScrollBarEnabled(false); webView.setVerticalScrollBarEnabled(false);&#125;@Overrideprotected void onResume() &#123; super.onResume(); webView.loadUrl(URL); webView.setWebViewClient(new WebViewClient()&#123; @Override public void onPageFinished(WebView view, String url)&#123; &#125; &#125;);&#125;@Overrideprotected void onDestroy() &#123; super.onDestroy(); if (webView != null) &#123; if (webContainer != null) &#123; webContainer.removeView(webView); &#125; webView.stopLoading(); webView.setWebChromeClient(null); webView.setWebViewClient(null); webView.setTag(null); webView.clearHistory(); webView.removeAllViews(); webView.destroy(); webView = null; &#125;&#125; 个人认为这种方法的关键是将WebView与Application绑定，也就是这句代码 webView = new WebView(getApplicationContext());。而这也意味着你不能在xml中声明webview节点，而必须动态添加进去。另外，在onDestroy()方法中必须手动将WebView从整棵View树中移除（至少在Android4.4的华为P7上需要这样做）。 前面说这种做法能暂时解决问题，但如果WebView需要用到Activity其他的元素，那么它会将Context强转为Activity对应的Context，这时这种做法就会出问题。 那有没有什么方法来“根治”这种问题呢？胡凯在他的文章中提到这种方法： Android中的WebView存在很大的兼容性问题，不仅仅是Android系统版本的不同对WebView产生很大的差异，另外不同的厂商出货的ROM里面WebView也存在着很大的差异。更严重的是标准的WebView存在内存泄露的问题，看这里WebView causes memory leak - leaks the parent Activity。所以通常根治这个问题的办法是为WebView开启另外一个进程，通过AIDL与主进程进行通信，WebView所在的进程可以根据业务的需要选择合适的时机进行销毁，从而达到内存的完整释放。 也就是通过一条新的进程来控制WebView的生命周期。这种方法可以更好地防止内存泄漏影响到主进程。目前我尚未尝试这种方法（好吧还没学会）。 &lt;br&gt; 内部类 内部类指的是那些在类的内部定义的类，又称嵌套类。Java中的内部类会持有外部类的引用，这是虚拟机帮我们处理的（所以才能在内部类中通过.this获得外部类）。如果内部类泄漏了，那么会进一步导致外部类也泄漏。 这种情况发生最多的案例是Activity中持有Handler的引用。Handler的生命周期比较特殊，当Handler发送Message到MessageQueue时，Message会持有一个名为target的引用，这个引用就是Handler本身。熟悉Handler的童鞋知道，MessageQueue是跟线程绑定在一起的消息队列，而我们用的最多的一般都是UI线程的MessageQueue。所以如果Activity有一个继承自Handler的内部类，在Activity启动finish()方法的时候，如果该Handler还在发送消息（即MessageQueue间接持有了该Handler的引用），便容易导致Activity的泄漏。 解决办法有两个，其一是将Handler声明为static，因为静态内部类不会持有外部类的引用，如果要在静态内部类中使用外部类的成员，可以通过WeakReference来持有外部类的引用。另一种方法是将Handler定义在单独的类文件中。方法的选择可以依个人喜好决定。 参考 Avoiding memory leaks Android内存优化之OOM Android WebView：性能优化不得不说的事 Android 彻底关闭WebView，防止WebView造成OOM 【Android】 WebView内存泄漏优化之路 Android 5.1 Webview 内存泄漏新场景 Android中Handler引起的内存泄露 细话Java：“失效”的private修饰符","raw":null,"content":null,"categories":[{"name":"Android","slug":"Android","permalink":"https://jermmy.github.io/categories/Android/"}],"tags":[{"name":"Android","slug":"Android","permalink":"https://jermmy.github.io/tags/Android/"},{"name":"内存优化","slug":"内存优化","permalink":"https://jermmy.github.io/tags/内存优化/"}]},{"title":"Thinking in Java — 容器类深入研究","slug":"2016-9-25-Thinking-In-Java-容器类深入研究","date":"2016-09-25T02:22:52.000Z","updated":"2017-03-25T01:31:59.000Z","comments":true,"path":"2016/09/25/2016-9-25-Thinking-In-Java-容器类深入研究/","link":"","permalink":"https://jermmy.github.io/2016/09/25/2016-9-25-Thinking-In-Java-容器类深入研究/","excerpt":"容器已经是现代编程语言必备的组件了。（注：由于一些常用容器的使用已经非常熟悉，因此本文着眼于我不熟悉的容器）\n完整的容器分类法\n下面这张图摘自《Thinking in Java》，是Java中容器类很好的概览图\n\nfull_container_taxonomy_thinking_in_java\n","text":"容器已经是现代编程语言必备的组件了。（注：由于一些常用容器的使用已经非常熟悉，因此本文着眼于我不熟悉的容器） 完整的容器分类法 下面这张图摘自《Thinking in Java》，是Java中容器类很好的概览图 full_container_taxonomy_thinking_in_java Java SE5中新添加的接口有： Queue接口及其实现PriorityQueue和各种风格的BlockingQueue； ConcurrentMap接口及其实现ConcurrentHashMap（用于多线程）； CopyOnWriteArrayList和CopyOnWriteArraySet，它们也是用于多线程机制的； EnumSet和EnumMap，为使用enum而设计的Set和Map的特殊实现； 在Collections类中的多个便利方法。 &lt;br&gt; Set和存储顺序 HashSet, TreeSet, LinkedHashSet Set是一种集合类，它需要一种方式来维护存储顺序。不同的Set实现类具有不同的存储行为，通常我们使用内置数据类型（如：String）的时候不需要考虑存储顺序，因为这些数据类型已经被设计为可以在容器内部使用。但如果是使用我们自定义的数据类型，就有必要了解一些内部机制了。 类型 说明 Set 存入Set的每个元素都必须是唯一的，因为Set不保存重复元素。加入Set的元素必须定义equals()方法以确保对象的唯一性。Set和Collection有完全一样的接口。Set接口不保证维护元素的次序。 HashSet 为快速查找而设计的Set。存入HashSet的元素必须定义hashCode() TreeSet 保持次序的Set，底层为树结构。使用它可以从Set中提取有序的序列。元素必须实现Comparable接口 LinkedHashSet 具有HashSet的查询速度，且内部使用链表维护元素的顺序（插入的次序）。于是在使用迭代器遍历Set时，结果会按元素插入的次序显示。元素也必须定义hashCode()方法 （作者推荐，如果没有其他限制，默认使用HashSet） 必须为散列存储和树形存储创建一个 equals()方法，但是hashCode()只有在这个类将会被置于HashSet或者LinkedHashSet时才是必须的。好的编程习惯是：在覆盖equals()方法时，同时覆盖hashCode()。 看一个例子： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061import java.util.*;class SetType &#123; int i; public SetType(int n) &#123; i = n; &#125; public boolean equals(Object o) &#123; return o instanceof SetType &amp;&amp; (i == ((SetType)o).i); &#125; public String toString() &#123; return Integer.toString(i); &#125;&#125;class HashType extends SetType &#123; public HashType(int n) &#123; super(n); &#125; public int hashCode() &#123; return i; &#125;&#125;class TreeType extends SetType implements Comparable&lt;TreeType&gt; &#123; public TreeType(int n) &#123; super(n); &#125; public int compareTo(TreeType arg) &#123; return (arg.i &lt; i ? -1 : (arg.i == i ? 0 : 1)); &#125;&#125;public class TypesForSets &#123; static &lt;T&gt; Set&lt;T&gt; fill(Set&lt;T&gt; set, Class&lt;T&gt; type) &#123; try &#123; for (int i = 0; i &lt; 10; i++) &#123; set.add(type.getConstructor(int.class).newInstance(i)); &#125; &#125; catch(Exception e) &#123; throw new RuntimeException(e); &#125; return set; &#125; static &lt;T&gt; void test(Set&lt;T&gt; set, Class&lt;T&gt; type) &#123; fill(set, type); fill(set, type); // Try to add duplicates fill(set, type); System.out.println(set); &#125; public static void main(String[] args) &#123; test(new HashSet&lt;HashType&gt;(), HashType.class); test(new LinkedHashSet&lt;HashType&gt;(), HashType.class); test(new TreeSet&lt;TreeType&gt;(), TreeType.class); // Things that don't work: test(new HashSet&lt;SetType&gt;(), SetType.class); test(new HashSet&lt;TreeType&gt;(), TreeType.class); test(new LinkedHashSet&lt;SetType&gt;(), SetType.class); test(new LinkedHashSet&lt;TreeType&gt;(), TreeType.class); try &#123; test(new TreeSet&lt;SetType&gt;(), SetType.class); &#125; catch (Exception e) &#123; System.out.println(e.getMessage()); &#125; try &#123; test(new TreeSet&lt;HashType&gt;(), HashType.class); &#125; catch (Exception e) &#123; System.out.println(e.getMessage()); &#125; &#125;&#125; 输出： 123456789[0, 1, 2, 3, 4, 5, 6, 7, 8, 9][0, 1, 2, 3, 4, 5, 6, 7, 8, 9][9, 8, 7, 6, 5, 4, 3, 2, 1, 0][5, 8, 1, 7, 3, 7, 9, 1, 9, 6, 7, 8, 4, 0, 3, 4, 9, 3, 2, 0, 5, 2, 1, 6, 6, 5, 4, 2, 8, 0][6, 1, 7, 8, 0, 6, 3, 0, 9, 1, 2, 8, 0, 7, 2, 9, 7, 4, 5, 5, 2, 9, 1, 6, 4, 3, 5, 3, 4, 8][0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9][0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9]java.lang.ClassCastException: SetType cannot be cast to java.lang.Comparablejava.lang.ClassCastException: HashType cannot be cast to java.lang.Comparable 这个例子分别演示了HashSet、LinkedHashSet、TreeSet的用法。test()函数会往集合Set中放入相同的元素（通过三个fill()），通过不同的SetType可以看到：HashSet、LinkedHashSet会根据hashCode()保持元素的唯一性（注意SetType、TreeType由于没有覆写hashCode()函数，所以使用了默认的hashCode()方法，导致每个元素的hash结果都不一样），TreeSet则根据Comparable接口判断元素的唯一性。 SortedSet SortedSet是一个接口定义，它保证内部的元素处于排序状态。 例子： 123456789101112131415161718192021222324import java.util.*;public class SortedSetDemo &#123; public static void main(String[] args) &#123; SortedSet&lt;String&gt; sortedSet = new TreeSet&lt;String&gt;(); Collections.addAll(sortedSet, \"one two three four five six seven eight\".split(\" \")); System.out.println(sortedSet); String low = sortedSet.first(); String high = sortedSet.last(); System.out.println(low); System.out.println(high); Iterator&lt;String&gt; it = sortedSet.iterator(); for (int i = 0; i &lt;= 6; i++) &#123; if (i == 3) low = it.next(); if (i == 6) high = it.next(); else it.next(); &#125; System.out.println(low); System.out.println(high); System.out.println(sortedSet.subSet(low, high)); System.out.println(sortedSet.headSet(high)); System.out.println(sortedSet.tailSet(low)); &#125;&#125; 输出： 12345678[eight, five, four, one, seven, six, three, two]eighttwoonetwo[one, seven, six, three][eight, five, four, one, seven, six, three][one, seven, six, three, two] 这里记住SortedSet的几个接口： Object first() 返回容器中的第一个元素； Object last() 返回容器中的最末一个元素； SortedSet subSet(fromElement, toElement) 生成Set的子集，范围从fromElement（包含）到toElement（不包含）； SortedSet headSet(toElement) 生成此Set的子集，由小于toElement的元素组成； SortedSet tailSet(fromElement) 生成此Set的子集，由大于或等于fromElement的元素组成。 &lt;br&gt; 队列 除了并发应用，Queue在Java SE5中仅有的两个实现是LinkedList和PriorityQueue，它们的差异在于排序行为而不是性能。 优先级队列 PriorityQueue的使用： 1234567891011121314151617181920212223242526272829303132333435363738394041import java.util.*;public class ToDoList extends PriorityQueue&lt;ToDoList.ToDoItem&gt; &#123; static class ToDoItem implements Comparable&lt;ToDoItem&gt; &#123; private char primary; private int secondary; private String item; public ToDoItem(String td, char pri, int sec) &#123; primary = pri; secondary = sec; item = td; &#125; public int compareTo(ToDoItem arg) &#123; if (primary &gt; arg.primary) return 1; if (primary == arg.primary) if (secondary &gt; arg.secondary) return 1; else if (secondary == arg.secondary) return 0; return -1; &#125; public String toString() &#123; return Character.toString(primary) + secondary + \": \" + item; &#125; &#125; public void add(String td, char pri, int sec) &#123; super.add(new ToDoItem(td, pri, sec)); &#125; public static void main(String[] args) &#123; ToDoList toDoList = new ToDoList(); toDoList.add(\"Empty trash\", 'C', 4); toDoList.add(\"Feed dog\", 'A', 2); toDoList.add(\"Feed bird\", 'B', 7); toDoList.add(\"Mow lawn\", 'C', 3); toDoList.add(\"Water lawn\", 'A', 1); toDoList.add(\"Feed cat\", 'B', 1); while (!toDoList.isEmpty()) &#123; System.out.println(toDoList.remove()); &#125; &#125;&#125; 输出： 123456A1: Water lawnA2: Feed dogB1: Feed catB7: Feed birdC3: Mow lawnC4: Empty trash &lt;br&gt; 理解Map Map 的基本实现 类型 说明 HashMap Map基于散列表的实现（它取代了Hashtable）。插入和查询“键值对”的开销是固定的。可以通过构造器设置容量和负载因子，以调整容器的性能。 LinkedHashMap 类似于HashMap，但是迭代遍历它时，取得“键值对”的顺序是其插入次序，或者是最近最少使用（LRU）的次序。只比HashMap慢一点；而在迭代访问时反而更快，因为它使用链表维护内部次序。 TreeMap 基于红黑树的实现。查看“键”或“键值对”时，它们会被排序（次序由Comparable或Comparator决定）。TreeMap的特点在于，所得到的结果是经过排序的。TreeMap是唯一的带有subMap()方法的Map，它可以返回一个子树。 WeakHashMap 弱键（weak kay）映射，允许释放映射所指向的对象；这是为解决某类特殊问题而设计的。如果映射之外没有引用指向某个“键”，则此“键”可以被垃圾收集器回收。 ConcurrentHashMap 一种线程安全的Map，它不涉及同步加锁，效率更高。 IdentifyHashMap 使用==代替equals()对“键”进行比较的散列映射。专为解决特殊问题而设计的。 之前讲Set 的时候说过，一般情况下，HashSet的效率是最高的。Map也不例外，HashMap应该是开发者首选。因为HashMap内部使用散列码提高了搜索速度。散列的速度远高于线性搜索，甚至树结构的搜索。 由于Map的使用与Set基本一样，这里便不再举例说明。 SortedMap 与SortedSet一样，SortedMap是一个接口定义，其唯一的实现类（Java SE5）是TreeMap，插入SortedMap的键必须实现Comparable接口。 LinkedHashMap 为了提高访问速度，LinkedHashMap散列化所有的元素，但是在遍历键值对时，却以元素的插入顺序返回键值对。此外，可以在构造器中设定LinkedHashMap，使之采用基于访问的最近最少使用算法。 &lt;br&gt; 散列与散列码 HashMap底层原理 如果使用自定义的类作为HashMap，那么这个类必须覆写equals()和hashCode()方法。理由是：HashMap是一种拉链式的哈希数组，类似于数据库中静态哈希存储结构。在HashMap中会有一个类似list[]的数组，Key的hashCode()用来计算这个Key对应数组哪个位置，之后再将对应的Value插入到这个位置的list后面，为了判断这个Key是否已经存在，HashMap又会调用Key的equals()函数对list做一次线性扫描（可能会有优化），只有不存在这个Key时才将Value插入list。 基于以上原理，hashCode()的结果可以相同，但equals()的结果一定要有所区分！ 覆盖hashCode() 设计hash函数要避开几个雷区： hashCode不能依赖于对象中易变的数据，这样，可能业务逻辑上原本相同的对象，get和put的位置不同； hashCode不能依赖于具有唯一性的对象信息，尤其是this，这样逻辑上相同的两个实例对象会产生不同的hashCode 对于如何设计一个好的hashCode，作者引用了Joshua Bloch的指导： 给int变量result赋予某个非零值常量，例如17； 为对象内每个有意义的域f（即每个可以做equals()操作的域）计算出一个int散列码c： 域类型 计算 boolean c = ( f ? 0 : 1) byte、char、short或int c = (int)f long c = (int)(f ^ (f &gt;&gt;&gt; 32)) float c = Float.floatToIntBits(f); double long I = Double.doubleToLongBits(f); c = (int)(I ^ (I &gt;&gt;&gt; 32)) Object，其equals() 调用这个域的equals() c = f.hashCode() 数组 对每个元素应用上述规则 合并计算得到的散列码： result = 37*result + c; 返回result 检查hashCode()最后生成的结果，确保相同的对象有相同的散列码 下面是一个应用上述规则的例子 123456789101112131415161718192021222324252627282930313233343536373839404142import java.util.*;public class CountedString &#123; private static List&lt;String&gt; created = new ArrayList&lt;String&gt;(); private String s; private int id = 0; public CountedString(String str) &#123; s = str; created.add(s); for (String s2 : created) &#123; if (s2.equals(str)) &#123; ++id; &#125; &#125; &#125; public String toString() &#123; return \"String: \" + s + \" id: \" + id + \" hashCode(): \" + hashCode(); &#125; public int hashCode() &#123; // Using Joshua Bloch's recipe int result = 17; result = 37 * result + s.hashCode(); result = 37 * result + id; return result; &#125; public boolean equals(Object o) &#123; return o instanceof CountedString &amp;&amp; s.equals(((CountedString)o).s) &amp;&amp; id == ((CountedString)o).id; &#125; public static void main(String[] args) &#123; Map&lt;CountedString, Integer&gt; map = new HashMap&lt;CountedString, Integer&gt;(); CountedString cs[] = new CountedString[5]; for (int i = 0; i &lt; cs.length; i++) &#123; cs[i] = new CountedString(\"hi\"); map.put(cs[i], i); &#125; System.out.println(map); for (CountedString cstring : cs) &#123; System.out.println(\"Looking up \" + cstring); System.out.println(map.get(cstring)); &#125; &#125;&#125; 输出： 1234567891011&#123;String: hi id: 4 hashCode(): 146450=3, String: hi id: 5 hashCode(): 146451=4, String: hi id: 2 hashCode(): 146448=1, String: hi id: 3 hashCode(): 146449=2, String: hi id: 1 hashCode(): 146447=0&#125;Looking up String: hi id: 1 hashCode(): 1464470Looking up String: hi id: 2 hashCode(): 1464481Looking up String: hi id: 3 hashCode(): 1464492Looking up String: hi id: 4 hashCode(): 1464503Looking up String: hi id: 5 hashCode(): 1464514 例子中的CountedString有两个成员变量：String s和int id。根据Joshua Bloch规则，int类型直接取这个值本身，而Object则取该对象hashCode()函数的值。从输出例子也可以看到，这种算法的hashCode计算结果受（有意义的）成员变量影响较大，可以产生较均匀的分配。（后面的结论是我杜撰的）。 &lt;br&gt; 性能测试 Java提供的容器类型实在太多，具体该选用哪个实现呢？这一节探讨不同容器之间的异同。（书中测试代码较多，本人认为要想真正认识这些容器还是应该从源代码入手，所以准备改天再花时间对这些容器的源码做一层剖析） 由于List比较熟悉了，所以以下只关注Set、Map这些稍微复杂的容器。 对Set的选择 Set的类型无非是TreeSet、HashSet、LinkedHashSet。TreeSet的优点是可以维持元素的顺序，所以只有当需要一个排好序的Set时，才应该使用TreeSet。HashSet是首选目标，因为相比排序操作，添加和查找元素往往更为常见，所以HashSet的性能总体上优于TreeSet。对于插入操作，LinkedHashSet比HashSet代价更高，这是由维护链表所带来的额外开销造成的（这一点出人意料）。 对Map的选择 Map的实现有TreeMap，HashMap，LinkedHashMap，IdentifyHashMap，WeakHashMap，Hashtable这几种。除了IdentifyHashMap，所有Map的插入操作都会随着Map尺寸的变大而明显变慢，但是查找的代价通常比插入小得多。 TreeMap通常比HashMap慢（想想数据库里的动态哈希跟B+树，前者在定位某个特定位置时，在溢出链不长的情况下几乎是一步得到，而TreeMap的效率应该还没有B+树高，所以HashMap的查找和插入操作比TreeMap快很多）。 LinkedHashMap的性能特点与LinkedHashSet一样，可类比。 HashMap的性能因子 我们可以手工调整HashMap的性能，这主要通过调整性能参数实现（下面是几个术语的意思，有些是可以手工设置的）： 容量：表中桶的位数 初始容量：表在创建时所拥有的桶位数。可以在构造函数指定 尺寸：表中当前存储的项数 负载因子：尺寸/容量。空表的负载因子是0，而半满表是0.5。负载轻的表产生冲突的可能性小，因此对于插入和查找效果较理想（但用迭代器遍历元素时速度会偏慢，因为空元素太多）。HashMap在负载情况达到负载因子时会自动增加容量，并重新散列（很像数据库的动态散列存储技术）。HashMap的默认负载因子是0.75。 &lt;br&gt; 使用方法 Collection或Map的同步控制 容器中的线程同步是一个很重要的问题，Java在Collections中提供了一种工具方法来解决不同步的问题。 使用方法的例子： 123456789101112import java.util.*;public class Synchronization &#123; public static void main(String[] args) &#123; Collection&lt;String&gt; c = Collections.synchronizedCollection(new ArrayList&lt;String&gt;()); List&lt;String&gt; list = Collections.synchronizedList(new ArrayList&lt;String&gt;()); Set&lt;String&gt; s = Collections.synchronizedSet(new HashSet&lt;String&gt;()); Set&lt;String&gt; ss = Collections.synchronizedSortedSet(new TreeSet&lt;String&gt;()); Map&lt;String, String&gt; m = Collections.synchronizedMap(new HashMap&lt;String, String&gt;()); Map&lt;String, String&gt; sm = Collections.synchronizedSortedMap(new TreeMap&lt;String, String&gt;()); &#125;&#125; 另外，java提供了一种快速报错的机制防止多线程修改造成的“灾难”: 例子： 1234567891011121314import java.util.*;public class FailTest &#123; public static void main(String[] args) &#123; Collection&lt;String&gt; c = new ArrayList&lt;String&gt;(); Iterator&lt;String&gt; it = c.iterator(); c.add(\"An object\"); try &#123; String s = it.next(); &#125; catch (ConcurrentModificationException e) &#123; System.out.println(e); &#125; &#125;&#125; 这段代码会抛出java.util.ConcurrentModificationException，因为在得到迭代器Iterator后，又往集合中加入新的对象，导致Iterator失效了。 Java SE5中新增的ConcurrentHashMap、CopyOnWriteArrayList和CopyOnWriteArraySet提供了避免java.util.ConcurrentModificationException的机制。 &lt;br&gt; 持有引用 java.lang.ref类库中包含了一组类用于垃圾回收机制。有三个继承自Reference的类：SoftReference，WeakReference，PhantomReference，它们为垃圾回收提供了不同级别的指示。 SoftReference，WeakReference，PhantomReference由强到弱排列。 在WeakHashMap的实现中，将插入的键和值用WeakReference封装了一遍，我的理解是，这样可以防止Map长期hold住对象的引用而不被回收，因为WeakHashMap中的引用跟普通对象的引用相比，回收的级别更高。 &lt;br&gt; Java 1.0/1.1 的容器 回顾历史，不要踩坑！ Java最开始的版本中内置了Vector、Enumeration、Hashtable、Stack，这些容器只在历史版本中工作了，它们中不乏失败的地方，因此不应该再被使用。","raw":null,"content":null,"categories":[{"name":"Java","slug":"Java","permalink":"https://jermmy.github.io/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://jermmy.github.io/tags/Java/"}]},{"title":"Android Volley源码浅析","slug":"2016-9-21-Android-Volley-源码浅析","date":"2016-09-21T14:01:25.000Z","updated":"2017-04-09T08:14:32.000Z","comments":true,"path":"2016/09/21/2016-9-21-Android-Volley-源码浅析/","link":"","permalink":"https://jermmy.github.io/2016/09/21/2016-9-21-Android-Volley-源码浅析/","excerpt":"Volley，中文翻译为「万箭齐发」，也就是适合大规模的小数据包发送的场景。Google 于 2013 年 I/O 大会上发布这个类库，试图弥补 UrlConnection 封装性太差的问题（Android 原生系统中用于网络请求的组件一般是 HttpUrlConnection 和 HttpClient 两种，后者适合 Android2.2 及以前的版本，但在 API23 开始便被舍弃了，前者适合于 Android2.3 及以上版本）。由于 Volley 自带异步线程回调机制，可以代替 AsyncTask 繁琐的接口，另外加上 Volley 的缓存功能，因此总体来说是一个大的改进。","text":"Volley，中文翻译为「万箭齐发」，也就是适合大规模的小数据包发送的场景。Google 于 2013 年 I/O 大会上发布这个类库，试图弥补 UrlConnection 封装性太差的问题（Android 原生系统中用于网络请求的组件一般是 HttpUrlConnection 和 HttpClient 两种，后者适合 Android2.2 及以前的版本，但在 API23 开始便被舍弃了，前者适合于 Android2.3 及以上版本）。由于 Volley 自带异步线程回调机制，可以代替 AsyncTask 繁琐的接口，另外加上 Volley 的缓存功能，因此总体来说是一个大的改进。 &lt;br&gt; 开发者接口 Volley 通常的使用接口如下： 1234567RequestQueue mRequestQueue = Volley.newRequestQueue(mContext.getApplicationContext());Request request = new Request();// 实现回调接口request.listener = new Response.Listener() &#123;...&#125;;request.errorListener = new Response.ErrorListener() &#123;...&#125;;mRequestQueue.add(request); 简单来讲，开发者需要关注的类只有 RequestQueue、Request，所以接下来就把目标集中在这两个类上。 &lt;br&gt; 浅析源代码 Volley.java（片段） 1234567891011121314151617181920212223242526272829public static RequestQueue newRequestQueue(Context context, HttpStack stack) &#123; File cacheDir = new File(context.getCacheDir(), \"volley\"); String userAgent = \"volley/0\"; try &#123; String network = context.getPackageName(); PackageInfo queue = context.getPackageManager().getPackageInfo(network, 0); userAgent = network + \"/\" + queue.versionCode; &#125; catch (NameNotFoundException var6) &#123; ; &#125; if(stack == null) &#123; if(VERSION.SDK_INT &gt;= 9) &#123; stack = new HurlStack(); &#125; else &#123; stack = new HttpClientStack(AndroidHttpClient.newInstance(userAgent)); &#125; &#125; BasicNetwork network1 = new BasicNetwork((HttpStack)stack); RequestQueue queue1 = new RequestQueue(new DiskBasedCache(cacheDir), network1); queue1.start(); return queue1;&#125;public static RequestQueue newRequestQueue(Context context) &#123; return newRequestQueue(context, (HttpStack)null);&#125; Volley 内部创建 RequestQueue 的时候，会根据版本号创建不同的 HttpStack，因为开头说过 Android 的两大网络组件适用的系统版本号不同。在实例化 RequestQueue 的时候，Android 传入两个组件：DiskBasedCache, BasicNetwork。前面也提到过，Volley 内部除了做网络请求，还会缓存请求，而这两个组件分别对应这两种功能。注意缓存目录是 app 内部的 cache file。 下面进入 RequestQueue 内部。 RequestQueue.java（片段） 1234567891011121314151617181920212223242526272829303132public RequestQueue(Cache cache, Network network, int threadPoolSize, ResponseDelivery delivery) &#123; this.mSequenceGenerator = new AtomicInteger(); this.mWaitingRequests = new HashMap(); this.mCurrentRequests = new HashSet(); this.mCacheQueue = new PriorityBlockingQueue(); this.mNetworkQueue = new PriorityBlockingQueue(); this.mCache = cache; this.mNetwork = network; this.mDispatchers = new NetworkDispatcher[threadPoolSize]; this.mDelivery = delivery;&#125;public RequestQueue(Cache cache, Network network, int threadPoolSize) &#123; this(cache, network, threadPoolSize, new ExecutorDelivery(new Handler(Looper.getMainLooper())));&#125;public RequestQueue(Cache cache, Network network) &#123; this(cache, network, 4);&#125;public void start() &#123; this.stop(); this.mCacheDispatcher = new CacheDispatcher(this.mCacheQueue, this.mNetworkQueue, this.mCache, this.mDelivery); this.mCacheDispatcher.start(); for(int i = 0; i &lt; this.mDispatchers.length; ++i) &#123; NetworkDispatcher networkDispatcher = new NetworkDispatcher(this.mNetworkQueue, this.mNetwork, this.mCache, this.mDelivery); this.mDispatchers[i] = networkDispatcher; networkDispatcher.start(); &#125;&#125; 这一段代码基本把所有重要的组件涵盖了。其中最重要的变量分别是：mCacheQueue, mNetworkQueue, mCache, mNetwork, mDispatchers, mCacheDispatcher, mDelivery。接下来不会深入去了解这些组件的细节，只大概分析一下它们的作用。 mDispatchers 和 mCacheDispatcher 分别是负责网络请求以及本地缓存的任务线程（继承自 Thread ），其中网络线程默认实例化了四条。 mCacheQueue, mNetworkQueue 是存放 Request 的队列，缓存线程和网络线程会分别从这两个队列获取 Request。队列类型是 PriorityBlockingQueue，是 java 并发库的一种队列实现，能够对队列内的元素进行优先级排序（实现 Request 的优先级请求），同时是属于阻塞性队列（ Blocking，如果没有元素，会使相应的线程陷入阻塞，而不是空转）。 mCache, mNetwork 是之前传入的 DiskBasedCache, BasicNetwork。这两个组件分别是为了解析缓存的请求和网络请求使用的。可以认为是请求解析类。 mDelivery 主要用于回调接口的调用，Volley 将它实例化成 ExecutorDelivery，从构造函数中可以看出，Volley 将它和 Handler（绑定 MainLooper ）绑定，从而可以和 UI 线程交互。 &lt;br&gt; 总体架构图 屏幕快照 2016-09-22 上午12.27.08","raw":null,"content":null,"categories":[{"name":"Android","slug":"Android","permalink":"https://jermmy.github.io/categories/Android/"}],"tags":[{"name":"Android","slug":"Android","permalink":"https://jermmy.github.io/tags/Android/"}]},{"title":"Android-WebView加载非Assets目录下的文件","slug":"2016-9-13-Android-WebView加载非Asset目录下的文件","date":"2016-09-13T12:38:21.000Z","updated":"2016-09-23T15:42:36.000Z","comments":true,"path":"2016/09/13/2016-9-13-Android-WebView加载非Asset目录下的文件/","link":"","permalink":"https://jermmy.github.io/2016/09/13/2016-9-13-Android-WebView加载非Asset目录下的文件/","excerpt":"背景\n最近遇到这样一个需求：WebView里的文件需要定期更新，而且是在java层获取后台的json数据后，更新到原来的js文件中。由于之前app的html、js等文件都是放在Assets目录下的，所以最开始的想法当然是看能不能对Assets目录进行读写。google一番后，SO上有人给出了答复：You cannot write data’s to asset/Raw folder, since it is packed(.apk) and not expandable in size. 然后我出于好奇想知道apk安装后，这两个文件夹的资源会被存储在哪。于是便进入到data/data/packageName目录下，结果发现这两个目录相关的资源并不存在，不仅如此，res目录下的资源也不在这里。于是又google了一番，发现apk安装后，除了data/data/packageName目录下会有东西，在data/app目录下会有一个与该apk相关的文件，暂时不知道是什么，但十有八九就是跟res等资源有关的东西。","text":"背景 最近遇到这样一个需求：WebView里的文件需要定期更新，而且是在java层获取后台的json数据后，更新到原来的js文件中。由于之前app的html、js等文件都是放在Assets目录下的，所以最开始的想法当然是看能不能对Assets目录进行读写。google一番后，SO上有人给出了答复：You cannot write data’s to asset/Raw folder, since it is packed(.apk) and not expandable in size. 然后我出于好奇想知道apk安装后，这两个文件夹的资源会被存储在哪。于是便进入到data/data/packageName目录下，结果发现这两个目录相关的资源并不存在，不仅如此，res目录下的资源也不在这里。于是又google了一番，发现apk安装后，除了data/data/packageName目录下会有东西，在data/app目录下会有一个与该apk相关的文件，暂时不知道是什么，但十有八九就是跟res等资源有关的东西。 解决方法 现在解决方案就很明显了：既然我不能向Assets目录下写数据，又不能在html中通过相对路径找到其他文件（因为data/app下那个文件不是很了解），那么可行的办法就是将html、js等文件都从Assets目录中拿下来，重新放在别的地方，这样不管做什么就方便多了。 再次google一番，发现WebView的loadUrl()方法本身就可以加载不同路径下的文件。比如，加载raw目录下的文件，可以写成 webView.loadUrl(&quot;file:///android_res/raw/your_file_name.html&quot;);，形式和加载Assets下的文件差不多。 考虑到html等文件不能被误删，我决定将这些文件转移到内部存储空间（也就是data/data/packageName）下面，这样需要更新的js文件也需要存储到内部存储空间。好在这些文件本身并不大，因此对存储器空间几乎没有影响。于是，在用户第一次进入app的时候，需要先通过 Activity 的 openFileOutput()方法将html等文件转移到data/data/packageName/files目录下，之后一旦有新的更新，也需要将更新应用到这个目录下的js等文件。在WebView中访问的方式：webView.loadUrl(&quot;file:///data/data/packageName/files/your_file_name.html&quot;);。如果是要访问外部存储器的文件，只需要将文件路径改为外部文件路径即可。 参考 Load html files from raw folder in web view How to write files to assets folder or raw folder in android? How exactly the android assets are stored in device’s storage (closed) Loading local html file in webView android","raw":null,"content":null,"categories":[{"name":"Android","slug":"Android","permalink":"https://jermmy.github.io/categories/Android/"}],"tags":[{"name":"Android","slug":"Android","permalink":"https://jermmy.github.io/tags/Android/"}]},{"title":"Thinking in Java — 并发","slug":"2016-9-10-Thinking-In-Java-并发","date":"2016-09-10T02:50:00.000Z","updated":"2016-10-20T11:12:32.000Z","comments":true,"path":"2016/09/10/2016-9-10-Thinking-In-Java-并发/","link":"","permalink":"https://jermmy.github.io/2016/09/10/2016-9-10-Thinking-In-Java-并发/","excerpt":"今天温习一下Java多线程的知识（文中代码除非特别说明，否则均摘自《Thinking in Java》(第四版)）。\nRunnable，Thread的简单使用表过不谈，只总结一下不熟悉的知识点。\n&lt;br&gt;","text":"今天温习一下Java多线程的知识（文中代码除非特别说明，否则均摘自《Thinking in Java》(第四版)）。 Runnable，Thread的简单使用表过不谈，只总结一下不熟悉的知识点。 &lt;br&gt; 使用Executor 作者只是简单提了一下这个东西，具体知识属于进阶内容。 Executor可以认为是一个管理线程池的东西，它允许你管理异步任务的执行，而无须显示地管理线程的生命周期。Executor在Java SE5/6是启动线程的优选方法。 12345678910111213141516171819202122232425262728293031import java.util.concurrent.*;class LiftOff implements Runnable &#123; protected int countDown = 10; private static int taskCount = 0; private final int id = taskCount++; public LiftOff() &#123;&#125; public LiftOff(int countDown) &#123; this.countDown = countDown; &#125; public String status() &#123; return \"#\" + id + \"(\" + (countDown &gt; 0 ? countDown : \"LiftOff!\") + \"), \"; &#125; @Override public void run() &#123; while (countDown-- &gt; 0) &#123; System.out.print(status()); Thread.yield(); &#125; &#125;&#125;public class CachedThreadPool &#123; public static void main(String[] args) &#123; ExecutorService exec = Executors.newCachedThreadPool(); for (int i = 0; i &lt; 5; i++) &#123; exec.execute(new LiftOff()); &#125; exec.shutdown(); &#125;&#125; 输出如下： 1#0(9), #1(9), #3(9), #0(8), #2(9), #1(8), #2(8), #4(9), #2(7), #4(8), #2(6), #0(7), #2(5), #1(7), #3(8), #0(6), #1(6), #4(7), #0(5), #3(7), #4(6), #2(4), #3(6), #0(4), #1(5), #4(5), #2(3), #3(5), #0(3), #1(4), #4(4), #2(2), #3(4), #0(2), #1(3), #2(1), #4(3), #3(3), #0(1), #1(2), #2(LiftOff!), #4(2), #3(2), #0(LiftOff!), #1(1), #4(1), #3(1), #1(LiftOff!), #4(LiftOff!), #3(LiftOff!), 通常，我们会通过Executors的工厂方法得到一个ExecutorService服务，上例中我们使用了CachedThreadPool作为线程池，在程序执行的过程中，它会创建与所需数量相同的线程，然后在回收旧线程时开始复用线程。作者将它作为Executors的首选服务，另外还有FixedThreadPool和SingleThreadPool等。 使用这个类库方便的地方在于，我们只要实现好Runnable接口，至于线程的实例化、销毁等，全部交由ExecutorService管理即可。开发者只需要调用execute()方法将Runnable对象传入。关闭ExecutorService服务时，可以调用shutdown()方法，这样新的任务将无法提交给ExecutorService，等之前提交的所有任务执行完毕后，服务也会尽快退出。 &lt;br&gt; 从线程产生返回值 众所周知，Runnable执行结束后是无法返回任何值的。如果希望任务在完成时能够返回一个值，那么可以实现Callable接口。Callable是一个泛型类，它的类型参数表示的是从call方法返回的值（call类似于Runnable的run方法）。还有一点，必须使用ExecutorService.submit()方法调用它。 12345678910111213141516171819202122232425262728293031323334import java.util.concurrent.*;import java.util.*;class TaskWithResult implements Callable&lt;String&gt; &#123; private int id; public TaskWithResult(int id) &#123; this.id = id; &#125; @Override public String call() &#123; return \"result of TaskWithResult \" + id; &#125;&#125;public class CallableDemo &#123; public static void main(String[] args) &#123; ExecutorService exec = Executors.newCachedThreadPool(); ArrayList&lt;Future&lt;String&gt;&gt; results = new ArrayList&lt;Future&lt;String&gt;&gt;(); for (int i = 0; i &lt; 10; i++) &#123; results.add(exec.submit(new TaskWithResult(i))); &#125; for (Future&lt;String&gt; fs : results) &#123; try &#123; System.out.println(fs.get()); &#125; catch(InterruptedException e) &#123; e.printStackTrace(); &#125; catch(ExecutionException e) &#123; e.printStackTrace(); &#125; finally &#123; exec.shutdown(); &#125; &#125; &#125;&#125; 输出如下结果：（貌似没体现出多线程的特性） 12345678910result of TaskWithResult 0result of TaskWithResult 1result of TaskWithResult 2result of TaskWithResult 3result of TaskWithResult 4result of TaskWithResult 5result of TaskWithResult 6result of TaskWithResult 7result of TaskWithResult 8result of TaskWithResult 9 submit()方法会产生Future对象，它会对Callable返回结果的特定类型进行参数化。可以用isDone()来判断Future是否已经完成，然后用get()来获得线程返回的结果。上面的例子直接调用fs.get()，此时将阻塞直至结果返回。 更详细的用法请参考其他资料。 &lt;br&gt; Join 关于join的用法，我总结成一句话（可能不全）： 假设A、B是两个线程，A执行的时候调用B.join()，A会被挂起直到B结束才执行。 join()方法还可以传入一个超时参数，这样超过指定时间线程也可以开始执行。 另外，调用interrupt()方法可以中断join()方法。 例子： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546class Sleeper extends Thread &#123; private int duration; public Sleeper(String name, int sleepTime) &#123; super(name); duration = sleepTime; start(); &#125; public void run() &#123; try &#123; sleep(duration); &#125; catch(InterruptedException e) &#123; System.out.println(getName() + \" was interrupted. \" + \"isInterrupted(): \" + isInterrupted()); return; &#125; System.out.println(getName() + \" has awakened\"); &#125;&#125;class Joiner extends Thread &#123; private Sleeper sleeper; public Joiner(String name, Sleeper sleeper) &#123; super(name); this.sleeper = sleeper; start(); &#125; public void run() &#123; try &#123; sleeper.join(); &#125; catch(InterruptedException e) &#123; System.out.println(\"Interrupted\"); &#125; System.out.println(getName() + \" join completed\"); &#125;&#125;public class Joining &#123; public static void main(String[] args) &#123; Sleeper sleepy = new Sleeper(\"Sleepy\", 1500), grumpy = new Sleeper(\"Grumpy\", 1500); Joiner dopey = new Joiner(\"Dopey\", sleepy), doc = new Joiner(\"Doc\", grumpy); grumpy.interrupt(); &#125;&#125; 输出： 1234Grumpy was interrupted. isInterrupted(): falseDoc join completedSleepy has awakenedDopey join completed 上面的例子中，Joiner线程在执行的时候，会先调用Sleeper线程的join()方法，这样，前者只有等后者执行完才会继续执行，除非被interrupt()打断。 这里要关注的应该是join()的使用时机，我认为，join的目的是为了线程间的同步，所以通常用法应该是：A执行到某一步的时候，需要等B执行完，才调用B.join()。 上面的例子中还有一个小tip，就是线程调用interrupt()方法后，该线程会设定一个标志位，表明被中断。但异常被捕获时会清理标志位，所以能看到为什么例子中的isInterrupted()方法会返回false。 &lt;br&gt; Synchronized 同步方法 同步是java为防止资源冲突提供的内置支持。 所有对象都自动含有单一的锁（也称为监视器）。当在对象上调用其任意synchronized方法时，该对象都被加锁，这时该对象上的其他synchronized方法只有等到前一个方法调用完毕并释放锁后才能被调用。所以，对于某个特定对象而言，其所有synchronized方法共享同一个锁。 针对每个类，也有一个锁（作为Class对象的一部分），所以synchronized static方法可以在类的范围内防止对static数据的并发访问。 什么时候应该上锁呢？Bruce建议：每个访问临界共享资源的方法都必须被同步，否则它们就不会正确地工作。 临界区 除了将整个函数上锁的方式，还可以使用同步控制块的方式防止冲突。 123synchronized (object) &#123; &#125; 这种方式相比前一种而言，更加灵活，且效率更高。 同步对象 synchronized同步块必须给定一个同步对象，最合理的方式是使用当前对象，也就是synchronized(this)，在这种方式中，如果获得了synchronized块的锁，那么该对象其他的synchronized方法和临界区就不能被调用了。 也可以在其他对象上同步，请看下面的例子： 12345678910111213141516171819202122232425262728293031323334353637383940class DualSynch &#123; private Object syncObject = new Object(); public synchronized void f() &#123; for (int i = 0; i &lt; 5; i++) &#123; System.out.println(\"f()\"); Thread.yield(); &#125; &#125; public synchronized void e() &#123; for (int i = 0; i &lt; 5; i++) &#123; System.out.println(\"e()\"); Thread.yield(); &#125; &#125; public void g() &#123; synchronized (syncObject) &#123; for (int i = 0; i &lt; 5; i++) &#123; System.out.println(\"g()\"); Thread.yield(); &#125; &#125; &#125;&#125;public class SyncObject &#123; public static void main(String[] args) &#123; final DualSynch ds = new DualSynch(); new Thread() &#123; public void run() &#123; ds.f(); &#125; &#125;.start(); new Thread() &#123; public void run() &#123; ds.e(); &#125; &#125;.start(); ds.g(); &#125;&#125; 我在原书代码的基础上增加了同步方法public synchronized void e()，这样更容易看到区别。Thread.yield()是为了使线程相互抢占cpu的现象更加明显。 输出如下： 123456789101112131415f()g()f()g()g()g()f()f()f()e()e()g()e()e()e() 可以清楚地看到，虽然f()和g()都有同步代码块，但由于二者的同步对象不同（f()是this对象，而g()是syncObject），所以二者依然可以“同时”执行。但e()需要的对象先被f()占有了，所以必须等f()执行完释放同步对象后，e()才能执行。 被互斥所阻塞 如果尝试着在一个对象上调用其synchronized方法，而这个对象的锁已经被其他任务获得，那么调用任务将被挂起（阻塞）。但如果是同一个任务访问这个synchronized方法，那么它可以继续获得这个锁。 12345678910111213141516171819202122public class MultiLock &#123; public synchronized void f1(int count) &#123; if (count-- &gt; 0) &#123; System.out.println(\"f1() calling f2() with count \" + count); f2(count); &#125; &#125; public synchronized void f2(int count) &#123; if (count-- &gt; 0) &#123; System.out.println(\"f2() calling f1() with count \" + count); f1(count); &#125; &#125; public static void main(String[] args) &#123; final MultiLock multiLock = new MultiLock(); new Thread() &#123; public void run() &#123; multiLock.f1(10); &#125; &#125;.start(); &#125;&#125; 输出： 12345678910f1() calling f2() with count 9f2() calling f1() with count 8f1() calling f2() with count 7f2() calling f1() with count 6f1() calling f2() with count 5f2() calling f1() with count 4f1() calling f2() with count 3f2() calling f1() with count 2f1() calling f2() with count 1f2() calling f1() with count 0 可以看到，尽管f1()和f2()都hold住同一个锁，但由于是同一个任务，因此不会出现死锁现象。 &lt;br&gt; 线程状态 Bruce将线程的状态分为四种：新建(New)、就绪(Runnable)、阻塞(Blocked)、死亡(Dead)。我觉得分为五种更加合适：新建(New)、就绪(Runnable)、运行(Running)、阻塞(Blocked)、死亡(Dead)。从这篇博文http://www.runoob.com/java/thread-status.html中可以如下状态转换图： new &lt;br&gt; 中断（interrupt） Thread提供了interrupt()方法，该方法提供了一种在Runnable.run()中间中断线程的可能。注意，只是可能。 如果使用Executor来执行线程，可以直接使用shutdownNow()方法，它将发送一个interrupt()调用给它启动的所有线程。然而，如果只希望中断某个单一任务，可以使用submit()而不是execute()来启动任务。正如之前所提到的，submit()将返回一个Future&lt;?&gt;对象，后者代表线程的上下文（或者简单理解为该线程的引用）。我们可以通过Future.cancle(true)方法来中断线程。 下面这个例子比较长，主要是为了演示interrupt()能够作用的情况。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172import java.util.concurrent.*;import java.io.*;class SleepBlocked implements Runnable &#123; public void run() &#123; try &#123; TimeUnit.SECONDS.sleep(100); &#125; catch(InterruptedException e) &#123; System.out.println(\"InterruptedException\"); &#125; System.out.println(\"Exiting SleepBlocked.run()\"); &#125;&#125;class IOBlocked implements Runnable &#123; private InputStream in; public IOBlocked(InputStream is) &#123; this.in = is; &#125; public void run() &#123; try &#123; System.out.println(\"Waiting for read()\"); in.read(); &#125; catch(IOException e) &#123; if (Thread.currentThread().isInterrupted()) &#123; System.out.println(\"Interrupted from blocked I/O\"); &#125; else &#123; throw new RuntimeException(e); &#125; &#125; System.out.println(\"Exiting IOBlocked.run()\"); &#125;&#125;class SynchronizedBlocked implements Runnable &#123; public synchronized void f() &#123; while (true) &#123; // Never release lock Thread.yield(); &#125; &#125; public SynchronizedBlocked() &#123; new Thread() &#123; public void run() &#123; f(); // Lock acquired by this thread &#125; &#125;.start(); &#125; public void run() &#123; System.out.println(\"Trying to call f()\"); f(); System.out.println(\"Exiting SynchronizedBlocked.run()\"); &#125;&#125;public class Interrupting &#123; private static ExecutorService exec = Executors.newCachedThreadPool(); static void test(Runnable r) throws InterruptedException &#123; Future&lt;?&gt; f = exec.submit(r); TimeUnit.MILLISECONDS.sleep(100); System.out.println(\"Interrupting \" + r.getClass().getName()); f.cancel(true); System.out.println(\"Interrupt sent to \" + r.getClass().getName()); &#125; public static void main(String[] args) throws Exception &#123; test(new SleepBlocked()); test(new IOBlocked(System.in)); test(new SynchronizedBlocked()); TimeUnit.SECONDS.sleep(3); System.out.println(\"Aborting with System.exit(0)\"); System.exit(0); &#125;&#125; 输出： 1234567891011Interrupting SleepBlockedInterrupt sent to SleepBlocked &lt;--外部中断SleepBlocked线程InterruptedException &lt;--SleepBlocked触发InterruptedExceptionExiting SleepBlocked.run() &lt;--SleepBlocked线程结束Waiting for read()Interrupting IOBlockedInterrupt sent to IOBlocked &lt;--外部中断IOBlocked线程，但该线程仍在执行Trying to call f()Interrupting SynchronizedBlockedInterrupt sent to SynchronizedBlocked &lt;--外部中断SynchronizedBlocked线程，但该线程仍在执行Aborting with System.exit(0) 从输出结果至少可以得出以下结论： interrupt()可以中断对sleep()的调用，但对I/O操作和获取synchronized锁这两种阻塞，interrupt()无法中断线程。 &lt;br&gt; wait() wait()的目的 wait()主要是为了防止忙等待现象的出现。有时候线程执行到某一步的时候，需要等待一段时间让其他线程先跑，等后者做完某些事情的时候再“通知”前者让它继续执行。这种方式可以通过回调接口来实现，但如果要保持不同线程之间的独立性，就要提供一种机制来达到“通知”的效果。 最简单的方法是用一个无限循环和布尔变量： 1234while (!isNotified) &#123; &#125;doThing(); 线程A通过一个循环来实现“挂起”的效果（也就是什么事也不做），等线程B做完某些事情后，修改isNotified变量，使线程A跳出循环，来实现“通知”效果。这种实现方式称为“忙等待”，因为线程A其实仍然在执行循环代码，但却什么事也没做。 更有效率的实现方式是让线程A挂起（放弃CPU时间），这一点通常需要操作系统提供支持。而JVM就有这样的底层支持，暴露给开发者的接口就是wait()方法。 wait()与sleep(), yield()的区别 调用sleep(), yield()的时候，虽然线程会让出cpu，但锁并没有释放（如果事先拥有的话）。但wait()不仅会将线程挂起，还会释放之前抢到的锁。 wait()的使用 wait()通常与notify(), notifyAll()同时出现。 wait()有两种调用形式，一种不带参数wait()，另一种带一个参数wait(time)，前者会无限等待，直到通过notify()或notifyAll()唤醒，后者可以通过notify()等唤醒，或者等到达了参数time指定的时间，由系统自动唤醒。 要记住的一点是，wait(), notify()这些看似和线程相关的操作，其实是基类Object提供的方法。因为这些方法操作的锁是对象的一部分，而不仅仅是线程的一部分。notify()和notifyAll()的区别是：前者只唤醒一条线程，后者唤醒所有线程，正常情况下应该使用后者，前者只在只有一个线程等待的情况下使用。 wait(), notify()等方法必须在同步代码块中执行，究其根本，在于它们必须“拥有”锁，才能被唤醒或唤醒其他线程。（锁 将 被唤醒的对象与唤醒它的对象联系在了一起）。 通常，获得锁的方法分为两种（需要注意的是，如果是在一个对象上进行同步，则必须通过这个对象的wait()或notify()方法进行工作，否则会抛出java.lang.IllegalMonitorStateException）： 12345678910// 指定锁对象synchronized(x) &#123; //x.notifyAll(); //x.wait(); // 这里必须使用x的notifyAll()或wait()方法&#125;// 使用同步方法，等价于锁住类本身public synchronized void f() &#123; // wait(); // notifyAll(); // 这里必须使用这个类本身的notifyAll()或wait()方法&#125; 下面这个例子其实模仿了生产者与消费者模型： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374import java.util.concurrent.*;class Car &#123; private boolean waxOn = false; public synchronized void waxed() &#123; waxOn = true; notifyAll(); &#125; public synchronized void buffed() &#123; waxOn = false; notifyAll(); &#125; public synchronized void waitForWaxing() throws InterruptedException &#123; while (waxOn == false) &#123; wait(); &#125; &#125; public synchronized void waitForBuffing() throws InterruptedException &#123; while (waxOn == true) &#123; wait(); &#125; &#125;&#125;class WaxOn implements Runnable &#123; private Car car; public WaxOn(Car car) &#123; this.car = car; &#125; public void run() &#123; try &#123; while (!Thread.interrupted()) &#123; System.out.print(\"Wax On!\"); TimeUnit.MILLISECONDS.sleep(400); car.waxed(); car.waitForBuffing(); &#125; &#125; catch(InterruptedException e) &#123; System.out.println(\"Exiting via interrupt\"); &#125; System.out.println(\"Ending Wax On task\"); &#125;&#125;class WaxOff implements Runnable &#123; private Car car; public WaxOff(Car c) &#123; this.car = c; &#125; public void run() &#123; try &#123; while (!Thread.interrupted()) &#123; car.waitForWaxing(); System.out.print(\"Wax Off!\"); TimeUnit.MILLISECONDS.sleep(400); car.buffed(); &#125; &#125; catch (InterruptedException e) &#123; System.out.println(\"Exiting via interrupt\"); &#125; System.out.println(\"Ending Wax Off task\"); &#125;&#125;public class WaxOMatic &#123; public static void main(String[] args) throws Exception &#123; Car car = new Car(); ExecutorService exec = Executors.newCachedThreadPool(); exec.execute(new WaxOff(car)); exec.execute(new WaxOn(car)); TimeUnit.SECONDS.sleep(5); exec.shutdownNow(); &#125;&#125; 输出： 1234Wax On!Wax Off!Wax On!Wax Off!Wax On!Wax Off!Wax On!Wax Off!Wax On!Wax Off!Wax On!Wax Off!Wax On!Exiting via interruptEnding Wax On taskExiting via interruptEnding Wax Off task 可以看到，cpu在两条线程之间交替执行。 &lt;br&gt; 使用显示的Lock和Condition对象 Condition提供了await()和signalAll()方法来将线程挂起或唤醒，书上的说法是：与使用notifyAll()相比，signalAll()是更安全的方式。 基本使用： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697import java.util.concurrent.*;import java.util.concurrent.locks.*;class Car &#123; private Lock lock = new ReentrantLock(); private Condition condition = lock.newCondition(); private boolean waxOn = false; public void waxed() &#123; lock.lock(); try &#123; waxOn = true; condition.signalAll(); &#125; finally &#123; lock.unlock(); &#125; &#125; public void buffed() &#123; lock.lock(); try &#123; waxOn = false; condition.signalAll(); &#125; finally &#123; lock.unlock(); &#125; &#125; public void waitForWaxing() throws InterruptedException &#123; lock.lock(); try &#123; while (waxOn == false) &#123; condition.await(); &#125; &#125; finally &#123; lock.unlock(); &#125; &#125; public void waitForBuffing() throws InterruptedException &#123; lock.lock(); try &#123; while (waxOn == true) &#123; condition.await(); &#125; &#125; finally &#123; lock.unlock(); &#125; &#125;&#125;class WaxOn implements Runnable &#123; private Car car; public WaxOn(Car c) &#123; car = c; &#125; public void run() &#123; try &#123; while (!Thread.interrupted()) &#123; System.out.println(\"Wax On!\"); TimeUnit.MILLISECONDS.sleep(200); car.waxed(); car.waitForBuffing(); &#125; &#125; catch (InterruptedException e) &#123; System.out.println(\"Exiting via interrupt\"); &#125; System.out.println(\"Ending Wax On task\"); &#125;&#125;class WaxOff implements Runnable &#123; private Car car; public WaxOff(Car c) &#123; car = c; &#125; public void run() &#123; try &#123; while (!Thread.interrupted()) &#123; car.waitForWaxing(); System.out.println(\"Wax Off!\"); TimeUnit.MILLISECONDS.sleep(200); car.buffed(); &#125; &#125; catch (InterruptedException e) &#123; System.out.println(\"Exiting via interrupt\"); &#125; System.out.println(\"Ending Wax Off task\"); &#125;&#125;public class WaxOMatic2 &#123; public static void main(String[] args) throws Exception &#123; Car car = new Car(); ExecutorService exec = Executors.newCachedThreadPool(); exec.execute(new WaxOff(car)); exec.execute(new WaxOn(car)); TimeUnit.SECONDS.sleep(5); exec.shutdownNow(); &#125;&#125; （输出与上一个例子一致） 上面的例子显得有些冗长，重点放在Car这个类的实现就可以。对于Car内部的四个方法，都是先用lock.lock()锁起来，然后调用condition.await()或者condition.signalAll()来等待或唤醒线程，注意这个Condition对象和Lock对象是绑定的。虽然看起来复杂了一些，但可以将lock.lock()看作是synchronized(object)，将condition.await看作是object.wait()，这样它们的用法和之前wait(), notify()的使用就差不多了。 这个解决方案代码上明显更加复杂，作者也强调，Lock和Condition对象只有在更加困难的多线程问题中才是需要的。 &lt;br&gt; 新类库中的构件 Java SE5的java.util.concurrent引入了大量用来解决并发问题的类库。（下面只摘录几个我遇到的，之后会继续补充＝。＝） PriorityBlockingQueue 这是一个很基础的优先级队列，它具有可阻塞的读取操作。 例子： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103import java.util.*;import java.util.concurrent.*;class PrioritizedTask implements Runnable, Comparable&lt;PrioritizedTask&gt; &#123; private Random rand = new Random(47); private static int counter = 0; private final int id = counter++; private final int priority; protected static List&lt;PrioritizedTask&gt; sequence = new ArrayList&lt;PrioritizedTask&gt;(); public PrioritizedTask(int priority) &#123; this.priority = priority; sequence.add(this); &#125; public int compareTo(PrioritizedTask arg) &#123; return priority &lt; arg.priority ? 1 : (priority &gt; arg.priority ? -1 : 0); &#125; public void run() &#123; try &#123; TimeUnit.MILLISECONDS.sleep(rand.nextInt(250)); &#125; catch (InterruptedException e) &#123;&#125; System.out.println(this); &#125; public String toString() &#123; return String.format(\"[%1$-3d]\", priority) + \" Task \" + id; &#125; public String summary() &#123; return \"(\" + id + \":\" + priority + \")\"; &#125; public static class EndSentinel extends PrioritizedTask &#123; private ExecutorService exec; public EndSentinel(ExecutorService e) &#123; super(-1); exec = e; &#125; public void run() &#123; int count = 0; for (PrioritizedTask pt : sequence) &#123; System.out.println(pt.summary()); if (++count % 5 == 0) &#123; System.out.println(\"\"); &#125; &#125; System.out.println(\"\"); System.out.println(this + \" Calling shutdownNow()\"); exec.shutdownNow(); &#125; &#125;&#125;class PrioritizedTaskProducer implements Runnable &#123; private Random rand = new Random(47); private Queue&lt;Runnable&gt; queue; private ExecutorService exec; public PrioritizedTaskProducer(Queue&lt;Runnable&gt; q, ExecutorService e) &#123; queue = q; exec = e; &#125; public void run() &#123; for (int i = 0; i &lt; 20; i++) &#123; queue.add(new PrioritizedTask(rand.nextInt(10))); Thread.yield(); &#125; try &#123; for (int i = 0; i &lt; 10; i++) &#123; TimeUnit.MILLISECONDS.sleep(250); queue.add(new PrioritizedTask(10)); &#125; for (int i = 0; i &lt; 10; i++) &#123; queue.add(new PrioritizedTask(i)); &#125; queue.add(new PrioritizedTask.EndSentinel(exec)); &#125; catch (InterruptedException e) &#123; &#125; System.out.println(\"Finished PrioritizedTaskProducer\"); &#125;&#125;class PrioritizedTaskConsumer implements Runnable &#123; private PriorityBlockingQueue&lt;Runnable&gt; q; public PrioritizedTaskConsumer(PriorityBlockingQueue&lt;Runnable&gt; q) &#123; this.q = q; &#125; public void run() &#123; try &#123; while(!Thread.interrupted()) &#123; q.take().run(); &#125; &#125; catch (InterruptedException e) &#123;&#125; System.out.println(\"Finished PrioritizedTaskConsumer\"); &#125;&#125;public class PriorityBlockingQueueDemo &#123; public static void main(String[] args) throws Exception &#123; Random rand = new Random(47); ExecutorService exec = Executors.newCachedThreadPool(); PriorityBlockingQueue&lt;Runnable&gt; queue = new PriorityBlockingQueue&lt;Runnable&gt;(); exec.execute(new PrioritizedTaskProducer(queue, exec)); exec.execute(new PrioritizedTaskConsumer(queue)); &#125;&#125; 这个例子长到不想看，简单来讲，PriorityBlockingQueue提供两种操作：add(), take()。当队列为空时，调用take()的线程会阻塞，只有add()操作可以将其唤醒。需要注意的是，因为PriorityBlockingQueue是优先队列，所以类型参数需要实现Comparable接口。 题外话：Android的Volley就是用PriorityBlockingQueue实现的。 &lt;br&gt; 性能调优 Java早期的容器类（Vector和Hashtable等）为了支持并发操作，提供了许多synchronized方法，这种实现在单线程环境下效率很低。 Java SE5中提供了新的容器，通过灵巧的技术消除加锁。这些免锁容器背后的通用策略是：对容器的修改可以与读取操作同时发生，只要读取者只能看到完成修改的结果即可。修改是在容器数据结构的某个部分的一个单独的副本上执行的，并且这个副本在修改过程中不可视。（有点类似数据库的隔离性，可以认为这些操作都是以原子单位进行的）。","raw":null,"content":null,"categories":[{"name":"Java","slug":"Java","permalink":"https://jermmy.github.io/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://jermmy.github.io/tags/Java/"},{"name":"并行","slug":"并行","permalink":"https://jermmy.github.io/tags/并行/"}]},{"title":"Android NDK-异常总结","slug":"2016-9-6-Android-NDK-异常总结","date":"2016-09-06T11:37:41.000Z","updated":"2017-04-23T05:41:24.000Z","comments":true,"path":"2016/09/06/2016-9-6-Android-NDK-异常总结/","link":"","permalink":"https://jermmy.github.io/2016/09/06/2016-9-6-Android-NDK-异常总结/","excerpt":"开发 NDK 时，由于大量用到指针等，在失去 Java 虚拟机保护（异常抛出）的情况下，常常面临崩溃闪退却不知道哪里出错的问题。更有甚者，这种情况还具有随机性，非常麻烦！本文记录一下我开发 NDK 时遇到的各种蛋疼问题，方便以后查找使用。","text":"开发 NDK 时，由于大量用到指针等，在失去 Java 虚拟机保护（异常抛出）的情况下，常常面临崩溃闪退却不知道哪里出错的问题。更有甚者，这种情况还具有随机性，非常麻烦！本文记录一下我开发 NDK 时遇到的各种蛋疼问题，方便以后查找使用。 1. A/libc(3347): Fatal signal 11 (SIGSEGV) at 0xdeadbaad (code=1) 这个异常会导致 app 闪退。我开发的时候是随机遇到的，当时调了一个早上。网上 google 后发现遇到问题的原因各种各样，但大多数是访问了不该访问的内存（如：数组越界）。在仔细查看了代码后，我对所有可能越界的地方做了改善，并打了断点，结果还是遇到这种问题，但同样的数据在电脑上跑完全正常。后来我猜想是否是栈空间不足导致的，于是将所有数组的内存分配到堆上，之后便一切顺利了。不管原因是否是栈空间不足导致的，总算是找到了一种解决方案。 参考 Fatal signal 11 (SIGSEGV) at 0xdeadbaad (code=1) 错误 解决方案(android-ndk)","raw":null,"content":null,"categories":[{"name":"Android","slug":"Android","permalink":"https://jermmy.github.io/categories/Android/"}],"tags":[{"name":"Android","slug":"Android","permalink":"https://jermmy.github.io/tags/Android/"},{"name":"NDK","slug":"NDK","permalink":"https://jermmy.github.io/tags/NDK/"}]},{"title":"Thinking in Java — 泛型","slug":"2016-9-5-Thinking-In-Java-泛型","date":"2016-09-05T15:24:10.000Z","updated":"2017-09-03T09:52:25.000Z","comments":true,"path":"2016/09/05/2016-9-5-Thinking-In-Java-泛型/","link":"","permalink":"https://jermmy.github.io/2016/09/05/2016-9-5-Thinking-In-Java-泛型/","excerpt":"刚开始学 JavaSE 的时候，买了一本业界经典的《Thinking in Java》，后来证明对于初学者来说完全是错误的决定。现在趁着大四有点时间，准备从头将一些重要的知识学一遍。\n今天要学的是泛型（文中代码除非特别说明，否则均摘自《Thinking in Java》(第四版)）。","text":"刚开始学 JavaSE 的时候，买了一本业界经典的《Thinking in Java》，后来证明对于初学者来说完全是错误的决定。现在趁着大四有点时间，准备从头将一些重要的知识学一遍。 今天要学的是泛型（文中代码除非特别说明，否则均摘自《Thinking in Java》(第四版)）。 什么是泛型 泛型是 Java SE5 引入的概念之一。所谓泛型就是指「适用于许许多多的类型」，即让程序自己去识别参数类型，而不是事先就将类型信息写死在代码中。Java SE5 之前是没法使用泛型的，这给 Java 泛型的设计添加了很多麻烦。 与C++的比较 书里关于泛型的介绍涵盖了整整一章，而且几乎是书里最厚的一章。初学时的我靠着一点 C++ 模板的基础，学了点语法糖就混过去了。然而事实上 Java 的泛型远不如 C++ 灵活，有点类似补丁的作用。 C++ 泛型的代码一般是这样的： 123456789101112131415161718192021222324252627#include &lt;iostream&gt;using namespace std;template&lt;class T&gt;class Manipulator &#123; T obj;public: Manipulator(T x) &#123; obj = x; &#125; void manipulate() &#123; obj.f(); &#125;&#125;;class HasF &#123;public: void f() &#123; cout &lt;&lt; \"HasF::f()\" &lt;&lt; endl; &#125;&#125;;int main() &#123; HasF hf; Manipulator&lt;HasF&gt; manipulator(hf); manipulator.manipulate();&#125; 结果会输出：HasF::f() 但如果同样翻译成Java版的： 1234567891011121314151617181920212223class Manipulator&lt;T&gt; &#123; private T obj; public Manipulator(T x) &#123; obj = x; &#125; public void manipulate() &#123; obj.f(); &#125;&#125;class HasF &#123; public void f() &#123; System.out.println(\"HasF.f()\"); &#125;&#125;;public class Manipulation &#123; public static void main(String[] args) &#123; HasF hf = new HasF(); Manipulator&lt;HasF&gt; manipulator = new Manipulator&lt;HasF&gt;(hf); Manipulator.manipulate(); &#125;&#125; 编译器却会报错：Error: cannot find symbol: method f() 为什么 C++ 里的泛型 T 可以找到 f 方法呢？很简单，当你实例化这个模板时，C++ 编译器会进行检查，因此在 Manipulator\\&lt;HasF\\&gt; 被实例化时，它检查到 HasF 存在一个 f 方法，所以编译通过，否则会报错。但 Java 的编译器却走了相反的道路：它干脆将类型信息「擦除」了。在 Java 的编译器看来，Manipulator\\&lt;T\\&gt; 中的 T 都被默认当作 Object 类型，因此找不到 f 方法。因此，为了实现上面的功能，我们要给定泛型的边界，以此告知编译器只能接受遵循这个边界的类型。具体做法是使用 extends 关键字，将上面代码中的 &lt;T&gt; 改为 &lt;T extends HasF&gt;。这样编译器知道T必须是 HasF 或其子类，因此可以调用 f 方法。 但聪明的读者很快会发现，这种做法完全可以这样实现： 123456789class Manipulator3 &#123; private HasF obj; public Manipulator3(HasF x) &#123; obj = x; &#125; public void manipulate() &#123; obj.f(); &#125;&#125; 这样泛型还有什么卵用呢？ Bruce 在书中说了这样一段话：只有当你希望使用的类型参数比某个具体类型（以及它的所有子类型）更加“泛化”时——也就是说，当你希望代码能够跨多个类工作时，使用泛型才有所帮助。 而事实上，以我浅薄的见识，泛型的主要作用是可以利用编译器来检查类型。例如：ArrayList&lt;String&gt; 总比 ArrayList&lt;Object&gt; 的作用要强些吧，至少当你传入非 String 类型（包括 String 的子类）的对象时，前者能够报错。 「擦除」的来历 所谓「擦除」，我的理解是：在编译期间，Java 的编译器不会像 C++ 的编译器一样去将类型参数 T 实例化。为什么 Java 要提供这种看似鸡肋的泛型呢？根本原因在于 Java 从诞生之初就没考虑过引入泛型功能。因此，JavaSE5 之前的类库都不具有泛型功能。为了能够兼容之前的类库，不得不弱化泛型的能力。总之，这是为了减少 bug 而提出的折中方案。 「擦除」的问题 因为「擦除」抹去了所有类型信息，所以转型、instanceof 操作都无法使用了。对于这样的代码： 12345class Foo&lt;T&gt; &#123; T var;&#125;Foo&lt;Cat&gt; f = new Foo&lt;Cat&gt;(); 在编译器看来，你的 Cat 都是 Object 类型的，除非你使用 extends。 “根据我的经验，理解了边界所在，你才能成为程序高手。因为只有知道了某个技术不能做到什么，你才能更好地做到所能做的（部分原因是，不必浪费时间在死胡同里乱转）” ——Bruce Eckel","raw":null,"content":null,"categories":[{"name":"Java","slug":"Java","permalink":"https://jermmy.github.io/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://jermmy.github.io/tags/Java/"}]},{"title":"从贝塞尔曲线反推控制点","slug":"2016-8-1-Bezier-Curve-SVG","date":"2016-08-01T15:17:19.000Z","updated":"2017-09-03T09:46:30.000Z","comments":true,"path":"2016/08/01/2016-8-1-Bezier-Curve-SVG/","link":"","permalink":"https://jermmy.github.io/2016/08/01/2016-8-1-Bezier-Curve-SVG/","excerpt":"由之前的文章我们可以得到贝塞尔曲线的方程，今天要通过贝塞尔曲线(三次)重新推出控制点。\n需求\n在得到并对贝塞尔曲线做完处理后，为了让浏览器重新渲染贝塞尔曲线，必须通过贝塞尔曲线重新取得控制点坐标。","text":"由之前的文章我们可以得到贝塞尔曲线的方程，今天要通过贝塞尔曲线(三次)重新推出控制点。 需求 在得到并对贝塞尔曲线做完处理后，为了让浏览器重新渲染贝塞尔曲线，必须通过贝塞尔曲线重新取得控制点坐标。 准备条件 了解 SVG 的 path 中 C/c 相关指令的用法，还有相对位置等一些概念。最好能提前获得贝塞尔曲线的表达式。 解决方案 假设我们已经得到了贝塞尔曲线的表达式： \\[ \\overline{P^3} = (1-t)^3\\overline{P_{0}} + 3t(1-t)^2\\overline{P_{1}}+3t^2(1-t)\\overline{P_{2}}+t^3\\overline{P_{3}} \\tag{1} \\] 其中，$ $ 是三次贝塞尔曲线上的点，\\(\\overline{P_{0}}\\)、\\(\\overline{P_{1}}\\)、\\(\\overline{P_{2}}\\)、\\(\\overline{P_{3}}\\) 分别是贝塞尔曲线的控制点，因为 \\(\\overline{P_{0}}\\)、\\(\\overline{P_{3}}\\) 本身就是贝塞尔曲线的两个端点，所以它们的坐标是事先知道的，我们的目标是要求出 \\(\\overline{P_{1}}\\)、\\(\\overline{P_{2}}\\)。注意到，贝塞尔曲线的点是 t 由 0 逐渐增加到 1 的过程中采样得到的（数学上需要对 t 取极限，但计算机是离散的，所以称为采样），因为项目中，我是通过原控制点得到贝塞尔曲线后，对曲线做形变处理，然后再反推控制点，所以我只需要在原来的贝塞尔曲线表达式中分别取 t=\\(\\frac{1}{3}\\) 和 \\(\\frac{2}{3}\\) ( t 的取值可以是 0 到 1 之间任意实数，当然不能是 0 和 1，不然就和端点重合了)，就可以得到两个 \\(\\overline{P^3}\\) 点的坐标，形变处理完后，我同样近似地认为这两个点是新曲线中 t 取 \\(\\frac{1}{3}\\) 和 \\(\\frac{2}{3}\\) 的坐标点。这样，我们相当于知道了四个点的坐标，对于一个二元一次方程，我们由这四个点可以得到两组方程，最终一定可以把 \\(\\overline{P_{1}} 、\\overline{P_{2}}\\) 解出来。对于原表达式不知道的情况，可以根据端点坐标来近似取点，具体可以看参考链接。 step1 对于 (1) 式，令 t＝\\(\\frac{1}{3}\\)，我们得到： \\[ \\overline{P^3}=(\\frac{2}{3})^3\\overline{P_{0}}+(\\frac{2}{3})^2\\overline{P_{1}}+3(\\frac{1}{3})^2\\frac{2}{3}\\overline{P_{2}}+(\\frac{1}{3})^3\\overline{P_{3}} \\] \\[ \\overline{P^3}-\\frac{8}{27}\\overline{P_{0}}-\\frac{1}{27}\\overline{P_{3}}=\\frac{4}{9}\\overline{P_{1}}+\\frac{2}{9}\\overline{P_{2}} \\tag{2} \\] 因为 \\(\\overline{P^3}\\)、\\(\\overline{P_{0}}\\)、\\(\\overline{P_{3}}\\) 的坐标是事先已知的，所以可以设 \\(\\overline{P^3}-\\frac{8}{27}\\overline{P_{0}}-\\frac{1}{27}\\overline{P_{3}}\\) 为 \\(\\overline{B}\\)。我们设 \\(\\overline{P_{1}}\\)、\\(\\overline{P_{2}}\\) 的坐标分别为 (x1, y1)、(x2, y2)，\\(\\overline{B}\\) 的坐标为(\\(x_{b}\\)， \\(y_{b}\\))，由 (2) 式可以得到如下方程： \\[ \\frac{4}{9}x_{1}+\\frac{2}{9}x_{2}=x_{b} \\] \\[ \\frac{4}{9}y_{1}+\\frac{2}{9}y_{2}=y_{b} \\] step2 按照 step1 的思路，令 t=\\(\\frac{2}{3}\\)，可以得到： \\[ \\overline{P^3}-\\frac{1}{27}\\overline{P_{0}}-\\frac{8}{27}\\overline{P_{3}}=\\frac{2}{9}\\overline{P_{1}}+\\frac{4}{9}\\overline{P_{2}} \\tag{3} \\] 令 \\(\\overline{P^3}-\\frac{1}{27}\\overline{P_{0}}-\\frac{8}{27}\\overline{P_{3}}\\) 为 \\(\\overline{C}\\)，设 \\(\\overline{C}\\) 的坐标为 (\\(x_{c}\\), \\(y_{c}\\))，同样的可以得到另一组方程： \\[ \\frac{2}{9}x_{1}+\\frac{4}{9}x_{2}=x_{c} \\] \\[ \\frac{2}{9}y_{1}+\\frac{4}{9}y_{2}=y_{c} \\] step3 联立 step1 和 step2 的方程组，最终可以吧 \\(\\overline{P_{1}}\\)、\\(\\overline{P_{2}}\\) 的坐标求出来。 \\[ x_{1}=3x_{b}-\\frac{3}{2}x_{c} \\] \\[ y_{1}=3y_{b}-\\frac{3}{2}y_{c} \\] \\[ x_{2}=3x_{c}-\\frac{3}{2}x_{b} \\] \\[ y_{2}=3y_{c}-\\frac{3}{2}y_{b} \\] 代码实现 12345678910111213141516171819202122232425262728293031323334353637383940414243/*** 计算第二，第三个控制点的坐标**/void get_control_points(Point &amp;p1, Point &amp;thirdOne, Point &amp;thirdTwo, Point &amp;p4, Point &amp;p2, Point &amp;p3) &#123; double xb1, yb1, xb2, yb2; // 计算的中间变量 double f1 = 0.037037037037037037037; // (1/3)^3 double f2 = 0.296296296296296296296; // (2/3)^3 double x2, y2, x3, y3; // 返回的p2、p3的坐标 xb1 = thirdOne.x - f2 * p1.x - f1 * p4.x; yb1 = thirdOne.y - f2 * p1.y - f1 * p4.y; xb2 = thirdTwo.x - f1 * p1.x - f2 * p4.x; yb2 = thirdTwo.y - f1 * p1.y - f2 * p4.y; x2 = 3 * xb1 - 3 / (double)2 * xb2; y2 = 3 * yb1 - 3 / (double)2 * yb2; x3 = 3 * xb2 - 3 / (double)2 * xb1; y3 = 3 * yb2 - 3 / (double)2 * yb1; p2.x = (int)x2; p2.y = (int)y2; p3.x = (int)x3; p3.y = (int)y3;&#125;/*** points里面，除了贝塞尔曲线前后的端点外，还包括曲线上1/3和2/3位置的点。* 返回贝塞尔曲线的控制点*/vector&lt;Point&gt; regain_new_points(vector&lt;Point&gt;&amp; points) &#123; vector&lt;Point&gt; newPoints; for (int i = 0; i &lt; points.size()-1; i+=3) &#123; Point p2, p3; get_control_points(points[i], points[i+1], points[i+2], points[i+3], p2, p3); if (i == 0) &#123; newPoints.push_back(points[i]); &#125; newPoints.push_back(p2); newPoints.push_back(p3); newPoints.push_back(points[i+3]); &#125; return newPoints;&#125; 测试结果 test 右图的脸为原 svg 在浏览器中的展示效果，左图的脸部轮廓白点为根据 svg 的控制点绘制出贝塞尔曲线后，再根据贝塞尔曲线反推的控制点坐标，而左图的脸部曲线则是根据这些控制点绘制出的贝塞尔曲线。从效果上看，两张人脸的轮廓基本很相似。 接下来，我对贝塞尔曲线做了平移、缩放和旋转，看看重新推出的控制点以及贝塞尔曲线的情况： 平移操作 translate 缩放操作 scale 旋转操作 rotate 看得出，这些基本的线性变换都不会导致贝塞尔曲线「失真」:-)，这是个好消息。 参考 Algorithm for deriving control points of a bezier curve from points along that curve?","raw":null,"content":null,"categories":[{"name":"计算机图形学","slug":"计算机图形学","permalink":"https://jermmy.github.io/categories/计算机图形学/"}],"tags":[{"name":"SVG","slug":"SVG","permalink":"https://jermmy.github.io/tags/SVG/"},{"name":"计算机图形学","slug":"计算机图形学","permalink":"https://jermmy.github.io/tags/计算机图形学/"}]},{"title":"Java-Deep and shallow copy","slug":"2016-7-28-Java-Deep-and-shallow-copy","date":"2016-07-27T16:00:00.000Z","updated":"2017-03-25T01:18:27.000Z","comments":true,"path":"2016/07/28/2016-7-28-Java-Deep-and-shallow-copy/","link":"","permalink":"https://jermmy.github.io/2016/07/28/2016-7-28-Java-Deep-and-shallow-copy/","excerpt":"最近将一段 C++ 算法代码改成 Java 版本迁移到 android 平台的时候，发现我的 Java 底子有点薄。比方说，连 Java 深拷贝和浅拷贝都没搞清。","text":"最近将一段 C++ 算法代码改成 Java 版本迁移到 android 平台的时候，发现我的 Java 底子有点薄。比方说，连 Java 深拷贝和浅拷贝都没搞清。 深拷贝和浅拷贝 其实，Java 里面大部分赋值操作都属于浅拷贝。比如下面这个例子： 12345678public static void main(String[] args) &#123; ArrayList&lt;Integer&gt; l1 = new ArrayList&lt;Integer&gt;(); l1.add(0); ArrayList&lt;Integer&gt; l2 = l1; l2.add(1); System.out.println(l1); System.out.println(l2);&#125; 输出结果为： 12[0, 1][0, 1] 也就是说，l1 和 l2 指向的是同一份内存空间（这一点在 C++ 写多了后就容易遗忘）。 那现在我想实现深拷贝，也就是修改 l2 的值，却不影响 l1，该怎么做呢？Java 问世的时候就已经提供了解决方案，那就是 Object 的 clone 方法。看下面的例子： 1234567891011121314public class Person implements Cloneable &#123; int age; public Person(int a) &#123; age = a; &#125; public Person clone() &#123; return new Person(age); &#125; public String toString() &#123; return \"age: \" + age; &#125; &#125; 我们定义了一个 Person 类，并实现 Cloneable 接口和 Object 中的 clone 方法。这个方法会返回一个新的 Person 实例，其中成员变量和原 Person 一样，但它们属于不同的内存空间。其实我看了 Cloneable 接口的代码后，发现这个接口是空的，也就是说，它只是起到一个标识符的作用。但实际操作的时候我发现不实现这个接口也能正常运行，暂时没搞明白是否一定要实现它。下面看 main 函数： 1234567891011121314151617public class Test &#123; public static void main(String[] args) &#123; ArrayList&lt;Person&gt; l1 = new ArrayList&lt;Person&gt;(); l1.add(new Person(3)); ArrayList&lt;Person&gt; l2 = (ArrayList&lt;Integer&gt;)l1.clone(); l2.add(l1.get(0)); l2.add(l1.get(0).clone()); System.out.println(\"(l2 == l1)? \" + (l2 == l1)); System.out.println(\"l2.get(0)==l1.get(0)? \" + (l2.get(0)==l1.get(0))); System.out.println(\"l2.get(1)==l1.get(0)? \" + (l2.get(1)==l1.get(0))); System.out.println(\"l2.get(2)==l1.get(0)? \" + (l2.get(2)==l1.get(0))); System.out.println(l1); System.out.println(l2); &#125;&#125; 输出结果： 123456(l2 == l1)? falsel2.get(0)==l1.get(0)? truel2.get(1)==l1.get(0)? truel2.get(2)==l1.get(0)? false[age: 3][age: 3, age: 3, age: 3] 可以看到，只有最后一次 l2.add(l1.get(0).clone()); 的时候做了内存的拷贝，之前的不管是新建 ArrayList 还是 add 元素，链表内部的元素都只是简单拷贝一下引用，指向的内存地址是一模一样的。 好了，既然 clone 方法可以返回新的内存空间，那是不是每次要用到深拷贝的时候就覆写这个方法即可呢？是的，这种做法肯定是有效的，但还要看你覆写的方式对不对。比如上面的例子中，我特意使用了 ArrayList 的 clone 方法，但是原本 l1 的元素还是被浅拷贝到 l2，虽然 l1 和 l2 这两个链表的内存不在一块了，但它们内部含有的Person引用却还是指向同一块地址，这就很蛋疼了。我比较好奇为什么内置的数据结构这一点也没做完善，就翻看了一下源码(Android平台的)： 12345678910111213141516/** * Returns a new &#123;@code ArrayList&#125; with the same elements, the same size and * the same capacity as this &#123;@code ArrayList&#125;. * * @return a shallow copy of this &#123;@code ArrayList&#125; * @see java.lang.Cloneable */@Override public Object clone() &#123; try &#123; ArrayList&lt;?&gt; result = (ArrayList&lt;?&gt;) super.clone(); result.array = array.clone(); return result; &#125; catch (CloneNotSupportedException e) &#123; throw new AssertionError(); &#125;&#125; 人家的说明里面已经很明显地告知这是一个 shallow copy。我看到源码里面对元素数组做了一遍 clone: result.array = array.clone();，显然就是这一步导致浅拷贝，于是又上网查了一下 Java 数组的 clone 实现，一句话就让真相大白于天下了：Java 数组的 clone 方法会逐个复制数组内的值。什么意思呢？如果这个数组是基本数据类型的话，就直接复制元素值，由于基本数据类型不是对象，直接赋值就相当于做了“深拷贝”，但如果数组里的元素都是一个个引用呢？也是直接复制这些引用的值，换句话说，数组的 clone 方法就是新建一个数组，然后这个把引用的值拷贝一遍，这样，两个数组内的元素指向的内存地址还是一样的。这就是为什么调用 ArrayList 的 clone 方法是浅拷贝的原因（因为 ArrayList 里面只能存放对象）。就是这个问题坑了我不少时间，毕竟 C++ 每次 push_back 都是深拷贝，我就惯性思维了囧。那我还是想对 ArrayList 做深拷贝怎么办呢？其实方法也很简单很弱智，直接 new 一个新的ArrayList，然后遍历一下，add(l1.get(i).clone()) 就可以了，注意强制类型转换。 看到这里，如果稍加思考的话会发现一个问题：要想让 clone 方法真正实现深拷贝，我们要逐个 clone 对象内部的对象，比方说，如果我有一个 Company 类，内部又有一个 Department 类，然后内部继续嵌套 Director 类、Employee 类之类的，那每次覆写 clone 方法的时候，我们都要把内部所有这些类都 clone 一遍，而且一旦添加或删除某个类，还要再修改一遍，简直蛋疼，有没有什么方法可以一键拷贝呢？有的，Java 提供了另一种序列化的方法，让虚拟机自动帮我们做这些繁琐的操作。因为目前项目里不会涉及到这些，所以暂时就不写了，后面的参考链接会详细讲解如何使用。 参考： Java对象克隆（Clone）及Cloneable接口、Serializable接口的深入探讨 Java基础笔记 – 对象的深复制与浅复制 实现Cloneable接口实现深复制 序列化实现深复制 java 数组复制:System.arrayCopy 深入解析 Java 数组 浅拷贝与深拷贝 How to clone ArrayList and also clone its contents?","raw":null,"content":null,"categories":[{"name":"Java","slug":"Java","permalink":"https://jermmy.github.io/categories/Java/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://jermmy.github.io/tags/Java/"}]},{"title":"用双线性插值做旋转缩放操作","slug":"2016-7-18-CV-旋转平移缩放","date":"2016-07-17T16:00:00.000Z","updated":"2019-05-02T03:50:54.000Z","comments":true,"path":"2016/07/18/2016-7-18-CV-旋转平移缩放/","link":"","permalink":"https://jermmy.github.io/2016/07/18/2016-7-18-CV-旋转平移缩放/","excerpt":"缩放操作\n缩放无非就是放大和缩小两种，不管是哪种操作，都可以得到长宽各自缩放的比值。得到的新图要么是像素增加了（拉长），要么是像素减少了（缩短）。不管是哪种情况，都需要对新图中的像素值赋值（也叫插值）。本文只介绍一下最常见的最邻近插值和双线性插值。","text":"缩放操作 缩放无非就是放大和缩小两种，不管是哪种操作，都可以得到长宽各自缩放的比值。得到的新图要么是像素增加了（拉长），要么是像素减少了（缩短）。不管是哪种情况，都需要对新图中的像素值赋值（也叫插值）。本文只介绍一下最常见的最邻近插值和双线性插值。 什么是插值 首先讲讲什么叫插值。简单地说，给缩放后得到的新图的每一个像素点赋值，就叫做插值。最常见的插值方法分两种： 一、向前映射：遍历原图中的每一个像素点，根据映射关系可以计算出它们在新图中的位置（遇到小数的情况要取整），这个新位置可以直接取原图对应点的像素值，也可以取一个接近的值。这种方法可能导致新图中存在空洞点，即原图中的所有像素点都无法映射到这个位置点，另一个缺陷是，原图中两个不同的像素点可能映射到同一个位置，这样肯定会有一个像素的信息丢失了； 二、向后映射：这里我们是反过来求出新图到原图的映射变换（按照线性代数的知识，就是将向前映射取一个逆），然后遍历新图中的每个像素，计算出它们在原图中的对应位置，再取值。同样的，这个位置可能不是整数，简单的做法可以直接取整，比较好的做法是综合一下周围点的像素值取一个均值。前一种做法称为最邻近插值，后一种做法根据综合像素的方式又分为几种方法，这里只讲最常见的双线性插值。 最邻近插值 我们就拿一个方向的缩放为例。 假设右边两个像素点的图是原图，左边四个像素点的图是新图，对于这种宽度扩大两倍，高度保持不变的情况，最邻近插值的做法是这样：我们先计算出新图回到原图的缩放因子是 0.5，所以，对新图中的每个像素，我们计算出它们乘以 0.5 后得到的位置，这个位置是它们对应于原图的位置。新图中，1、2 像素点的位置分别为 (0,0)、(0,1)，缩小后得到的新位置为 (0,0)，也就是它们对应原图中的 1 像素点，所以它们的像素值都取原图中 1 像素点的值，同理，3、4 像素点的值取原图中 2 像素点的值。这样，新图的像素肯定能被插满。对于原图缩小的情况道理是一样的。 这种做法的优点是简单粗暴，计算量小且容易理解，缺点是由于太过简单粗暴，导致没有很好的利用原图的信息，比方说上图，原图被拉长后，1 和 2 之间的像素应该要有一个过渡，这样人眼看的时候才不会觉得突兀（或者说变化太快），图像中常见的锯齿就是这种由突兀产生的。 上一段简单的代码，用到了 CImg 库（太简单不解释）： 1234567891011121314151617181920CImg&lt;unsigned char&gt; nearest_scale(const CImg&lt;unsigned char&gt;&amp; srcImg, int width, int height) &#123; int srcWidth = srcImg.width(); int srcHeight = srcImg.height(); CImg&lt;unsigned char&gt; outImg(width, height, 1, 3, 0); // 缩放因子 double width_scale = srcWidth / (double)width; double height_scale = srcHeight / (double)height; for (int r = 0; r &lt; height; r++) &#123; for (int c = 0; c &lt; width; c++) &#123; outImg(c, r, 0, 0) = srcImg((int)(c*width_scale), (int)(r*height_scale), 0, 0); outImg(c, r, 0, 1) = srcImg((int)(c*width_scale), (int)(r*height_scale), 0, 1); outImg(c, r, 0, 2) = srcImg((int)(c*width_scale), (int)(r*height_scale), 0, 2); &#125; &#125; return outImg;&#125; 双线性插值 既然有双线性插值，我们不妨先看下单线性插值。 上图中，我们一样假设右图为原图，左图为缩放后的新图。我们通过计算缩放因子，发现左图中 2 这个像素点还原到新图后，对应的坐标不是整数，它恰好落在原图两个像素点之间 0.75 : 0.25 的位置（注意最邻近插值有个取整操作，把位置上的这点细节忽略了）。此时，为了更好地取值，我们不是简单粗暴地用原图中的一个像素值去取代新图，而是根据距离的比值取了一个综合的结果。线性插值的做法是，根据距离比 0.75 : 0.25，对原像素值取一个加权平均，距离近的，像素值要接近，所以权值更大，应该取 0.75，距离远的则相反，取 0.25，这样一来，原图中的两个像素值，根据距离的反比取一个加权平均，就是新图中 2 这个像素的值了。双线性插值道理同上，只不过推广到二维的情况： 除了水平方向，我们发现高度其实也不是整数，双线性插值会进一步再考虑高度的距离比，假设宽度的距离比是 u : (1-u)，高度的距离比是 v : (1-v)，此时，我们要综合考虑周围四个点的情况，比方说，对于原图右下角的像素值，对应的加权值为 (1-0.25)*(1-0.25)，而右上角的像素值，对应的加权值为 (1-0.75)*(1-0.25)，左边的类似，最后将这些值全部加起来就是新的像素值了。 示例代码： 12345678910111213141516171819202122232425262728293031CImg&lt;unsigned char&gt; bilinear_scale(const CImg&lt;unsigned char&gt;&amp; srcImg, int width, int height) &#123; int srcWidth = srcImg.width(); int srcHeight = srcImg.height(); CImg&lt;unsigned char&gt; outImg(width, height, 1, 3, 0); // 缩放因子 double width_scale = srcWidth / (double)width; double height_scale = srcHeight / (double)height; // 新图对应原图的坐标 double srcX, srcY, u, v; for (int r = 0; r &lt; height; r++) &#123; for (int c = 0; c &lt; width; c++) &#123; srcX = c * width_scale; srcY = r * height_scale; u = srcX - (int)srcX; v = srcY - (int)srcY; for (int channel = 0; channel &lt; 3; channel++) &#123; outImg(c, r, 0, channel) = (int)((1-u)*(1-v)*srcImg(valueWidth(srcX, srcWidth), valueHeight(srcY, srcHeight), 0, channel) +(1-u)*v*srcImg(valueWidth(srcX, srcWidth), valueHeight(srcY+1, srcHeight), 0, channel) +u*(1-v)*srcImg(valueWidth(srcX+1, srcWidth), valueHeight(srcY, srcHeight), 0, channel) +u*v*srcImg(valueWidth(srcX+1, srcWidth), valueHeight(srcY+1, srcHeight), 0, channel)); &#125; &#125; &#125; return outImg;&#125; 旋转操作 旋转跟缩放本质上是一样的，只是映射变换不同。 先看看映射函数怎么求。 假设点 (\\(x_0\\), \\(y_0\\)) 逆时针旋转到 (\\(x_1\\), \\(y_1\\)) 的位置，旋转角度为 A，(\\(x_1\\), \\(y_1\\)) 与 x 轴正方向的夹角为 B，且知道点 (\\(x_0\\), \\(y_0\\)) 到原点的距离为 r (这个量只是起到中间变量的作用)。现在需要求出 \\(x_0\\)，\\(y_0\\)，\\(x_1\\)，\\(y_1\\) 之间的映射关系。 根据高中三角函数的知识，可以列出如下等式： \\[x_0=r cos(B-A)=rcosBcosA+rsinBsinA \\tag{1}\\] \\[y_0=r sin(B-A)=r sinBcosA-rsinAcosB \\tag{2}\\] \\[x_1=r cosB \\tag{3}\\] \\[y_1=r sinB \\tag{4}\\] 将 (3) (4) 式分别代入 (1) (2) 得到： \\[x_0 = x_1 cosA + y_1 sinA \\tag{5} \\] \\[y_0=y_1cosA-x_1sinA \\tag{6} \\] 这样，向后映射的表达式我们就求出来了。 接着，由 (5) (6) 式分别可得： \\[y_1=\\frac{x_0-x_1cosA}{sinA} \\tag{7}\\] \\[y_1=\\frac{y_0+x_1sinA}{cosA} \\tag{8}\\] 再由 (7) = (8) 可以解出： \\[ x_1=x_0*cosA-y_0*sinA \\] 同样的方法可以解出： \\[ y_1=y_0*cosA+x_0*sinA \\] 这样，向前映射也得到了。之后，按照缩放里面的思路，我们只要先算出新图的大小(根据向前映射可以求得)，然后遍历新图里的像素，根据向后映射反推在原图中的像素位置，再用插值的方法就可以给每个像素点赋值了。要注意的一点是，我们这里是以原点为旋转中心计算出来的表达式，如果直接套用上面的计算结果，那么你得到的新图像是原图绕左上角的点旋转A度角后得到的图（旋转方向取决于 A 的符号）。这是因为计算机中，图片的坐标原点在左上角。如果要让图片让自己的中心旋转，必须先将图片的中心与左上角原点对齐（只需做一下平移即可），旋转完成之后，再按照之前平移的距离方向重新调整位置。 示例代码（采用双线性插值）： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980/* * 旋转图片，旋转角为theta，并采用双线性插值减少锯齿*/CImg&lt;unsigned char&gt; rotate_biliinear(const CImg&lt;unsigned char&gt;&amp; srcImg, double theta) &#123; int width = srcImg.width(); int height = srcImg.height(); // 原图的四个顶点坐标，这里以图片中心为坐标原点 Position lt(0-width/2, 0+height/2), lb(0-width/2, 0-height/2), rt(0+width/2, 0+height/2), rb(0+width/2, 0-height/2); // 获得旋转后的图片的四个顶点坐标 Position new_lt((int)(lt.x*cos(theta)+lt.y*sin(theta)), (int)(lt.y*cos(theta)-lt.x*sin(theta))), new_lb((int)(lb.x*cos(theta)+lb.y*sin(theta)), (int)(lb.y*cos(theta)-lb.x*sin(theta))), new_rt((int)(rt.x*cos(theta)+rt.y*sin(theta)), (int)(rt.y*cos(theta)-rt.x*sin(theta))), new_rb((int)(rb.x*cos(theta)+rb.y*sin(theta)), (int)(rb.y*cos(theta)-rb.x*sin(theta))); int newWidth = max(abs(new_rt.x-new_lb.x), abs(new_lt.x-new_rb.x)); int newHeight = max(abs(new_lt.y-new_rb.y), abs(new_lb.y-new_rt.y)); CImg&lt;unsigned char&gt; newImg(newWidth, newHeight, 1, 3, 0); // 开始填充新图片的灰度值 bilinear_interpolation(newImg, srcImg, theta); return newImg; &#125;/*** 采用双线性插值填充新图*/void bilinear_interpolation(CImg&lt;unsigned char&gt;&amp; outImg, const CImg&lt;unsigned char&gt;&amp; srcImg, double theta) &#123; int newWidth = outImg.width(), newHeight = outImg.height(); int srcWidth = srcImg.width(), srcHeight = srcImg.height(); int halfW = newWidth / 2; int halfH = newHeight / 2; double srcX, srcY, u, v; for (int r = 0; r &lt; newHeight; r++) &#123; for (int c = 0; c &lt; newWidth; c++) &#123; if (get_origin_pos(c-halfW, r-halfH, srcWidth, srcHeight, theta, srcX, srcY)) &#123; u = srcX - (int)srcX; v = srcY - (int)srcY; for (int channel = 0; channel &lt; 3; channel++) &#123; outImg(c, r, 0, channel) = (int)((1-u)*(1-v)*srcImg(valueWidth(srcX, srcWidth), valueHeight(srcY, srcHeight), 0, channel) +(1-u)*v*srcImg(valueWidth(srcX, srcWidth), valueHeight(srcY+1, srcHeight), 0, channel) +u*(1-v)*srcImg(valueWidth(srcX+1, srcWidth), valueHeight(srcY, srcHeight), 0, channel) +u*v*srcImg(valueWidth(srcX+1, srcWidth), valueHeight(srcY+1, srcHeight), 0, channel)); &#125; &#125; &#125; &#125;&#125;// 获得旋转后图片的像素对应于原图的像素位置，用于双线性插值bool get_origin_pos(int x, int y, int srcWidth, int srcHeight, double theta, double&amp; srcX, double&amp; srcY) &#123; // 找到(x, y)在原图中对应的位置(srcX, srcY) srcX = (double)x * cos(theta) - (double)y * sin(theta); srcY = (double)x * sin(theta) + (double)y * cos(theta); if (srcX &gt;= (0-srcWidth/2-1) &amp;&amp; srcX &lt;= srcWidth/2+1 &amp;&amp; srcY &gt;= (0-srcHeight/2-1) &amp;&amp; srcY &lt;= srcHeight/2+1) &#123; srcX += srcWidth/2; srcY += srcHeight/2; return true; &#125; else &#123; return false; &#125;&#125;// 检查像素位置，防止超过图片宽度int valueWidth(double srcX, int width) &#123; if (srcX &lt; 0) srcX = 0; if (srcX &gt;= width) srcX--; return srcX;&#125;// 检查像素位置，防止超过图片高度int valueHeight(double srcY, int height) &#123; if (srcY &lt; 0) srcY = 0; if (srcY &gt;= height) srcY--; return srcY;&#125; 最后放上最邻近插值和双线性插值的旋转效果图，可以看到最邻近插值的锯齿略多于双线性插值 （原图） （最邻近插值） （双线性插值） 参考 图像旋转算法原理– 旋转矩阵","raw":null,"content":null,"categories":[{"name":"图像处理","slug":"图像处理","permalink":"https://jermmy.github.io/categories/图像处理/"}],"tags":[{"name":"图像处理","slug":"图像处理","permalink":"https://jermmy.github.io/tags/图像处理/"}]},{"title":"利用SVG中的控制点绘制贝塞尔曲线","slug":"2016-7-17-Bezier-Curve-SVG","date":"2016-07-16T16:00:00.000Z","updated":"2017-09-03T09:29:47.000Z","comments":true,"path":"2016/07/17/2016-7-17-Bezier-Curve-SVG/","link":"","permalink":"https://jermmy.github.io/2016/07/17/2016-7-17-Bezier-Curve-SVG/","excerpt":"需求\nSVG 标准指令中的 C/c 可以用于构造三次贝塞尔曲线(cube bezier curve)，具体用法是：X0, Y0 C X1 Y1, X2 Y2, X3 Y3，这里面的 X、Y 用的是绝对坐标，它们代表三次贝塞尔曲线的控制点（ X0， Y0 和 X3、Y3 恰好是曲线前后端点，所以实际上只有两个控制点）。但我的目的是想对贝塞尔曲线做一次 wrap，而这些控制点并不一定就在曲线上，所以必须先把曲线求出来，再对曲线做 wrap 形变。","text":"需求 SVG 标准指令中的 C/c 可以用于构造三次贝塞尔曲线(cube bezier curve)，具体用法是：X0, Y0 C X1 Y1, X2 Y2, X3 Y3，这里面的 X、Y 用的是绝对坐标，它们代表三次贝塞尔曲线的控制点（ X0， Y0 和 X3、Y3 恰好是曲线前后端点，所以实际上只有两个控制点）。但我的目的是想对贝塞尔曲线做一次 wrap，而这些控制点并不一定就在曲线上，所以必须先把曲线求出来，再对曲线做 wrap 形变。 准备条件 首先，总得先知道贝塞尔曲线是什么吧~~，这里推荐一篇扫盲文章贝塞尔曲线扫盲，重点要知道曲线中的那些比例关系。其次，既然我们是要从 SVG 的控制点来获得贝塞尔曲线，那总得知道 SVG 是什么以及它的标准语法，这里再推荐一篇深度好文深度掌握SVG路径path的贝塞尔曲线指令，重点是要了解 SVG 中的 path 以及 C/c 相关的指令的用法，还有相对位置等一些概念。 解决方案 好了，既然已经知道贝塞尔曲线以及 SVG 是什么东西了，那么想要自己绘制贝塞尔曲线（曲线要画的多好取决于 wrap 的要求），我们得先拿到控制点（本文只针对 SVG 中的 C/c 指令，也就是三次贝塞尔曲线）。思路就很清晰了：先解析 SVG 的 path，拿到控制点，然后根据控制点绘制曲线，done。 step1：如何根据控制点绘制贝塞尔曲线 先搞定这个大头~~。其实只要了解贝塞尔曲线的概念以及它是怎么画出来的，我发现这其实是个小头。我稍微看了下这篇文章贝塞尔曲线原理(简单阐述)，用仅存的一点高中向量知识就画出来了。这里把我看上文时的思考结果记录一下（以下贝塞尔曲线相关的图皆取自上文）。 我们先看看二次贝塞尔曲线怎么画的（所谓二次就是除了前后端点外，只有一个控制点的情况）。 21154420-e9c48409b7d44b9baedc180352f6eb29 （注意，上面所标点中，右上角的数字不是幂指数，而表示第几次取点），对于二次贝塞尔曲线，我们的目标就是求出一连串的 $P_{0}^2 $，这些 \\(P_{0}^2\\) 最终构成我们的贝塞尔曲线，当然，对于计算机这种离散化的工具，点要取得多密集取决于你自己的要求。下面，要灵活运用比例关系以及向量工具了： \\[ \\frac{|\\overline {P_{0} P_{0}^1}|}{|\\overline{P_{0}P_{1}}|}=\\frac{|\\overline {P_{1} P_{1}^1}|}{|\\overline {P_{1}P_{2}}|}=\\frac{|\\overline {P_{0}^1 P_{0}^2}|}{|\\overline {P_{0}^1P_{1}^1}|}=\\frac{t}{1} \\ \\ \\ ( t\\in [0, 1]) \\] 上式中的 || 表示向量的模，因为我印象中向量好像是不能直接相比的，会涉及到方向问题，但在本例中，相比的向量方向都是一致的，所以我们可以把 || 去掉： \\[ \\frac{\\overline {P_{0} P_{0}^1}}{\\overline{P_{0}P_{1}}}=\\frac{\\overline {P_{1} P_{1}^1}}{\\overline {P_{1}P_{2}}}=\\frac{\\overline {P_{0}^1 P_{0}^2}}{\\overline {P_{0}^1P_{1}^1}}=\\frac{t}{1} \\ \\ \\ (t\\in [0, 1]) \\] 然后改变一下分母（结合图像）： \\[ \\frac{\\overline {P_{0} P_{0}^1}}{\\overline{P_{0}^1P_{1}}}=\\frac{\\overline {P_{1} P_{1}^1}}{\\overline {P_{1}^1P_{2}}}=\\frac{\\overline {P_{0}^1 P_{0}^2}}{\\overline {P_{0}^2P_{1}^1}}=\\frac{t}{1-t} \\ \\ \\ (t\\in [0, 1]) \\] 下一步，求出 \\(\\overline {p_{0}^1}\\)，由上式可得： \\[ (1-t)\\overline{P_{0}P_{0}^1}=t\\overline{P_{0}^1P_{1}} \\\\\\\\ (1-t)(\\bar{P_{0}^1}-\\bar{P_{1}})=t(\\bar{P_{1}}-\\bar{P_{0}}) \\] 注意这一步开始将向量拆成两个向量相减的形式，这样后者中的向量我们可以直接用坐标表示了（比如：\\(\\bar P_{0}\\) 就表示 \\(\\bar P_{0}\\) 这个点本身的坐标）。将上式化简得到： \\[ \\overline {P_{0}^1} = (1-t)\\overline{P_{0}}+t\\overline{P_{1}} \\tag{1} \\] 同样的方法求解出 \\(\\overline{P_{1}^1}\\) 和 \\(\\overline{P_{0}^2}\\)： \\[ \\overline {P_{1}^1} = (1-t)\\overline{P_1}+t\\overline{P_2} \\tag{2} \\] \\[ \\overline{P_0^2}=(1-t)\\overline{P_0^1}+t\\overline{P_1^1} \\tag{3} \\] 将 (1)(2) 式代入 (3) 式并化简可得： \\[ \\overline{P_{0}^2}=(1-t)^2 \\overline{P_{0}} + 2t(1-t) \\overline{P_{1}} + t^2 \\overline{P_{2}} \\] 如果耐心看到这一步，那么二次贝塞尔曲线就应该会画了，因为三个点的坐标已知，需要做的就是改变 t 的值来重复得到 \\(\\overline{P_{0}^2}\\) 的坐标，这个 t 的值在 [0,1] 之间变化，变化的间隔决定了曲线点的疏密程度，下面是计算二次贝塞尔曲线的代码片段，用了「CImg」库，t 的间隔取 0.01，参照上面的计算结果很快就明白了: 12345678void draw_quad_bezier(CImg&lt;unsigned char&gt; &amp;img, Point p0, Point p1, Point p2) &#123; int x, y; for (float t = 0; t &lt;= 1; t += 0.001) &#123; x = (int)((1-t)*(1-t)*p0.x + 2*t*(1-t)*p1.x + t*t*p2.x); y = (int)((1-t)*(1-t)*p0.y + 2*t*(1-t)*p1.y + t*t*p2.y); draw_point(img, x, y); &#125;&#125; 效果图如下（三个点的坐标分别取 (100,100)，(500,1000)，(1500,200) ）： quad_bezier 好了，二次的我们会求了，那三次的怎么求呢？道理是一样的，不然前面码那么多公式不就浪费了嘛^_^ 观察一下三次贝塞尔曲线的图： 21152048-9b5dee31b19349428c453b8bd5e20a3d 你会发现，原来它就是在原来二次曲线的三个控制点外围又套了一层，也就是说，我们要先通过三次贝塞尔曲线的四个控制点，求出绿色那一条线的三个端点，再把这三个端点代入之前求出来的二次曲线的公式即可。道理就是这么简单，所以我也懒得码公式了，直接上一幅我的草稿图（纯粹给自己以后看的=.=）： 屏幕快照 2016-07-17 下午4.57.48 屏幕快照 2016-07-17 下午4.59.03 最后得出来的计算结果： \\[ \\overline{P_{0}^3}=(1-t)^3 \\overline{P_{0}}+3t(1-t)^2 \\overline{P_{1}}+3t^2(1-t) \\overline{P_{2}}+t^3 \\overline{P_{3}} \\] 来，照例给出代码片段: 12345678910void draw_cube_bezier(CImg&lt;unsigned char&gt; &amp;img, Point p0, Point p1, Point p2, Point p3) &#123; int x, y; for (float t = 0; t &lt;= 1; t += 0.001) &#123; x = (int)((1-t)*(1-t)*(1-t)*p0.x + 3*t*(1-t)*(1-t)*p1.x + 3*t*t*(1-t)*p2.x + t*t*t*p3.x); y = (int)((1-t)*(1-t)*(1-t)*p0.y + 3*t*(1-t)*(1-t)*p1.y + 3*t*t*(1-t)*p2.y + t*t*t*p3.y); draw_point(img, x, y); &#125;&#125; 效果图如下 cube_bezier 这样一来，三次贝塞尔曲线的绘制部分基本结束了。不过，虽然我们的目标是绘制三次贝塞尔曲线，也就是输入四个点输出一条曲线的情况，可如果遇到输入的点退化成三个的情况，我们的方法是否适用呢？（鉴于我的项目，这里主要指四个控制点中有两个控制点相同的情况，也就是起码有三个点不同，而且输入的控制点数量一定是 4 个）。 没有具体去证明，稍微假设我们上面得到的三次曲线的最终表达式中，\\(\\overline{P_{0}}\\) 和 \\(\\overline{P_{1}}\\) 相同，那么最终结果跟二次曲线的表达式还是有较大不同的，但实际操作时发现，这种不同在人眼可以忍受的误差范围内，下面是我尝试的一个例子： 二次贝塞尔曲线，输入点分别为：(100,100), (500,1000), (1500,200) quad_bezier_test 三次贝塞尔曲线，输入点分别为：(100,100), (100,100), (500,1000), (1500,200)，即前两个点一样 cube_bezier_test 可以看出形状上基本一样，也就是说，对于退化的情况，我们的算法基本可以得到一致的二次贝塞尔曲线。而且真实项目中，贝塞尔曲线的控制点都靠的比较近，因此人眼基本看不出区别，所以这里不再进一步细化算法。 step2: 如何提取控制点 要提取的 svg 中的控制点位于 path 标签内，大致如下： 1234567891011&lt;svg xmlns=\"http://www.w3.org/2000/svg\" width=\"450px\" height=\"600px\" viewBox=\"0 0 2550 3300\"&gt; ...... &lt;path class=\"fil0 str0\" d=\"M205 922c0,0 49,496 50,613 2,117 22,298 36,344 14,4525,67 49,101 23,35 162,172 220,213 58,40 131,79 154,85 24,5 92,15 135,17 43,3 116,-4 156,-23 40,-19 236,-141 267,-175 30,-35 100,-96 123,-158 23,-62 31,-6939,-110 8,-40 13,-166 16,-183 4,-18 23,-199 25,-247 1,-48 24,-302 25,-363 1,-61-10,-271 -33,-358 -24,-88 -109,-205 -166,-245 -49,-34 -114,-77 -218,-95 -104,-19-212,-31 -340,-11 -129,20 -232,65 -297,117 -64,51 -160,149 -188,205 -28,55 -53,146-56,175 -3,28 3,99 3,99l0 -1z\"/&gt;&lt;/svg&gt; 浏览器中显示的是一张人脸图： 屏幕快照 2016-07-17 下午9.43.52 当然我要提取的只是最外层的脸轮廓。 虽然有些素材的路径不太一致，但大部分素材的格式还是很接近的，所以我的想法是用字符串分割的方式把绝大部分素材的控制点都提取出来，对于比较特殊的素材，要么放弃，要么手动修改 svg，或者微调一下算法都可以。然而就是这看似简单的一步让我颓废了一个下午。原因不是 svg 的解析，也不是字符串的切割，而是控制点的还原。注意到上面的 svg 中，贝塞尔曲线用的是 c 指令，也就是相对位置，但我之前的算法要求使用绝对位置。于是，我的做法是：每隔四个点取一段，其中每段的最后一个点作为下一段的起点，在取的过程中，根据相对位置重新计算出点的绝对位置。这里的坑就在计算绝对位置这一步。我看了 w3school 和 mozilla 上的教程，说的都是：相对于上一个点的坐标，根据相对位置计算出下一个点的坐标，于是我简单地写了一个循环 1234for (int i = 1; i &lt; points.size(); i++) &#123; points.get(i).x += points.get(i-1).x; points.get(i).y += points.get(i-1).y;&#125; 但结果中，我惊异地发现出现了坐标值为负数的情况，反复检查后，发现问题只可能出现在计算绝对位置这一步，然后就是不停地 goolging，连百度都用上了，但是居然没查到类似的问题（当然可能是我查问题的姿势不对，不过当时怎么就没想到去查 SVG 的官方资料呢，看来还是懒=。=）。然后我又怀疑是不是浏览器内部对点的坐标做了归一化处理，也就是对越界的点做了平移。于是就把我的计算结果扔到 chrome 里面，结果很欣慰的，脸消失了一部分，那问题只可能是坐标计算错了。第二天，我就在 StackOverflow 上发了处女帖，Draw Bezier Curve with relative path in SVG，吃顿饭的功夫就收到了答复。在这里不得不佩服国外程序员（也可能是天朝的？）的 manner，让我这种平时只找答案不回答的人即感动又内疚。原来，对于贝塞尔曲线而言，相对位置的坐标是以上一段曲线或直线的终点，也就是本段贝塞尔曲线的起点为标准的，所以应该把代码改为： 12345678int lastEndPoint = 0;for (int i = 1; i &lt; points.size(); i++) &#123; points.get(i).x += points.get(lastEndPoint).x; points.get(i).y += points.get(lastEndPoint).y; if (i % 3 == 0) &#123; lastEndPoint = i; &#125;&#125; 然后我终于成功地画出一张人脸（右边为浏览器中的对照结果）： 仔细一看我发现，好像素材中的控制点也基本在贝塞尔曲线上啊，这样直接拿控制点做形变误差应该也不会差很多才对😂。然而都已经把曲线画出来了。。。。。。 参考 贝塞尔曲线扫盲 深度掌握SVG路径path的贝塞尔曲线指令 贝塞尔曲线原理(简单阐述) 还有一篇我提问的SO😢：Draw Bezier Curve with relative path in SVG","raw":null,"content":null,"categories":[{"name":"计算机图形学","slug":"计算机图形学","permalink":"https://jermmy.github.io/categories/计算机图形学/"}],"tags":[{"name":"SVG","slug":"SVG","permalink":"https://jermmy.github.io/tags/SVG/"},{"name":"计算机图形学","slug":"计算机图形学","permalink":"https://jermmy.github.io/tags/计算机图形学/"}]},{"title":"绘制三角形","slug":"2016-7-11-CV-绘制三角形","date":"2016-07-11T11:06:31.000Z","updated":"2017-04-23T05:42:15.000Z","comments":true,"path":"2016/07/11/2016-7-11-CV-绘制三角形/","link":"","permalink":"https://jermmy.github.io/2016/07/11/2016-7-11-CV-绘制三角形/","excerpt":"其实这应该属于图像处理的入门内容，因为跟三角形相关所以 mark 一下。\n需求\n我们要实现这样一个函数，输入参数是绘制的图片以及三角形三个点：\n1function(image, point1, point2, point3)\n简单起见，不考虑输入的点超过图片范围等异常情况，假设输入都合法，并且输入图片是像素全为 0 的黑图。任务是将三个点构成的区域像素值设为 255，即白色。","text":"其实这应该属于图像处理的入门内容，因为跟三角形相关所以 mark 一下。 需求 我们要实现这样一个函数，输入参数是绘制的图片以及三角形三个点： 1function(image, point1, point2, point3) 简单起见，不考虑输入的点超过图片范围等异常情况，假设输入都合法，并且输入图片是像素全为 0 的黑图。任务是将三个点构成的区域像素值设为 255，即白色。 准备条件 图像读取采用 Cimg 这个轻量级的库，编程语言采用 C++。 思路 绘制方法无非是遍历图片中的每个像素，判断该像素是否落在三角形内，是的话将像素值设为 255。那么重点便是如何判断一个像素是否在三角形内。这里参考了这篇文章中的第二种方法：Point in triangle test 。稍微讲解如下： 屏幕快照 2016-07-11 下午9.57.48 假设我们的三角形长这样，现在要让程序判断 P 点是否在三角形内。这里要用到一点高中向量的知识。假设 A、B、C 三点坐标分别为 (x1, y1)，(x2, y2)，(x3, y3)，P 点坐标是 (x, y)。根据向量知识，有这样一个等式：[PA]=u*[BA]+v*[CA] (这里的[]表示一个向量，不知道怎么用 md 表示向量，先将就一下)，如果 u+v&lt;=1 &amp;&amp; u&gt;=0 &amp;&amp; v&gt;=0 ，那么 P 就在三角形中，否则 P 在三角形外。因为 A、B、C、P 的坐标都是知道的，只要解出 u 和 v，就能判断出这个 P 点坐标的情况。那么如何求出这两个值呢？其实方法也非常简单，我们先把之前公式里的向量用坐标的形式表示，可以得出 [x-x1, y-y1]=u*[x2-x1, y2-y1]+v*[x3-x1, y3-y1] ，而后将 x、y 分离得到两个等式：x-x1=u*(x2-x1)+v*(x3-x1) 、y-y1=u*(y2-y1)+v*(y3-y1) ，两个方程两个未知数，可以求出： \\[ u=\\frac{(y-y1)(x3-x1)-(x-x1)(y3-y1)}{(y2-y1)(x3-x1)-(x2-x1)(y3-y1)} \\] \\[ v=\\frac{(y-y1)(x2-x1)-(x-x1)(y2-y1)}{(y3-y1)(x2-x1)-(x3-x1)(y2-y1)} \\] (可能有老眼昏花算错的地方) 代码 简单把上面的思路翻译一下就是代码了 12345678910111213141516171819void draw_triangle(CImg&lt;unsigned char&gt;&amp; srcImg, int x1, int y1, int x2, int y2, int x3, int y3) &#123; int width = srcImg.width(); int height = srcImg.height(); for (int i = 0; i &lt; width; i++) &#123; for (int j = 0; j &lt; height; j++) &#123; double vx1 = x2 - x1, vy1 = y2 - y1, vx2 = x3 - x1, vy2 = y3 - y1; double vx0 = i - x1, vy0 = j - y1; double u = (vx2*vy0 - vx0*vy2) / (double)(vx2*vy1 - vx1*vy2), v = (vx1*vy0 - vx0*vy1) / (double)(vx1*vy2 - vx2*vy1); if (u &gt;= 0 &amp;&amp; v &gt;= 0 &amp;&amp; u+v &lt;= 1) &#123; for (int channel = 0; channel &lt; 3; channel++) &#123; srcImg(i, j, 0, channel) = 0; &#125; &#125; &#125; &#125; &#125; 缺陷： 这种方法计算量不算大，但由于使用了除法，可能出现除数为 0 的情况（讲道理应该不可能，u、v 的值应该不会无穷大才对，不知道是否有前辈已经证明）。 参考： Point in triangle test","raw":null,"content":null,"categories":[{"name":"计算机视觉","slug":"计算机视觉","permalink":"https://jermmy.github.io/categories/计算机视觉/"}],"tags":[{"name":"计算机视觉","slug":"计算机视觉","permalink":"https://jermmy.github.io/tags/计算机视觉/"}]},{"title":"Shell脚本跑adb，快速替换.so","slug":"2016-7-8-Shell工具-shell脚本跑adb命令","date":"2016-07-08T06:08:16.000Z","updated":"2017-05-02T13:11:27.000Z","comments":true,"path":"2016/07/08/2016-7-8-Shell工具-shell脚本跑adb命令/","link":"","permalink":"https://jermmy.github.io/2016/07/08/2016-7-8-Shell工具-shell脚本跑adb命令/","excerpt":"NDK开发遇到的麻烦\n最近使用 Android Studio 开发 jni 程序时，遇到一个极其蛋疼的问题：AS 编译运行的速度实在是太慢了！！！而且 jni 开发的时候一定要先 clean 一遍，再重新 build 工程，整个过程总耗时＝上个厕所＋喝一杯咖啡，这在调试时尤为不便。于是我上网兜了一遍，终于找到一种更快捷的方法。","text":"NDK开发遇到的麻烦 最近使用 Android Studio 开发 jni 程序时，遇到一个极其蛋疼的问题：AS 编译运行的速度实在是太慢了！！！而且 jni 开发的时候一定要先 clean 一遍，再重新 build 工程，整个过程总耗时＝上个厕所＋喝一杯咖啡，这在调试时尤为不便。于是我上网兜了一遍，终于找到一种更快捷的方法。 参考链接：使用QtCreator加速Android NDK开发调试 前期准备工作 一部 Android 手机(最好是 root 过的，我用华为P7测试的时候发现没 root 会失败，用没有 roo t的Samsung GT9105则成功了)。虚拟机没有尝试，毕竟 jni 开发还是真机靠谱一点。另外，adb、ndk 等程序肯定也是需要的。 具体步骤 大致流程是：先用 ndk-build 编译 c/c++ 代码，再用 adb 将生成的so库上传到手机里面，然后直接运行 app 即可(如果改了 java 代码当然还是得用 AS 编译一下再运行的，但是这个时间比之前的 clean ＋ build 所花时间少一个数量级)。下面以我的实践为例看看怎么玩： ndk-build 编译代码 这里我直接使用了 AS 的 External Tools 工具，具体可以看这篇博客 Android NDK and OpenCV development with Android Studio 。配置好 External Tools 后，只需要对着你的 jni 文件夹，右键选择 External Tools 中的 ndk-build 命令即可，之后正常的话会在 jniLibs 目录(这里取决于你的 ndk-build 参数怎么设置)下生成 .so 链接库。 adb push到手机 现在 .so 链接库已经有了，接下来就是将库推到手机上，需要用到 adb 提供的 push 命令。这里先讲一下 Android 文件系统的权限问题。一般来说，Android 的文件系统分为两种：一种是 app 私有文件空间，在 /data/data/com.yourcompany.yourapp 目录下，这个空间是该 app 特有的，一般会存放 sharepreference 以及数据库等文件，用户无权访问；另一种是内置存储器的文件空间(在 java 代码中通过Environment.getExternalStorageDirectory()得到的就是这一部分的文件路径，所以也可以称之为外部空间吧)，用手机上的文件管理工具可以直接访问，用户有读写权限。言归正传，我的做法是先将 .so 文件推到 sdcard 目录里，之后再复制到 app 所在的文件夹(之后会给出详细的命令操作)。这样做的原因是，在进入手机 shell 之前，我无法得到 app 内部文件的读写权限，所以先暂时将文件推到手机上再说。 拷贝.so文件到app文件夹 这里的 app 文件夹就是前面提到的 app 私有文件空间。先用 adb shell 进入手机后，再 cd /data/data/com.yourcompany.yourapp 就进入该 app 内部了，但此时我们是没有读写权限的。文章开头参考链接的文章提供了另一种方法：用 Android 提供的 run-as 命令来获取权限，这个命令具体我不清楚，貌似是为 debug 用的，所以应该是对那种 apk 签名是 debug 的起作用。具体用法是：run-as com.yourcompany.yourapp。此时，ls、touch 等命令应该是有效的。但我在华为P7上用 cp 命令的时候则提示 permission denied，而Samsung那部机子则没有问题，所以还是 root 的可靠一点。如果提示没有权限而手机已经 root 过，可以使用 su 命令获得权限，再用 cp 命令将文件 copy 到当前目录。说到这又有必要扯一下 .so 链接库的存放位置，如果你成功进入 app 内部空间用 ls 命令可以看到这些文件夹： 123456app_dataapp_webviewcachefileslibshared_prefs 别的不说，.so 链接库一般是放在 lib 文件夹下的，我们用来加载链接库的代码： 123static &#123; System.loadLibrary(\"name\");&#125; 默认会去 lib 文件夹下寻找指定的链接库。遗憾的是，这个文件即使 root 过也没有写权限。因此，我们只能退一步将 .so 文件拷贝到 app 总目录下，然后修改加载的代码来“曲线救国”： 123static &#123; System.load(\"/data/data/com.yourcompany.yourapp/libname.so\");&#125; load 函数也是加载文件，但需要用户指定文件位置以及文件名，注意前面的 loadLibrary 函数只需要指定文件名，而且不需要前缀 lib，而后面这个函数需要指定 so 文件的全名。之后还需要给动态链接库运行权限 1chmod 0755 libname.so build、 run 基本步骤到这里就结束了，接下来就是重新生成 apk 并跑起来。讲道理的话，如果你中间没有修改过 java 代码，那你完全可以将 .so 文件推到 app 文件夹内，直接跑就可以了(毕竟是动态链接的)。但如果改了也没关系，先用 AS build 一下(比 clean 再 build 快 n 倍)，但将 ndk-build 生成的 .so 文件按照之前的步骤推到手机上，然后运行 app 就可以了，讲道理的话，运行结果跟直接用 AS 编译运行的结果是一样的。之后如果你只是调试修改了 C/C++ 的代码，你只需要重新生成 .so 库，然后推到手机上就可以跑了，java 层完全不影响。 写个脚本吧 虽然这种方法让调试速度大幅提高，但敲那么多命令终究还是很耗时的，所以有必要用脚本批处理一下。因为 ndk-build 我不太熟悉，而且已经在 AS 里配置好了，一键运行即可，所以脚本只处理 adb 相关的命令。这里又涉及另一个问题：一开始用 adb 的时候，我们是在电脑的 shell 上运行的，而之后的命令又是在手机的 shell 上跑的，只用一个脚本会在 shell 切换的时候卡住。因此我在 adb shell 的时候用了重定向符 ‘&lt;’ 引用了另一个脚本文件。这两个脚本的命令如下： 123456789# file name: run.sh，在电脑上跑的命令# 停止运行appadb shell am force-stop com.yourcompany.yourapp# 这里要根据实际情况修改路径和文件名adb push app/src/main/jniLibs/armeabi-v7a/libname.so /sdcard/ echo \"push .so finish\"# 这一步重定向脚本文件adb shell &lt; cmd.sh 12345678910111213141516# file name: cmd.sh 在手机上跑的命令# 进入调试模式的app内部run-as com.weimanteam.weiman# 获取权限，视情况而定，可能有些手机不用获取也可以su# 拷贝文件到当前目录下cp /sdcard/libname.so .# 添加执行权限chmod 0755 libname.so# 离开su超级权限exit# 离开run-as调试权限exit# 离开手机shell，注意之后有空行，否则shell没读到回车键就会一直停在这里exit 其他小问题 我发现有些手机的命令行被阉割得很严重，甚至连 cp 这样的命令都没有，简单的解决办法是找到能用的命令代替，比如可以用 cat 代替 cp ，这样，5中 cp 的命令就要改成 cat /sdcard/libname.so &gt; libname.so。如果实在替换不了，可以试试 busybox 工具，文末提供了相关链接，因为本人没有尝试就不多说了。 ​ &lt;br&gt; 缺陷 可能需要一部 root 的手机。另外，对于静态链接库应该不适用。 参考 使用QtCreator加速Android NDK开发调试 Android NDK and OpenCV development with Android Studio Why do I get access denied to data folder when using adb? BAT脚本如何自动执行 adb shell 以后的命令 android shell 内，sh:cp not found 解决方法 在android中安装busybox时“cp: not found”的解决办法 为Android安装BusyBox —— 完整的bash shell","raw":null,"content":null,"categories":[{"name":"工具","slug":"工具","permalink":"https://jermmy.github.io/categories/工具/"}],"tags":[{"name":"Shell","slug":"Shell","permalink":"https://jermmy.github.io/tags/Shell/"},{"name":"adb","slug":"adb","permalink":"https://jermmy.github.io/tags/adb/"}]},{"title":"Shell工具－Mac批量修改文件名","slug":"2016-7-5-Shell工具-批量修改文件名","date":"2016-07-05T11:18:55.000Z","updated":"2017-03-24T01:03:56.000Z","comments":true,"path":"2016/07/05/2016-7-5-Shell工具-批量修改文件名/","link":"","permalink":"https://jermmy.github.io/2016/07/05/2016-7-5-Shell工具-批量修改文件名/","excerpt":"Mac 有自带的工具 Automator 可以批量修改文件名，但对程序员这种需要经常周游在各种 *nix 系统的生物来说，会用 shell 显然是更好的。","text":"Mac 有自带的工具 Automator 可以批量修改文件名，但对程序员这种需要经常周游在各种 *nix 系统的生物来说，会用 shell 显然是更好的。 SED 这里面起主要作用的 shell 工具是 SED（Stream Editor），这是一款擅长正则表达式处理的工具。 假设我们有如下命名类似的文件： 屏幕快照 2016-07-05 下午7.40.05 现在需要把这些文件名改为 eye_21.png ~ eye_37.png， 下面直接上一段 shell 代码，看看 sed 怎么用： 1234for file in `ls *.png`do mv \"$file\" `echo \"$file\" | sed 's/\\(.*\\_.*\\_\\)\\(.*\\)/\\eye_\\2/g'`done （ Shell 里面用到命令时一定要用``包起来） 奇怪的是 mac 没有提供 rename 命令，所以只能用 mv 来修改文件名。现在具体看 sed 指令的用法。 1sed 's/\\(.*\\_.*\\_\\)\\(.*\\)/eye_\\2/g' sed 后面跟着一段正则表达式，用 s/{reg}/g 包围，双斜杆里面的 {reg} 就是正则表达式了，就本例子而言，需要把文件名前面的 pic_s4_ 更换为 eye_。具体的做法是将字符串分成两部分（即两个括号的内容）。第一个括号的内容表示 pic_s4_，第二个括号则表示任意字符。但是怎么替换字符串呢？注意到正则表达式后面的内容(使用正则表达式时要注意用 ‘\\’ 转义)，“eye_\\2” 表示替换规则。“eye_” 表示替换后的文件名字符串中以 eye_ 开始，\\2 表示引用第二个括号的内容(注意需要使用 \\ 转义)。替换后的文件名如下： 屏幕快照 2016-07-05 下午8.23.57 sed 还有一个 ‘-r’ 的参数可以让正则表达式更加简洁，但我在 mac 发现这个参数没法使用。 sed 更多具体方法，之后有用到时再补充。","raw":null,"content":null,"categories":[{"name":"工具","slug":"工具","permalink":"https://jermmy.github.io/categories/工具/"}],"tags":[{"name":"Shell","slug":"Shell","permalink":"https://jermmy.github.io/tags/Shell/"},{"name":"Sed","slug":"Sed","permalink":"https://jermmy.github.io/tags/Sed/"}]},{"title":"Android-GridView设置选中状态","slug":"2016-6-29-Android-GridView选中效果","date":"2016-06-29T14:47:28.000Z","updated":"2017-03-21T01:38:47.000Z","comments":true,"path":"2016/06/29/2016-6-29-Android-GridView选中效果/","link":"","permalink":"https://jermmy.github.io/2016/06/29/2016-6-29-Android-GridView选中效果/","excerpt":"Mark一个今天被坑了很久的小问题\n需求\n实现脸萌的创作界面（如下）","text":"Mark一个今天被坑了很久的小问题 需求 实现脸萌的创作界面（如下） Screenshot_2016-06-28-09-37-07 而工作重点是实现底下的格子页面，有选中效果，格子间有间隔线。 方法 由于顶部的滑动条和底部的格子界面需要同步滑动，决定采用第三库 ViewPagerIndicator，底部自然是用 ViewPager 嵌套 Fragment，Fragment 的主要内容是格子页面。格子页面用 GridView 实现(不想用最新的 RecyclerView，因为对 Item 点击事件的支持不好)。 难点 仔细观察 GridView 的 Item ，发现其由一张图片、间隔线以及选中时的提示框组成。因此我用如下的布局文件实现(注意 GridView 的 Item 中最好不要放 Button 之类的，否则 GridView 的 Item 点击事件拿不到)： 12345678910111213&lt;?xml version=\"1.0\" encoding=\"utf-8\"?&gt;&lt;LinearLayout xmlns:android=\"http://schemas.android.com/apk/res/android\" android:layout_width=\"match_parent\" android:layout_height=\"match_parent\" android:orientation=\"vertical\"&gt; &lt;ImageView android:id=\"@+id/image\" android:layout_centerInParent=\"true\" android:layout_width=\"match_parent\" android:layout_height=\"match_parent\" /&gt;&lt;/LinearLayout&gt; ImageView主要放图片( ImageResource )和边框间隔线( BackgroundResource )，选中效果我一开始是用 GridView 的 listSelector 属性来做，然而此处遇到第一个坑，见下图： [屏幕快照 2016-06-29 下午6.56.28](/images/2016-6-29/屏幕快照 2016-06-29 下午6.56.28.png) [屏幕快照 2016-06-29 下午6.56.52](/images/2016-6-29/屏幕快照 2016-06-29 下午6.56.52.png) 在快速滚动 GridView 的时候，选中框出现滚动 bug，找了好多资料，貌似没人遇到这种问题，只能作罢(另外，我在写 selector 文件的时候发现，当我把 state_selected 设为 false 时，GridView 的表现反而是选中状态，不明所以)。我又上网查了其他方案，结果都是些怎么改变 listSelector 样式啊之类的。还有一些是建议在 Item 最外层的 LinearLayout 上加 selector，然后在 GridView 的 ItemClickListener 中设置监听手动更换背景。我想，如果 listSelector 的思路不行的话，这个方法应该是唯一的解决方案了。于是，我将 LinearLayout 的 background 设为 selector，然后在 OnItemClickListener 中手动设置，大致代码如下： 123456gridView.setOnItemClickListener(new AdapterView.OnItemClickListener() &#123; @Override public void onItemClick(AdapterView&lt;?&gt; parent, View view, int position, long id) &#123; view.setSelected(position); &#125;&#125;); 然而这样做，虽然点击的时候会出现选中框，但滑动 GridView，再滑回去时，选中框消失了。后来又试了在 Adapter 的 getView 方法中设置，都没什么效果。之后，我突然想起以前用 ListView 的时候，曾经踩过 CheckBox 的选中状态因为滚动而重置的情况，于是我猜测选中框的消失可能和适配器中 View 的重用有关。所以，干脆另辟蹊径，用一个 boolean 数组记录每个 Item 的选中状态，然后每次调用 getView 函数时，根据数组刷新 Item，也就是放弃 GridView 中的 setSelection 方法，手动更换背景。于是得到最终的代码： 123456gridView.setOnItemClickListener(new AdapterView.OnItemClickListener() &#123; @Override public void onItemClick(AdapterView&lt;?&gt; parent, View view, int position, long id) &#123; adapter.setSelected(position); &#125;&#125;); 以下是 Adapter 的部分代码: 12345678910111213141516171819202122232425262728293031323334353637383940414243class GridAdapter extends BaseAdapter &#123; Context mContext; ArrayList&lt;Integer&gt; icons; LayoutInflater inflater; boolean isSelected[]; GridAdapter(Context context, ArrayList&lt;Integer&gt; icons) &#123; this.mContext = context; this.icons = icons; this.inflater = LayoutInflater.from(mContext); this.isSelected = new boolean[icons.size()]; for (int i = 0; i &lt; icons.size(); i++) &#123; isSelected[i] = false; &#125; this.isSelected[0] = true; &#125; @Override public View getView(int position, View convertView, ViewGroup parent) &#123; if (convertView == null) &#123; convertView = inflater.inflate(R.layout.item_gridview_edit, null, false); ImageView image = (ImageView) convertView.findViewById(R.id.image); image.setImageResource(icons.get(position)); image.setBackgroundResource(R.drawable.shape_item_gridview_border); &#125; if (isSelected[position]) &#123; convertView.setBackgroundResource(R.drawable.shape_item_gridview_edit); &#125; else &#123; convertView.setBackgroundResource(R.color.color_homepage); &#125; return convertView; &#125; public void setSelected(int position) &#123; for (int i = 0; i &lt; isSelected.length; i++) &#123; isSelected[i] = false; &#125; this.isSelected[position] = true; notifyDataSetChanged(); &#125;&#125; 在 OnItemClickListener 中修改 Adapter 的数据并刷新 View，在 getView 中根据 boolean 数组手动设置背景，结果证明有效。","raw":null,"content":null,"categories":[{"name":"Android","slug":"Android","permalink":"https://jermmy.github.io/categories/Android/"}],"tags":[{"name":"Android","slug":"Android","permalink":"https://jermmy.github.io/tags/Android/"}]},{"title":"Android Touch事件分发过程","slug":"2016-3-3-Android-Touch事件分发","date":"2016-03-03T15:01:24.000Z","updated":"2017-09-03T09:18:07.000Z","comments":true,"path":"2016/03/03/2016-3-3-Android-Touch事件分发/","link":"","permalink":"https://jermmy.github.io/2016/03/03/2016-3-3-Android-Touch事件分发/","excerpt":"最近在系统学习 Android 一些底层的实现。今天花了一天时间，查阅各种文章源码，决定对 Touch 事件的分发过程做一次梳理。","text":"最近在系统学习 Android 一些底层的实现。今天花了一天时间，查阅各种文章源码，决定对 Touch 事件的分发过程做一次梳理。 网上有大把的文章介绍这类主题，但能够从源头说起的不多，博主搜到了一篇 Android 事件分发机制详解 ，感觉讲得比较全面，而且分析过程也比较适合对底层理解不多的新手（比如我）。 说起 Touch 事件的分发，当然还是得从手指 touch 到屏幕说起啦（大部分文章直入主题说是 Activity 的 dispatchTouchEvent() 方法开启分发过程，可博主比较喜欢刨根问底）。很自然地我们会想到是屏幕把 touch 这个信号传到了，这么底层的东西自然而然也要由 Linux 来提供支持。如果你有点进去上面那篇文章的话，你会知道这个信号经由 Linux 处理封装后，会通过内核中的 InputManager 模块将事件传给 Android 的 WindowManagerService ，后者可以认为是 Android 系统提供的服务了，博主是把它当作 Android 系统的常驻服务进程对待的。WindowManagerService得到这个事件后，中间又经历多方曲折，最终会把它传递到 PhotoWindow 的内部类 DecorView 的 dispatchTouchEvent() 函数这里（其实这里博主不是很确定是不是这样，因为上面那篇文章分析的代码比较旧，和博主看的 5.0 的代码有很大的不同，WindowManagerService 的代码博主实在没有能力看懂，但我想虽然细节变了很多，但总体的框架还是要保持兼容的，所以这种猜测应该是正确的）。 PhotoWindow和DecorView 好吧，既然锁定了位置，就该看看这个 PhotoWindow 和 DecorView是何方神圣。为了搞清楚这个问题，我们不得不从后往前推。 突破点是 Activity ! 相信如果你也研究过该主题并阅读过一些文章的话，你一定对 Activity 的dispatchTouchEvent() 方法不陌生。 1234567891011121314151617181920/** * Called to process touch screen events. You can override this to * intercept all touch screen events before they are dispatched to the * window. Be sure to call this implementation for touch screen events * that should be handled normally. * * @param ev The touch screen event. * * @return boolean Return true if this event was consumed. */public boolean dispatchTouchEvent(MotionEvent ev) &#123; if (ev.getAction() == MotionEvent.ACTION_DOWN) &#123; onUserInteraction(); &#125; // 这里getWindow()得到的是跟这个Activity绑定的PhoneWindow if (getWindow().superDispatchTouchEvent(ev)) &#123; return true; &#125; return onTouchEvent(ev);&#125; 这里就是函数源码，我们逐句看下。首先第一个判断语句，当我们发出 down 这个动作时，会执行 onUserInteraction()，这个方法是空的，是让开发者自己去覆写实现一些功能。那么重点就放在中间那一块了，getWindow() 得到的是什么？博主一路跟踪进去，得到下面这些重要信息： 12345678910111213141516171819202122private Window mWindow;public Window getWindow() &#123; return mWindow;&#125;final void attach(Context context, ActivityThread aThread, Instrumentation instr, IBinder token, int ident, Application application, Intent intent, ActivityInfo info, CharSequence title, Activity parent, String id, NonConfigurationInstances lastNonConfigurationInstances, Configuration config, String referrer, IVoiceInteractor voiceInteractor) &#123; attachBaseContext(context); mFragments.attachHost(null /*parent*/); mWindow = new PhoneWindow(this); mWindow.setCallback(this); mWindow.setOnWindowDismissedCallback(this); mWindow.getLayoutInflater().setPrivateFactory(this); ....&#125; 原来 mWindow是 Window 类型的引用， 重点是在 attach 函数里，我们发现 mWindow 被实例化为 PhotoWindow 。前面说 Touch 事件的分发也跟这个 window 有关，那么这个 PhotoWindow 到底是什么呢？博主搜了一圈，发现这篇文章 Android ViewTree and DecorView 对它的阐述还是很不错的。我们常说，Activity 是应用程序的窗口，严格来讲，PhotoWindow 才是。在 Window 这个类文件里，你可以找这样一段说明： 1234567891011121314/** * Abstract base class for a top-level window look and behavior policy. An * instance of this class should be used as the top-level view added to the * window manager. It provides standard UI policies such as a background, title * area, default key processing, etc. * * &lt;p&gt;The only existing implementation of this abstract class is * android.policy.PhoneWindow, which you should instantiate when needing a * Window. Eventually that class will be refactored and a factory method * added for creating Window instances without knowing about a particular * implementation. */public abstract class Window &#123; ....... 第二段里说道，抽象类 Window 的唯一实现类是 PhotoWindow。我们可以认为，每个 Activity 其实都绑定着一个 PhotoWindow，并由它来控制整个视图的显示。 回到 dispatchTouchEvent 函数，我们已经找到了 getWindow 的返回值，那就去 PhotoWindow 的 superDispatchTouchEvent 方法里瞧瞧： 1234@Overridepublic boolean superDispatchTouchEvent(MotionEvent event) &#123; return mDecor.superDispatchTouchEvent(event);&#125; 我们貌似看到一点 DecorView 的影子，继续跟踪这个 mDecor： 1234567891011121314151617181920212223242526 // This is the top-level view of the window, containing the window decor. private DecorView mDecor; @Override public final View getDecorView() &#123; if (mDecor == null) &#123; installDecor(); &#125; return mDecor; &#125; private void installDecor() &#123; if (mDecor == null) &#123; mDecor = generateDecor(); mDecor.setDescendantFocusability(ViewGroup.FOCUS_AFTER_DESCENDANTS); mDecor.setIsRootNamespace(true); if (!mInvalidatePanelMenuPosted &amp;&amp; mInvalidatePanelMenuFeatures != 0) &#123; mDecor.postOnAnimation(mInvalidatePanelMenuRunnable); &#125; &#125; ......&#125; protected DecorView generateDecor() &#123; return new DecorView(getContext(), -1); &#125; 没得说，mDecor 就是 DecorView。它是 PhotoWindow 的内部类，前面说 PhotoWindow 才代表手机窗口，但窗口内那些的具体元素，其实是 DecorView 负责管理的，简单点说，每次我们创建完 Activity后，不管我们有没有定义 layout 布局，Activity 所在的窗口的最外层都会被套上一层布局（你可以新建一个 Activity，但不要调用 setContentView 方法，看跑起来是怎样的），这层布局就是这里的 DecorView ，也就是说它才是整个 ViewTree 的根节点，而我们的布局只是内容布局，所以叫 ContentView。 好了，现在我们知道 PhotoView 和 DecorView都代表什么之后，暂时把 Activity 放下，回到文章开头。我们前面说 WindowManagerService会将 touch 事件传递到 DecorView的 dispatchTouchEvent 函数，这个函数代码不多： 123456@Overridepublic boolean dispatchTouchEvent(MotionEvent ev) &#123; final Callback cb = getCallback(); return cb != null &amp;&amp; !isDestroyed() &amp;&amp; mFeatureId &lt; 0 ? cb.dispatchTouchEvent(ev) : super.dispatchTouchEvent(ev);&#125; 这里又出现一个 Callback的类，这个 getCallback 其实是 Window 类中定义的方法，而 Callback 也是其内部声明的接口： 12345678910111213141516171819202122/** * API from a Window back to its caller. This allows the client to * intercept key dispatching, panels and menus, etc. */ public interface Callback &#123; ..... /** * Called to process touch screen events. At the very least your * implementation must call * &#123;@link android.view.Window#superDispatchTouchEvent&#125; to do the * standard touch screen processing. * * @param event The touch screen event. * * @return boolean Return true if this event was consumed. */ public boolean dispatchTouchEvent(MotionEvent event); .... &#125; 这个接口内部声明了包括 dispatchTouchEvent() 在内的众多函数接口，我们比较关心这个 Callback 的赋值， 123456789/** * Set the Callback interface for this window, used to intercept key * events and other dynamic operations in the window. * * @param callback The desired Callback interface. */public void setCallback(Callback callback) &#123; mCallback = callback;&#125; 其实，这个方法在我们前面分析的时候已经出现过了，还记得 Activity 绑定 PhotoWindow 的 attach 方法吗？ 1234567891011121314final void attach(Context context, ActivityThread aThread, Instrumentation instr, IBinder token, int ident, Application application, Intent intent, ActivityInfo info, CharSequence title, Activity parent, String id, NonConfigurationInstances lastNonConfigurationInstances, Configuration config, String referrer, IVoiceInteractor voiceInteractor) &#123; .... mWindow = new PhoneWindow(this); mWindow.setCallback(this); mWindow.setOnWindowDismissedCallback(this); mWindow.getLayoutInflater().setPrivateFactory(this); ....&#125; Callback 就是在这里实现了赋值，而传进去的参数就是当前窗口的 Activity 类（ Activity 实现了 Callback 接口）。重新回到 DecorView 的 dispatchTouchEvent 函数： 123456@Overridepublic boolean dispatchTouchEvent(MotionEvent ev) &#123; final Callback cb = getCallback(); return cb != null &amp;&amp; !isDestroyed() &amp;&amp; mFeatureId &lt; 0 ? cb.dispatchTouchEvent(ev) : super.dispatchTouchEvent(ev);&#125; 这里拿到 Callback（也就是 Activity ）之后，return 语句中判断这个 Activity 是否为空，有没有被 destroy，然后根据 mFeatureId 的值来决定调用哪个 dispatchTouchEvent 函数。mFeatureId 在 DecorView 的构造函数中被赋值为 －1，所以一般都会调用 cb.dispatchTouchEvent(ev)，也就是 Activity 的 dispatchTouchEvent 函数。终于，我们又绕了回去。 总结一下其实很简单，WindowManagerService将事件分发给 DecorView 的 dispatchTouchEvent 函数，接着再分发给 Activity 的 dispatchTouchEvent 函数。从这里开始，我们就可以跟大多数文章那样分析 Activity 的 dispatchTouchEvent 函数了。 ViewGroup.dispatchTouchEvent 前面我们在肢解 Activity 的 dispatchTouchEvent 函数分析 PhotoWindow 的同时，已经逐渐把事件分发的流程也理了一下 1234567891011// Activity类 public boolean dispatchTouchEvent(MotionEvent ev) &#123; if (ev.getAction() == MotionEvent.ACTION_DOWN) &#123; onUserInteraction(); &#125; // 这里getWindow()得到的是跟这个Activity绑定的PhoneWindow if (getWindow().superDispatchTouchEvent(ev)) &#123; return true; &#125; return onTouchEvent(ev); &#125; 12345// PhotoWindow类@Overridepublic boolean superDispatchTouchEvent(MotionEvent event) &#123; return mDecor.superDispatchTouchEvent(event);&#125; 仔细跟踪梳理的话，我们会发现，第二个 if 语句最后会执行 DecorView 的 superDispatchTouchEvent 函数 123public boolean superDispatchTouchEvent(MotionEvent event) &#123; return super.dispatchTouchEvent(event);&#125; 这个函数简单粗暴地将任务扔给父类 FrameLayout，而后者最终会调用 ViewGroup 的 dispatchTouchEvent 函数。关于这个函数的讲解，网上已经有很多文章了，所以这里也不细讲（好吧，我承认我也不是很懂）。下面还是保留大段源码，希望有朝一日看懂后继续写注释： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219@Overridepublic boolean dispatchTouchEvent(MotionEvent ev) &#123; ....... boolean handled = false; if (onFilterTouchEventForSecurity(ev)) &#123; final int action = ev.getAction(); final int actionMasked = action &amp; MotionEvent.ACTION_MASK; // 如果这是一个down的动作，就把之前触摸的记录全部清空 // 所以说down是触摸事件的开始 // Handle an initial down. if (actionMasked == MotionEvent.ACTION_DOWN) &#123; // Throw away all previous state when starting a new touch gesture. // The framework may have dropped the up or cancel event for the previous gesture // due to an app switch, ANR, or some other state change. cancelAndClearTouchTargets(ev); resetTouchState(); &#125; // 检查这个事件是否会被ViewGroup拦截，onInterceptTouchEvent方法默认返回false // 也就是不拦截，我们可以覆写这个方法来拦截所有事件。 // 这里还是涉及到TouchTarget这个概念，在ViewGroup中可以找到这个类的定义， // 博主就是一直搞不懂这个东西，所以很多地方看不明白 // Check for interception. final boolean intercepted; if (actionMasked == MotionEvent.ACTION_DOWN || mFirstTouchTarget != null) &#123; // 在判断是否拦截之前，还要注意是否允许拦截， // 可以通过requestDisallowInterceptTouchEvent来设置mGroupFlags， // 比如ViewPager会调用父节点这个方法来防止事件被拦截 final boolean disallowIntercept = (mGroupFlags &amp; FLAG_DISALLOW_INTERCEPT) != 0; if (!disallowIntercept) &#123; intercepted = onInterceptTouchEvent(ev); ev.setAction(action); // restore action in case it was changed &#125; else &#123; intercepted = false; &#125; &#125; else &#123; // 如果这不是down的动作，且TouchTarget链表中没有任何触摸记录， // 就默认这个事件被ViewGroup拦截了 // 其实我搞不懂这种情况该如何理解 // There are no touch targets and this action is not an initial down // so this view group continues to intercept touches. intercepted = true; &#125; // 检查动作是否被取消，但我不知道MotionEvent.ACTION_CANCEL // 在实际中代表什么 // Check for cancelation. final boolean canceled = resetCancelNextUpFlag(this) || actionMasked == MotionEvent.ACTION_CANCEL; // Update list of touch targets for pointer down, if needed. final boolean split = (mGroupFlags &amp; FLAG_SPLIT_MOTION_EVENTS) != 0; TouchTarget newTouchTarget = null; boolean alreadyDispatchedToNewTouchTarget = false; // 如果动作没被取消或拦截，就执行下面一大串代码 // 在这里卡了一天，主要还是TouchTarget不理解 // 我简单的理解是把当前触摸点所在的子View找出来 if (!canceled &amp;&amp; !intercepted) &#123; if (actionMasked == MotionEvent.ACTION_DOWN || (split &amp;&amp; actionMasked == MotionEvent.ACTION_POINTER_DOWN) || actionMasked == MotionEvent.ACTION_HOVER_MOVE) &#123; final int actionIndex = ev.getActionIndex(); // always 0 for down final int idBitsToAssign = split ? 1 &lt;&lt; ev.getPointerId(actionIndex) : TouchTarget.ALL_POINTER_IDS; // Clean up earlier touch targets for this pointer id in case they // have become out of sync. removePointersFromTouchTargets(idBitsToAssign); final int childrenCount = mChildrenCount; if (newTouchTarget == null &amp;&amp; childrenCount != 0) &#123; final float x = ev.getX(actionIndex); final float y = ev.getY(actionIndex); // Find a child that can receive the event. // Scan children from front to back. // 这里的buildOrderedChildList方法是用了插入排序把子View放入队列， // Z轴小的在队列前面，大的在外面。Z轴大的表示离屏幕远，更加靠外， // 因此Z轴大的View会覆盖Z轴小的View final ArrayList&lt;View&gt; preorderedList = buildOrderedChildList(); final boolean customOrder = preorderedList == null &amp;&amp; isChildrenDrawingOrderEnabled(); final View[] children = mChildren; // 这里从preorderedList的后面找起，其实是从Z轴大的View逐步 // 遍历到Z轴小的View for (int i = childrenCount - 1; i &gt;= 0; i--) &#123; final int childIndex = customOrder ? getChildDrawingOrder(childrenCount, i) : i; final View child = (preorderedList == null) ? children[childIndex] : preorderedList.get(childIndex); // 如果这个子View不能接收事件 或者 这个事件的坐标不在子View的范围内 // 继续循环查找 if (!canViewReceivePointerEvents(child) || !isTransformedTouchPointInView(x, y, child, null)) &#123; continue; &#125; // 找到目标子View，退出循环 newTouchTarget = getTouchTarget(child); if (newTouchTarget != null) &#123; // Child is already receiving touch within its bounds. // Give it the new pointer in addition to the ones it is handling. newTouchTarget.pointerIdBits |= idBitsToAssign; break; &#125; // 会进入下面的代码段的前提是，这个事件的坐标在子view的范围内， // 但TouchTarget链表中没有这个View的记录，这怎么理解？果然还是TouchTarget // 的问题！！ resetCancelNextUpFlag(child); if (dispatchTransformedTouchEvent(ev, false, child, idBitsToAssign)) &#123; // Child wants to receive touch within its bounds. mLastTouchDownTime = ev.getDownTime(); if (preorderedList != null) &#123; // childIndex points into presorted list, find original index for (int j = 0; j &lt; childrenCount; j++) &#123; if (children[childIndex] == mChildren[j]) &#123; mLastTouchDownIndex = j; break; &#125; &#125; &#125; else &#123; mLastTouchDownIndex = childIndex; &#125; mLastTouchDownX = ev.getX(); mLastTouchDownY = ev.getY(); newTouchTarget = addTouchTarget(child, idBitsToAssign); alreadyDispatchedToNewTouchTarget = true; break; &#125; &#125; // 提醒一下，ArrayList的插入函数是深拷贝，结束循环要clear一下， // 以前一直以为是简单地拷贝引用！ if (preorderedList != null) preorderedList.clear(); &#125; if (newTouchTarget == null &amp;&amp; mFirstTouchTarget != null) &#123; // Did not find a child to receive the event. // Assign the pointer to the least recently added target. newTouchTarget = mFirstTouchTarget; while (newTouchTarget.next != null) &#123; newTouchTarget = newTouchTarget.next; &#125; newTouchTarget.pointerIdBits |= idBitsToAssign; &#125; &#125; &#125; // mFirstTouchTarget是TouchTarget链表的头， // 如果头都是空的，证明这条列表就不存在，那是否意味着当前 // 整个触摸事件的发生过程中，没有触及到任何子view？ // 如果真是这样的话，那事件只能仍交给ViewGroup处理了。 // 事实上，看dispatchTransformedTouchEvent的函数定义我们发现： // ViewGroup会继续传递给它的父类的dispatchTouchEvent方法。 // 好吧，继续不懂。 // Dispatch to touch targets. if (mFirstTouchTarget == null) &#123; // No touch targets so treat this as an ordinary view. handled = dispatchTransformedTouchEvent(ev, canceled, null, TouchTarget.ALL_POINTER_IDS); &#125; else &#123; // 下面这里才是真正将事件交给子View的过程 // // Dispatch to touch targets, excluding the new touch target if we already // dispatched to it. Cancel touch targets if necessary. TouchTarget predecessor = null; TouchTarget target = mFirstTouchTarget; while (target != null) &#123; final TouchTarget next = target.next; // 如果这个触摸点是我们前面已经找出来的newTouchTarget // 就默认已经处理过了，因为前面有一个if语句中确实向这个触摸点的子View分发了事件 if (alreadyDispatchedToNewTouchTarget &amp;&amp; target == newTouchTarget) &#123; handled = true; &#125; else &#123; final boolean cancelChild = resetCancelNextUpFlag(target.child) || intercepted; // 这里给TouchTarget链中的各个子View分发事件 if (dispatchTransformedTouchEvent(ev, cancelChild, target.child, target.pointerIdBits)) &#123; handled = true; &#125; if (cancelChild) &#123; if (predecessor == null) &#123; mFirstTouchTarget = next; &#125; else &#123; predecessor.next = next; &#125; target.recycle(); target = next; continue; &#125; &#125; predecessor = target; target = next; &#125; &#125; // Update list of touch targets for pointer up or cancel, if needed. if (canceled || actionMasked == MotionEvent.ACTION_UP || actionMasked == MotionEvent.ACTION_HOVER_MOVE) &#123; resetTouchState(); &#125; else if (split &amp;&amp; actionMasked == MotionEvent.ACTION_POINTER_UP) &#123; final int actionIndex = ev.getActionIndex(); final int idBitsToRemove = 1 &lt;&lt; ev.getPointerId(actionIndex); removePointersFromTouchTargets(idBitsToRemove); &#125; &#125; ..... return handled;&#125; 虽然很多代码细节没搞懂，但事件分发都是交给这个函数处理的，这里就截取代码片段了： 12345678910111213141516171819202122232425262728293031/** * Transforms a motion event into the coordinate space of a particular child view, * filters out irrelevant pointer ids, and overrides its action if necessary. * If child is null, assumes the MotionEvent will be sent to this ViewGroup instead. */private boolean dispatchTransformedTouchEvent(MotionEvent event, boolean cancel, View child, int desiredPointerIdBits) &#123; final boolean handled; ..... // 如果函数传进来的子View是空的，直接调用父类的分发函数(递归) // 否则，会调用子View的dispatchTouchEvent函数 // Perform any necessary transformations and dispatch. if (child == null) &#123; handled = super.dispatchTouchEvent(transformedEvent); &#125; else &#123; final float offsetX = mScrollX - child.mLeft; final float offsetY = mScrollY - child.mTop; transformedEvent.offsetLocation(offsetX, offsetY); if (! child.hasIdentityMatrix()) &#123; transformedEvent.transform(child.getInverseMatrix()); &#125; handled = child.dispatchTouchEvent(transformedEvent); &#125; // Done. transformedEvent.recycle(); return handled;&#125; 从前面 ViewGroup 的 dispatchTouchEvent 函数我们已经发现：ViewGroup 会将事件分发给 TouchTarget 链上的各个子 View，然后通过 dispatchTransformedTouchEvent 函数来调用子View自己的 dispatchTransformedTouchEvent 函数。如果子 View 是一个 ViewGroup ，那么它会跟我们前面分析的一样，继续走 ViewGroup 的分发流程，如果子 View 是一个普通的 View，比如说是一个 Button，那么会调用 Button 的 dispatchTouchEvent 函数，因为大部分控件都没有覆写 View 的这个方法，所以我们继续将目光转向这个函数： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657/** * Pass the touch screen motion event down to the target view, or this * view if it is the target. * * @param event The motion event to be dispatched. * @return True if the event was handled by the view, false otherwise. */public boolean dispatchTouchEvent(MotionEvent event) &#123; boolean result = false; // 调试用，不理会 if (mInputEventConsistencyVerifier != null) &#123; mInputEventConsistencyVerifier.onTouchEvent(event, 0); &#125; // 判断这个动作是否是down，是的话调用stopNestedScroll方法 // 这个方法我按照官方文档理解是停止滚动，比如说当滚动ScrollView时，如果按下手指 // 则应该停止滚动。Androiod里的每一个View都自带滚动特性。 final int actionMasked = event.getActionMasked(); if (actionMasked == MotionEvent.ACTION_DOWN) &#123; // Defensive cleanup for new gesture stopNestedScroll(); &#125; if (onFilterTouchEventForSecurity(event)) &#123; //noinspection SimplifiableIfStatement // 如果我们设置了OnTouchListener，且这个View是Enabled的，就执行OnTouchListener.onTouch // 所以如果我们在onTouch函数中返回true，那么这个事件就在这里被直接消费了 // 这样后一个if语句就不会执行，这个函数也基本结束，返回true ListenerInfo li = mListenerInfo; if (li != null &amp;&amp; li.mOnTouchListener != null &amp;&amp; (mViewFlags &amp; ENABLED_MASK) == ENABLED &amp;&amp; li.mOnTouchListener.onTouch(this, event)) &#123; result = true; &#125; // 如果前面onTouch没有消费掉事件(返回false)，这里还会进入onTouchEvent函数 if (!result &amp;&amp; onTouchEvent(event)) &#123; result = true; &#125; &#125; if (!result &amp;&amp; mInputEventConsistencyVerifier != null) &#123; mInputEventConsistencyVerifier.onUnhandledEvent(event, 0); &#125; // Clean up after nested scrolls if this is the end of a gesture; // also cancel it if we tried an ACTION_DOWN but we didn't want the rest // of the gesture. if (actionMasked == MotionEvent.ACTION_UP || actionMasked == MotionEvent.ACTION_CANCEL || (actionMasked == MotionEvent.ACTION_DOWN &amp;&amp; !result)) &#123; stopNestedScroll(); &#125; return result;&#125; 相比前面 ViewGroup ，这个方法好看多了。这里面主要的焦点都落在中间两句 if 语句那里。我们平时传入的 OnTouchListener 就是在这里执行的，而 onTouchEvent 函数会继续调用 OnClickListener 回调（当然 OnTouchListener 不能消费掉事件，否则系统认为你这个 View 后续不需要再处理事件）。可以说，View 收到触摸事件后的逻辑操作都集中在 onTouchEvent 函数里面。具体我也不深入分析了，主要是在动作是 ACTION_UP 的时候执行 performClick 方法，这个方法里面回调我们的 OnClickListener 接口。 最后我们回顾一下整个流程，Activity 的 dispatchTouchEvent 方法会调用整个窗口根节点 DecorView 的 superDispatchTouchEvent，而后者其实直接调用了 ViewGroup 的事件分发函数。在 ViewGroup 分发的过程中，会判断是否要进行拦截，这个过程系统留了接口让开发者自己去决定（也就是 onInterceptTouchEvent 函数）。如果没有拦截，会将事件逐个分发给 TouchTarget 链上的子 View，不管这个子 View 是 ViewGroup 还是一般的 View，只要其中一个消费了事件（返回 true ），最顶层的 ViewGroup 就返回 true，回到 Activity 里，就是在第二条判断语句返回 true，那后面的 Activity 自身的 onTouchEvent 函数也就不会执行了。 另外，在 View 的分发过程中，如果 onTouch 消费了事件，onClick 也不会再执行。","raw":null,"content":null,"categories":[{"name":"Android","slug":"Android","permalink":"https://jermmy.github.io/categories/Android/"}],"tags":[{"name":"Android","slug":"Android","permalink":"https://jermmy.github.io/tags/Android/"}]},{"title":"Android NDK编译","slug":"2016-2-27-Android-NDK-编译","date":"2016-02-27T02:10:57.000Z","updated":"2017-04-23T05:42:52.000Z","comments":true,"path":"2016/02/27/2016-2-27-Android-NDK-编译/","link":"","permalink":"https://jermmy.github.io/2016/02/27/2016-2-27-Android-NDK-编译/","excerpt":"（在使用 NDK 之前，应该先确定一定以及肯定 C/C++ 能更好地提升程序性能，如果 Java 也能做得很好的事，讲道理的话是不应该用。当然，隐藏代码细节的除外。）","text":"（在使用 NDK 之前，应该先确定一定以及肯定 C/C++ 能更好地提升程序性能，如果 Java 也能做得很好的事，讲道理的话是不应该用。当然，隐藏代码细节的除外。） 关于 NDK 编译的文章已经烂大街了。这里只是简单总结一下在 AS 中怎么做，方便博主日后查看。有必要提供这篇文章作为参考Android NDK and OpenCV development with Android Studio，作者不仅认真负责，与时俱进，还富有情调，为广大程序员所不及也。这篇文章主要讲了怎么用 AS 来更快捷地使用 javah, ndk-build等命令，如何在 gradle 里面配置 task，当然也说了一下怎么来编译 opencv。 我觉得这里面比较复杂的是编译 ndk 这个过程，所以就只是简单描述一下这个流程^_^ 首先，要先确定 java 和 C/C++ 的交互接口，说白了就是 java 要调用哪些 C/C++ 函数，假设是以下这个： 1234567public class NdkJniUtils &#123; public native String getCLangString(); static &#123; System.loadLibrary(\"jni_name\"); &#125;&#125; 这里面加载外部依赖库的代码要放在 static 里面，这样会先于 onCreate 等方法执行，并且只加载一次依赖库。依赖库的名称也比较重要，下面会提到。 之后就要开始实现 C/C++ 代码了。由于 NDK 对 C/C++ 的函数名要求比较严格，新手容易出错，这个时候便可以借助 javah 这个工具了，javah可以根据你的 native 函数，自动生成本地头文件。我这里使用 AS 的 External Tools（如何在 External Tools 中使用 javah，请看前面那篇文章），右键 NdkJniUtils.java 使用 javah，这时会在 jni 目录下生成 your_package_NdkJniUtils.h 这个头文件。打开这个头文件，可以在里面看到函数声明： 1234567/* * Class: your_package_NdkJniUtils * Method: getCLangString * Signature: ()Ljava/lang/String; */JNIEXPORT jstring JNICALL Java_your_package_NdkJniUtils_getCLangString (JNIEnv *, jobject); 有了这个函数声明，我们可以新建一个对应的 your_package_NdkJniUtils.cpp 或 c 文件，然后实现这个函数 1234567891011#include \"your_package_NdkJniUtils.h\"/* * Class: your_package_NdkJniUtils * Method: getCLangString * Signature: ()Ljava/lang/String; */JNIEXPORT jstring JNICALL Java_your_package_NdkJniUtils_getCLangString (JNIEnv *env, jobject obj) &#123; return env-&gt;NewStringUTF(\"This is just a test\");&#125; 写完代码，接下来就要准备编译了，编译的方法有两种。 方法一：使用gradle 这种方法不需要编码 Android.mk ，gradle 会自动帮我们生成。我们要做的是修改 gradle 的配置文件，在 defaultConfig 下面添加 ndk 配置： 123456789defaultConfig &#123; ...... ndk &#123; moduleName \"jni_name\" //生成的so名字 abiFilters \"armeabi\", \"armeabi-v7a\", \"x86\" //输出指定三种abi体系结构下的so库。 &#125;&#125; ndk 里面有一个 moduleName，它就是我们前面在 Java 代码中添加的依赖库的名称。 为了让 gradle 知道 ndk 放在哪，需要在 local.properties 文件中添加 ndk 目录： 1ndk.dir=/your-dir-path/android-ndk-r10e 这时再 build 一下工程，gradle 会自动调用 ndk-build 命令，并且自动生成 Android.mk ，进入到你的工程目录，可以在 app/build/intermediates/ndk/debug 下面看到 Android.mk 以及 lib/&lt;abi&gt;/*.so ，run 之后这些 so 依赖库都会打包到 apk 文件中。 方法二：自己使用ndk-build 对于一些习惯 eclipse 的朋友，可能这种方式会更亲切一点。如果是自己在命令行编译代码的话，需要在 jni 目录下编写 Android.mk 文件（ Application.mk 貌似可有可无），然后进入jni这个目录用 ndk-build 进行编译。博主也喜欢这样的方式，但博主直接用 AS 的 External Tools 调用 ndk-build （如何在 External Tools 中使用 ndk-build，请看前面那篇文章）。 首先需要自己配置 Android.mk（关于这个文件如何配置的，之后再学习）： 123456789101112LOCAL_PATH := $(call my-dir)include $(CLEAR_VARS)LOCAL_SRC_FILES := your_package_NdkJniUtils.cppLOCAL_LDLIBS += -llogLOCAL_MODULE := jni_nameLOCAL_C_INCLUDES += /your_project_dir/app/src/main/jniLOCAL_C_INCLUDES += your_project_dir/app/src/debug/jniinclude $(BUILD_SHARED_LIBRARY) 看到里面有一个 LOCAL_MODULE，它就是我们在 java 代码中需要的依赖库名称。 如果想生成各个平台的依赖库，可以在 Application.mk 中这样写： 1APP_ABI := armeabi armeabi-v7a x86 之后，右键刚才创建的 your_package_NdkJniUtils.cpp/c 文件，执行 ndk-build，这样会在 jniLibs 目录下生成那些 .so 文件。接下来用 gradle 编译整个项目，注意要现在 gradle 配置文件中添加一句： 123456789101112android &#123; compileSdkVersion 23 buildToolsVersion \"23.0.1\" defaultConfig &#123; ...... &#125; // 添加 sourceSets.main.jni.srcDirs = []&#125; 这样 Android 的 build 系统才会根据我们自己的 Android.mk 寻找依赖库，然后链接各个模块，最终生成 apk 文件。 依赖其他第三库 当然啦，如果你没有依赖其他第三方的 .so 库，那么这两种方法都可以帮你完成编译，但如果用到第三方依赖库怎么办？对于第一种方法，需要你在 gradle 的配置文件中添加 task，声明 ndk-build 的参数，同时要自己声明 Android.mk。（这也是为什么我喜欢第二种方法的原因，既然都会用到 Android.mk，何必在 gradle 中写那么多配置）。由于配置的过程比较麻烦，这里不细说，具体可以参考最开始给出的那篇文章。 重点说说第二种方法。以编译 opencv 库为例吧。 由于需要引入 opencv 库，所以要修改我们的 Android.mk 文件 12345678910111213141516171819LOCAL_PATH := $(call my-dir)include $(CLEAR_VARS)#opencvOPENCVROOT := /your_opencv_dir/OpenCV-android-sdkOPENCV_CAMERA_MODULES := onOPENCV_INSTALL_MODULES := onOPENCV_LIB_TYPE := SHAREDinclude $&#123;OPENCVROOT&#125;/sdk/native/jni/OpenCV.mkLOCAL_SRC_FILES := your_package_NdkJniUtils.cppLOCAL_LDLIBS += -llogLOCAL_MODULE := jni_nameLOCAL_C_INCLUDES += /your_package_dir/app/src/main/jniLOCAL_C_INCLUDES += /your_package_dir/app/src/debug/jniinclude $(BUILD_SHARED_LIBRARY) 然后我们在原来 cpp 文件中引入 opencv 头文件： 1234567891011121314#include \"your_package_NdkJniUtils.h\"#include &lt;opencv2/opencv.hpp&gt;using namespace cv;/* * Class: your_package_NdkJniUtils * Method: getCLangString * Signature: ()Ljava/lang/String; */JNIEXPORT jstring JNICALL Java_your_package_NdkJniUtils_getCLangString (JNIEnv *env, jobject obj) &#123; return env-&gt;NewStringUTF(\"This is just a test\");&#125; 右键跑一下ndk-build，正常的话是可以编译成功的。但如果要run这个项目，需要在gradle配置文件中添加一句： 123456789101112android &#123; compileSdkVersion 23 buildToolsVersion \"23.0.1\" defaultConfig &#123; ...... &#125; // 添加 sourceSets.main.jni.srcDirs = []&#125; 这条语句的目的是让 gradle 使用我们自己定义的 Android.mk 文件，而不是像之前的方法一一样，自己去寻找依赖然后编译。 好了，整个操作流程就讲这么多，之后有时间再看看 Android.mk 以及 jni 具体该怎么使用。","raw":null,"content":null,"categories":[{"name":"Android","slug":"Android","permalink":"https://jermmy.github.io/categories/Android/"}],"tags":[{"name":"Android","slug":"Android","permalink":"https://jermmy.github.io/tags/Android/"},{"name":"NDK","slug":"NDK","permalink":"https://jermmy.github.io/tags/NDK/"}]},{"title":"Android-自定义控件之FlowLayout(三)","slug":"2016-2-20-Android-自定义控件之FlowLayout(三)","date":"2016-02-20T14:18:29.000Z","updated":"2017-04-23T05:42:40.000Z","comments":true,"path":"2016/02/20/2016-2-20-Android-自定义控件之FlowLayout(三)/","link":"","permalink":"https://jermmy.github.io/2016/02/20/2016-2-20-Android-自定义控件之FlowLayout(三)/","excerpt":"（首先声明，这篇文章是博主在 mooc 上学习了 hyman 的视频打造Android流式布局和热门标签后总结的小知识）","text":"（首先声明，这篇文章是博主在 mooc 上学习了 hyman 的视频打造Android流式布局和热门标签后总结的小知识） onLayout函数的作用 上一篇文章说到 onMeasure 函数会测量子 View 以及 ViewGroup 的宽高，而 onLayout 则是进一步确定子 View 在 ViewGroup 中的位置。 12345/** * Position all children within this layout. */@Overrideprotected void onLayout(boolean changed, int left, int top, int right, int bottom) onLayout 的参数表示这个 View 相对于父控件的位置，对于自定义 ViewGroup 而言，一般不会用到，我们暂时不管。 系统调用完 onMeasure 方法后，已经确定了 ViewGroup 以及内部子 View 的大小，之后会调用 onLayout 来摆放这些子 View。具体操作是通过遍历子 View 并调用 View.layout 方法 12345678/***Assign a size and position to a view and all of its descendantsThis is the second phase of the layout mechanism. (The first is measuring). In this phase, each parent calls layout on all of its children to position them. This is typically done using the child measurements that were stored in the measure pass().Derived classes should not override this method. Derived classes with children should override onLayout. In that method, they should call layout on each of their children.*/public void layout (int l, int t, int r, int b) 函数说明指出，在 onLayout 方法中，我们应该调用每个子 View 的 layout 方法，让子 View 自动布局到所需要的位置。需要注意的是，我们在 onMeasure 中调用 measureChild 方法来测量各个子控件，但其实这个方法内部也是调用了子 View 的 measure 方法来实现的，这是一种常用的分治策略。layout方法的四个参数的意义如下： Parameters l Left position, relative to parent t Top position, relative to parent r Right position, relative to parent b Bottom position, relative to parent 都是子 View 相对父控件的位置。因此，在 onLayout 中，我们只要根据我们的需求计算出子 View 的位置信息，并调用子 View 的 layout 方法即可。 可能有人会问 ViewGroup 的位置又怎么确定？当然是在 ViewGroup 的父控件中通过 onLayout 来调用 ViewGroup 的 layout 方法啦。 所以，onLayout 方法的重点自然是计算子 View 的位置啦，由于不同需求的计算方法是不一样的，这里贴上 FlowLayout 的 onLayout 实现，仅仅是一个模板作用 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273// 按行的方式记录子Viewprivate List&lt;List&lt;View&gt;&gt; mAllViews = new ArrayList&lt;&gt;();// 记录各行的高private List&lt;Integer&gt; mLineHeights = new ArrayList&lt;&gt;();@Overrideprotected void onLayout(boolean changed, int l, int t, int r, int b) &#123; mAllViews.clear(); mLineHeights.clear(); // 当前ViewGroup宽度 int width = getWidth(); int lineWidth = 0; int lineHeight = 0; List&lt;View&gt; lineViews = new ArrayList&lt;&gt;(); int cCount = getChildCount(); for (int i = 0; i &lt; cCount; i++) &#123; View child = getChildAt(i); MarginLayoutParams lp = (MarginLayoutParams) child.getLayoutParams(); int childWidth = child.getMeasuredWidth(); int childHeight = child.getMeasuredHeight(); // 换行 if (childWidth + lineWidth + lp.leftMargin + lp.rightMargin &gt; width - getPaddingLeft() - getPaddingRight()) &#123; mLineHeights.add(lineHeight); mAllViews.add(lineViews); lineViews = new ArrayList&lt;&gt;(); lineWidth = 0; lineHeight = childHeight + lp.topMargin + lp.bottomMargin; &#125; lineWidth += childWidth + lp.leftMargin + lp.rightMargin; lineHeight = Math.max(lineHeight, childHeight + lp.topMargin + lp.bottomMargin); lineViews.add(child); &#125; // 处理最后一行 mLineHeights.add(lineHeight); mAllViews.add(lineViews); // 设置子View的位置 int left = getPaddingLeft(); int top = getPaddingTop(); int lineNum = mAllViews.size(); for (int i = 0; i &lt; lineNum; i++) &#123; lineViews = mAllViews.get(i); lineHeight = mLineHeights.get(i); for (int j = 0; j &lt; lineViews.size(); j++) &#123; View child = lineViews.get(j); if (child.getVisibility() == View.GONE) &#123; continue; &#125; MarginLayoutParams lp = (MarginLayoutParams) child.getLayoutParams(); int lc = left + lp.leftMargin; int tc = top + lp.topMargin; int rc = lc + child.getMeasuredWidth(); int bc = tc + child.getMeasuredHeight(); // 这里是onLayout产生作用的重点 child.layout(lc, tc, rc, bc); left += child.getMeasuredWidth() + lp.rightMargin + lp.leftMargin; &#125; top += lineHeight; left = getPaddingLeft(); &#125;&#125; ###getWidth()和getMeasureWidth() 在测量或布局的时候需要用到子 View 的宽高，但系统提供了两个获取宽高的方法（这里以宽度为例，高度类比）：getWidth() , getMeasureWidth() 。一开始博主傻傻分不清这两个函数到底有什么区别，后来看了郭霖的博客后豁然开朗Android视图绘制流程完全解析，带你一步步深入了解View(二)。getMeasureWidth() 是在 onMeasure() 之后得到的，而 getWidth() 则在 onLayout() 之后获得。简单来说，getMeasureWidth()方法中的值是通过 setMeasuredDimension()方法来进行设置的，而 getWidth() 方法中的值则是通过视图右边的坐标减去左边的坐标计算出来的。所以，如果正常设计的话，这两个函数返回的值应该是一样的。所谓正常设计就是说，View 需要多大的宽高我们就给它布局多大的空间。 比方说在调用 layout 的时候，如果传这样的参数： 1child.layout(0, 0, child.getMeasureWidth(), child.getMeasureHeight()); 也就是说，我们尊重测量的时候，严格按测量的大小布局，这是两个函数等价。但如果这样传参： 1child.layout(0, 0, 200, 200); 那我们之前的测量结果就没有用到，此时 getMeasureWidth() 依然是测量出的宽度，而 getWidth() 就变成了 200。","raw":null,"content":null,"categories":[{"name":"Android","slug":"Android","permalink":"https://jermmy.github.io/categories/Android/"}],"tags":[{"name":"Android","slug":"Android","permalink":"https://jermmy.github.io/tags/Android/"}]},{"title":"Android-自定义控件之FlowLayout(二)","slug":"2016-2-16-Android-自定义控件之FlowLayout(二)","date":"2016-02-16T15:01:53.000Z","updated":"2017-03-22T01:34:24.000Z","comments":true,"path":"2016/02/16/2016-2-16-Android-自定义控件之FlowLayout(二)/","link":"","permalink":"https://jermmy.github.io/2016/02/16/2016-2-16-Android-自定义控件之FlowLayout(二)/","excerpt":"（首先声明，这篇文章是博主在 mooc 上学习了 hyman 的视频打造Android流式布局和热门标签后总结的小知识）","text":"（首先声明，这篇文章是博主在 mooc 上学习了 hyman 的视频打造Android流式布局和热门标签后总结的小知识） onMeasure实现过程 这篇文章总结一下 onMeasure 函数该如何完成测量过程。 再次看看官网对 onMeasure 函数的说明 123456/** * Ask all children to measure themselves and compute the measurement of this * layout based on the children. */@Overrideprotected void onMeasure(int widthMeasureSpec, int heightMeasureSpec) 前面讲过，我们通过该函数两个参数来确定 ViewGroup 的宽高及其测量模式。但同时，这个函数需要让子 View 去测量它们自己的宽高，这样，我们才能在 ViewGroup 中得到子 View 的宽高。让子 View 去测量自己的方法是调用 ViewGroup 提供的 measureChild 方法。调用该方法后，可以通过子 View 的 getMeasuredWidth 或 getMeasuredHeight 方法分别获得子 View 的宽高。之后通过我们自己的策略确定 ViewGroup 的宽高。前面讲过，如果是「EXACTLY」模式，那么宽高的值直接就是 onMeasure 传进来的参数值，如果是「AT_MOST」模式，则需要根据子 View 的宽高自行测量，最后通过 setMeasureDimension 方法将宽高作为参数传给 ViewGroup。 下面的代码是 FlowLayout 的 onMeasure 函数的实现方法： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253@Overrideprotected void onMeasure(int widthMeasureSpec, int heightMeasureSpec) &#123; // 如果布局文件的宽高使用match_parent或fill_parent，mode对应的是EXACTLY模式 // 如果布局文件的宽高使用wrap_content，mode对应的是AT_MOST模式 // EXACTLY模式中ViewGroup的宽高已经确定，AT_MOST模式中需要自己设定 int sizeWidth = MeasureSpec.getSize(widthMeasureSpec); int modeWidth = MeasureSpec.getMode(widthMeasureSpec); int sizeHeight = MeasureSpec.getSize(heightMeasureSpec); int modeHeight = MeasureSpec.getMode(heightMeasureSpec); // 以下计算当模式设定为AT_MOST时宽高的值 // 计算出的ViewGroup的宽和高 int width = 0; int height = 0; // 每一行的宽高 int lineWidth = 0; int lineHeight = 0; int cCount = getChildCount(); for (int i = 0; i &lt; cCount; i++) &#123; View child = getChildAt(i); // 测量子View的宽和高 measureChild(child, widthMeasureSpec, heightMeasureSpec); // 得到子View的LayoutParams，子View的LayoutParams是由父布局的LayoutParams决定的 MarginLayoutParams lp = (MarginLayoutParams) child.getLayoutParams(); int childWidth = child.getMeasuredWidth() + lp.leftMargin + lp.rightMargin; int childHeight = child.getMeasuredHeight() + lp.topMargin + lp.bottomMargin; if (lineWidth + childWidth &gt; sizeWidth - getPaddingLeft() - getPaddingRight()) &#123; width = Math.max(width, lineWidth); lineWidth = childWidth; height += lineHeight; lineHeight = childHeight; &#125; else &#123; lineWidth += childWidth; lineHeight = Math.max(lineHeight, childHeight); &#125; if (i == cCount - 1) &#123; width = Math.max(width, lineWidth); height += lineHeight; &#125; &#125; Log.i(TAG, \"sizeWidth====&gt;\" + sizeWidth + \" sizeHeight===&gt;\" + sizeHeight); Log.i(TAG, \"width====&gt;\" + width + \" height===&gt;\" + height); setMeasuredDimension( modeWidth == MeasureSpec.AT_MOST ? width + getPaddingLeft() + getPaddingRight() : sizeWidth, modeHeight == MeasureSpec.AT_MOST ? height + getPaddingTop() + getPaddingBottom() : sizeHeight );&#125;","raw":null,"content":null,"categories":[{"name":"Android","slug":"Android","permalink":"https://jermmy.github.io/categories/Android/"}],"tags":[{"name":"Android","slug":"Android","permalink":"https://jermmy.github.io/tags/Android/"}]},{"title":"Android-自定义控件之FlowLayout(一)","slug":"2016-2-15-Android-自定义控件之FlowLayout(一)","date":"2016-02-15T13:34:41.000Z","updated":"2017-03-23T01:29:37.000Z","comments":true,"path":"2016/02/15/2016-2-15-Android-自定义控件之FlowLayout(一)/","link":"","permalink":"https://jermmy.github.io/2016/02/15/2016-2-15-Android-自定义控件之FlowLayout(一)/","excerpt":"（首先声明，这篇文章是博主在 mooc 上学习了 hyman 的视频打造Android流式布局和热门标签后总结的小知识）","text":"（首先声明，这篇文章是博主在 mooc 上学习了 hyman 的视频打造Android流式布局和热门标签后总结的小知识） onMeasure()的两个参数 Android 自定义容器控件时，通常都会继承 ViewGroup 。继承后主要的工作便是覆写 onMeasure 和 onLayout 方法，重新制定布局文件测量以及显示子控件位置的策略。下面的 onMesure 函数摘自官网教程http://developer.android.com/intl/zh-cn/reference/android/view/ViewGroup.html 123456/** * Ask all children to measure themselves and compute the measurement of this * layout based on the children. */@Overrideprotected void onMeasure(int widthMeasureSpec, int heightMeasureSpec) 这个函数需要调用 measureChild 等方法来测量各种子 View 的宽高，以此来决定 ViewGroup 的宽高。这篇文章主要关注方法中传入的两个参数。以 widthMeasureSpec 为例，它包含的信息不仅仅包括 ViewGroup 的宽度（这个宽高是 ViewGroup 的父控件提供的建议宽高， ViewGroup 可以直接采纳，也可以根据实际情况调整），同时包括宽度的测量模式。什么叫测量模式呢？比方说，如果你设定的 ViewGroup 的宽度为 200 dp，那么这个模式就是精确模式「EXACTLY」，如果宽度为 wrap_content，则对应「AT_MOST」模式，表示要根据子 View 的内容撑到最大。事实上，Android 提供的模式只有三中：「EXACTLY」、「AT_MOST」、「UNSPECIFIED」。对于指定了具体数值或设置为 match_parent 、 fill_parent 的，测量模式均为「EXACTLY」，而 wrap_content 则是「AT_MOST」模式，「UNSPECIFIED」暂不清楚什么时候被使用。 这些模式有什么作用呢？ onMeasure 方法不是要测量 ViewGroup 的宽高吗，如果测量模式是“确切”的(「EXACTLY」)，那我们其实不用做任何处理，直接从 widthMeasureSpec 和 heightMeasureSpec 这两个参数直接获取宽高即可，但如果是「AT_MOST」模式，就需要我们去测量各个子 View 的宽高，并按照我们的规则去计算 ViewGroup 最终的宽高。 那这些模式该如何得到呢？Android提供了一个类 MeasureSpec，可以从 onMeasure 的两个参数获得宽高或者测量模式。具体做法是： 1234int sizeWidth = MeasureSpec.getSize(widthMeasureSpec); // 获得宽度int modeWidth = MeasureSpec.getMode(widthMeasureSpec); // 获得宽度的测量模式int sizeHeight = MeasureSpec.getSize(heightMeasureSpec);int modeHeight = MeasureSpec.getMode(heightMeasureSpec); 如果 modeWidth 和 modeHeight 的值都是 MeasureSpec.EXACTLY，那么我们可以直接调用 setMeasuredDimension(sizeWidth, sizeHeight) 来设定 ViewGroup 的宽高（ setMeasuredDimension 是 ViewGroup 提供的用于设置其宽高的方法，一般在 onMeasure 方法的最后使用）。如果是「AT_MOST」模式，则需要在代码中重新测量宽高，最后再设进去，具体做法后面的文章再讲。 三个构造函数 继承 ViewGroup 后一般会提供三个构造函数 1234567891011public CustomLayout(Context context) &#123; super(context);&#125;public CustomLayout(Context context, AttributeSet attrs) &#123; this(context, attrs, 0);&#125;public CustomLayout(Context context, AttributeSet attrs, int defStyle) &#123; super(context, attrs, defStyle);&#125; 这三个函数的用途分别是什么？第一个构造函数一般是用户在代码中用 new 来定义一个实例会使用。第二个构造函数则是用户在 xml 中声明了该控件，且没有使用到自定义属性时用到。第三个构造函数同第二个类似，但它会在使用到自定义属性时被调用，第三个参数 defStyle 表示我们自定义的属性的资源索引(在 R.java 这个类中)。 getLayoutParams 通常我们用 View 类的 getLayoutParams 方法得到 LayoutParams ，对应的是这个子 View 所在 ViewGroup 的 LayoutParams。例如，如果子 View 在 LinearLayout 内，那这个 LayoutParams 就是 LinearLayout.LayoutParams。那我们自己定义的 ViewGroup 的 LayoutParams 是怎么来的？ ViewGroup 提供了一个方法给开发者覆写 1234567891011/** * Returns a new set of layout parameters based on the supplied attributes set. * * @param attrs the attributes to build the layout parameters from * * @return an instance of &#123;@link android.view.ViewGroup.LayoutParams&#125; or one * of its descendants */public LayoutParams generateLayoutParams(AttributeSet attrs) &#123; return new LayoutParams(getContext(), attrs);&#125; 我们可以根据自己的需求，返回一个我们需要的 LayoutParams (可以是 LayoutParams 也可以是它的子类)。在 FlowLayout 中，我们只需要知道子 View 之间的间隔，因此返回一个 MarginLayoutParams。 12345// 与当前ViewGroup对应的LayoutParams@Overridepublic LayoutParams generateLayoutParams(AttributeSet attrs) &#123; return new MarginLayoutParams(getContext(), attrs);&#125;","raw":null,"content":null,"categories":[{"name":"Android","slug":"Android","permalink":"https://jermmy.github.io/categories/Android/"}],"tags":[{"name":"Android","slug":"Android","permalink":"https://jermmy.github.io/tags/Android/"}]},{"title":"Android—Handler运作机制剖析（二）","slug":"2016-2-12-Android-Handler运作机制剖析（2）","date":"2016-02-12T06:45:37.000Z","updated":"2017-03-21T01:35:43.000Z","comments":true,"path":"2016/02/12/2016-2-12-Android-Handler运作机制剖析（2）/","link":"","permalink":"https://jermmy.github.io/2016/02/12/2016-2-12-Android-Handler运作机制剖析（2）/","excerpt":"书接上文，我们忽悠完了 Handler 与 Looper 之间的关系以及 Looper 的由来，今天该讲讲 Looper 是怎么帮助 Handler 工作，以及如何支撑整个 app 运转的。对，你没听错，Looper 就是这么强大。","text":"书接上文，我们忽悠完了 Handler 与 Looper 之间的关系以及 Looper 的由来，今天该讲讲 Looper 是怎么帮助 Handler 工作，以及如何支撑整个 app 运转的。对，你没听错，Looper 就是这么强大。 Looper的作用 之前说过，Handler 发送的 Message 会被放入 MessageQueue 中，然后由 Looper 来轮询这个队列并执行消息。首先，为了证明 MessageQueue 确实是由 Looper 来轮询的，不妨先看个例子 1234567891011121314151617181920212223242526272829303132333435MyHandler handler = new MyHandler();@Overrideprotected void onCreate(Bundle savedInstanceState) &#123; super.onCreate(savedInstanceState); setContentView(R.layout.activity_main); MyThread thread = new MyThread(); handler.sendEmptyMessage(0); thread.start();&#125;class MyThread extends Thread &#123; MyHandler handler; @Override public void run() &#123; Looper.prepare(); handler = new MyHandler(); handler.sendEmptyMessage(1); &#125;&#125;class MyHandler extends Handler &#123; @Override public void handleMessage(Message msg) &#123; switch (msg.what) &#123; case 0: Log.i(\"MyHandler\", \"main thread\"); break; case 1: Log.i(\"MyHandler\", \"sub thread\"); break; &#125; &#125;&#125; 我在两条线程中分别用 Handler 发送消息，但在 Log 中只看到一条记录： 1com.xxx.myapplication I/MyHandler: main thread 也就是说，主线程的 Handler 发送的消息被执行了，而子线程的没有。为了探究是什么原因导致这样的区别，博主稍微修改了例子中的 MyThread 类： 1234567891011class MyThread extends Thread &#123; MyHandler handler; @Override public void run() &#123; Looper.prepare(); handler = new MyHandler(); handler.sendEmptyMessage(1); Looper.loop(); &#125;&#125; 再次运行程序后，发现 Log 中出现了两条记录： 12com.xxx.myapplication I/MyHandler: sub threadcom.xxx.myapplication I/MyHandler: main thread 结果跟我们想要的吻合了。于是，不难猜测 Handler 发送的消息其实是由 Looper 的 loop 方法执行的。我们再重头来看下整个消息从发送到被执行的过程。 Handler发送消息 首先，我们看下 Handler 的 sendEmptyMessage 方法背后都在做什么。博主一路跟踪 sendEmptyMessage 的源码，发现最终会调用这个函数 12345678910public boolean sendMessageAtTime(Message msg, long uptimeMillis) &#123; MessageQueue queue = mQueue; if (queue == null) &#123; RuntimeException e = new RuntimeException( this + \" sendMessageAtTime() called with no mQueue\"); Log.w(\"Looper\", e.getMessage(), e); return false; &#125; return enqueueMessage(queue, msg, uptimeMillis);&#125; （有心的读者可以跟踪 Handler 另外两个常用的方法 sendMessage 或 post ，你会发现最终调用的都是上面这个函数） 这个函数做的事情也非常简单，就是调用 enqueueMessage 方法把 Message 放入 MessageQueue 中（ enqueueMessage 我就没有进一步跟踪进去了，它不是文章重点）。 至此，Handler发送消息的过程就结束了。 Looper轮询MessageQueue 从上面的例子我们已经看出，如果没有调用 Looper 的 loop 方法，Handler 发送的消息是没法交给 handleMessage 方法执行的。所以，重头戏都在 loop 方法本身。博主裁剪了一些跟主题无关的源码，你会看到这个方法的重点是一个死循环 1234567891011121314151617181920212223242526272829/** * Run the message queue in this thread. Be sure to call * &#123;@link #quit()&#125; to end the loop. */public static void loop() &#123; final Looper me = myLooper(); if (me == null) &#123; throw new RuntimeException(\"No Looper; Looper.prepare() wasn't called on this thread.\"); &#125; final MessageQueue queue = me.mQueue; ...无关主题的代码 for (;;) &#123; Message msg = queue.next(); // might block if (msg == null) &#123; // No message indicates that the message queue is quitting. return; &#125; ...无关主题的代码 msg.target.dispatchMessage(msg); ...无关主题的代码 &#125; msg.recycleUnchecked(); &#125;&#125; 在 for 循环里面，Looper 会不断地从 queue 里面取出 Message，并通过 dispatchMessage 来执行消息。 msg.target 指的就是发送 msg 的 Handler 本身。我们再到 dispatchMessage 方法里面去看下 123456789101112131415/** * Handle system messages here. */public void dispatchMessage(Message msg) &#123; if (msg.callback != null) &#123; handleCallback(msg); &#125; else &#123; if (mCallback != null) &#123; if (mCallback.handleMessage(msg)) &#123; return; &#125; &#125; handleMessage(msg); &#125;&#125; 看到 handleMessage 方法，一切就水落石出了。这个方法先判断 msg.callback 是否为空，如果你对这个 callback 不熟悉，可以跟踪 Handler 的 post 方法，这个 callback 就是 post 传进去的 Runnable 对象，如果 callback 不为空，则直接执行它即可。否则，会进入 else 语句。mCallback是 Handler 暴露给开发者的接口，博主基本没用过，一般情况下也是 null 的，所以 else 语句大多数情况是执行我们覆写的 handleMessage 方法。 到此，大概的流程我们已经分析完了，总结一下：Handler发送 Message 到 MessageQueue ，Looper 会一直轮询 MessageQueue ，取出消息并重新交给 Handler 执行，所以，消息的发送和执行者都是 Handler ，而 Looper 则起到分发消息的作用。 如何结束轮询 现在，我们遇到的另一个问题是：既然 loop 方法在做一个不断循环地操作，我们要怎样才能让它停下来呢？如果仔细观察 for 循环内部的话，你会发现跳出循环的唯一方法是当 msg 为 null 时执行的 return 语句。 那 msg 什么时候为 null 呢？我们自己传一个空的 Message 进去？如果这样做的话，你的程序会抛出 NullPointerException 。正确的做法是调用 Looper 的 quit 方法，博主重新修改了最开始那个例子 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364MyHandler handler = new MyHandler();Button btn;@Overrideprotected void onCreate(Bundle savedInstanceState) &#123; super.onCreate(savedInstanceState); setContentView(R.layout.activity_main); btn = (Button) findViewById(R.id.btn); final MyThread thread = new MyThread(); handler.sendEmptyMessage(0); thread.start(); btn.setOnClickListener(new View.OnClickListener() &#123; @Override public void onClick(View v) &#123; thread.quitLoop(); &#125; &#125;);&#125;class MyThread extends Thread &#123; MyHandler handler; Looper looper; @Override public void run() &#123; Looper.prepare(); synchronized (this) &#123; looper = Looper.myLooper(); notifyAll(); &#125; handler = new MyHandler(); handler.sendEmptyMessage(1); Looper.loop(); Log.i(\"MyThread\", \"finish loop\"); &#125; public void quitLoop() &#123; synchronized (this) &#123; // 这里加锁的原因是为了防止线程间不同步，导致looper还没初始化就被调用 while (looper == null) &#123; try &#123; wait(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; looper.quit(); // 结束loop方法 &#125; &#125;&#125;class MyHandler extends Handler &#123; @Override public void handleMessage(Message msg) &#123; switch (msg.what) &#123; case 0: Log.i(\"MyHandler\", \"main thread\"); break; case 1: Log.i(\"MyHandler\", \"sub thread\"); break; &#125; &#125;&#125; 博主在 Looper.loop() 之后打了个 Log，并加入一个 Button 来调用 quitLoop 方法。运行起来后，loop 之后的 Log 一直没打印出来，只有点击按钮后才出现。证明 quit 方法确实终止了 loop 循环。关于 quit，还有另一个 quitSafely 函数，从名称上看就能猜出后者比前者安全。由于博主平时并没有使用过这两个方法，就不深入源码细讲了😅。 UI线程的Looper轮询 文章最开始还有一个悬而未解的问题，为什么子线程 Handler 发送的消息没有被处理，而 UI 线程的消息却能被接收处理呢？要知道问题的答案，我们又得看回 ActivityThread.java 中的 main 函数（不清楚地请看回上一篇文章Android–Handler运作机制剖析（一）），这里照搬一下代码 1234567891011121314151617181920212223public static void main(String[] args) &#123; ......无关主题的代码 Looper.prepareMainLooper(); ActivityThread thread = new ActivityThread(); thread.attach(false); if (sMainThreadHandler == null) &#123; sMainThreadHandler = thread.getHandler(); &#125; if (false) &#123; Looper.myLooper().setMessageLogging(new LogPrinter(Log.DEBUG, \"ActivityThread\")); &#125; // End of event ActivityThreadMain. Trace.traceEnd(Trace.TRACE_TAG_ACTIVITY_MANAGER); Looper.loop(); throw new RuntimeException(\"Main thread loop unexpectedly exited\");&#125; 应该能看到 Looper.loop 这一条语句吧，UI 线程也是这样不断轮询的。前面说过，我们整个 app 都是在这个 main 函数中执行的，那是否意味着我们整个 UI 线程，包括所有 Activity 的事件，其实都是在 Looper.loop 中执行的呢？答案就在 ActivityThread.java 那整整五千行的代码中。博主能力有限，暂时还搞不定它😅（感觉能把这个类看懂的同学 Android 的应用层框架也基本熟练掌握了）。在网上找到的关于这个类的解析，不是讲得太深奥就是讲得更深奥，博主找了几篇比较有启发性的（Looper.loop死循环问题这是找到的第一篇），大概翻了一下 ActivityThread.java 的代码，基本猜出 UI 线程的轮询在做什么事了，这里也可以简单说下。 在 main 函数中有这样一句 123if (sMainThreadHandler == null) &#123; sMainThreadHandler = thread.getHandler();&#125; 这个 sMainThreadHandler 是一个 Handler 类，loop 方法中接收和处理的消息也来自这个对象。我们进一步看看 thread.getHandler() 返回的内容是什么 123final Handler getHandler() &#123; return mH;&#125; 是一个叫 mH 的家伙，这个成员在最开始有声明 1final H mH = new H(); 居然又来了个 H 类，再找找吧 1234567891011121314151617181920212223242526272829private class H extends Handler &#123; public static final int LAUNCH_ACTIVITY = 100; public static final int PAUSE_ACTIVITY = 101; public static final int PAUSE_ACTIVITY_FINISHING= 102; public static final int STOP_ACTIVITY_SHOW = 103; public static final int STOP_ACTIVITY_HIDE = 104; public static final int SHOW_WINDOW = 105; public static final int HIDE_WINDOW = 106; ...... public void handleMessage(Message msg) &#123; if (DEBUG_MESSAGES) Slog.v(TAG, \"&gt;&gt;&gt; handling: \" + codeToString(msg.what)); switch (msg.what) &#123; case LAUNCH_ACTIVITY: &#123; Trace.traceBegin(Trace.TRACE_TAG_ACTIVITY_MANAGER, \"activityStart\"); final ActivityClientRecord r = (ActivityClientRecord) msg.obj; r.packageInfo = getPackageInfoNoCheck( r.activityInfo.applicationInfo, r.compatInfo); handleLaunchActivity(r, null); Trace.traceEnd(Trace.TRACE_TAG_ACTIVITY_MANAGER); &#125; break; case RELAUNCH_ACTIVITY: &#123; Trace.traceBegin(Trace.TRACE_TAG_ACTIVITY_MANAGER, \"activityRestart\"); ActivityClientRecord r = (ActivityClientRecord)msg.obj; handleRelaunchActivity(r); Trace.traceEnd(Trace.TRACE_TAG_ACTIVITY_MANAGER); &#125; break; ...... 代码太长，只截取了片段，到此我们不再深入了。其实从 H 类中的变量名以及函数名等，我们也能大概猜到，整个 App 中的事件，包括 Activity 的启动、销毁，Intent 跳转等，都是通过 mH 发出信息后，在 loop 中提取，再交回给 mH 去执行的，具体来讲，就是通过上面代码中的 handleLaunchActivity 这类函数去处理。所以，main 函数在走到 Looper.loop() 后，我们的 app 就一直在 loop 循环中不断地接收消息，并执行。而我们平时经常使用的如 startActivity 这样的函数，最后也是通过 mH 发出消息执行相应的处理函数来完成的。所以说，整个 app 都是靠 Looper 以及 Handler 的相互合作运转的。 至此，整个 Handler 的运作机制算是忽悠完了。这其中还有很多值得慢慢品味的地方，像 Message 的获取这些小细节，还是很能提高 app 的运行效率的。为了不让自己空虚堕落，之后博主会继续找时间研究代码写文章的😐。","raw":null,"content":null,"categories":[{"name":"Android","slug":"Android","permalink":"https://jermmy.github.io/categories/Android/"}],"tags":[{"name":"Android","slug":"Android","permalink":"https://jermmy.github.io/tags/Android/"}]},{"title":"Android--Handler运作机制剖析（一）","slug":"2016-2-11-Android-Handler运作机制剖析（1）","date":"2016-02-12T06:22:28.000Z","updated":"2017-03-21T01:29:09.000Z","comments":true,"path":"2016/02/12/2016-2-11-Android-Handler运作机制剖析（1）/","link":"","permalink":"https://jermmy.github.io/2016/02/12/2016-2-11-Android-Handler运作机制剖析（1）/","excerpt":"博主第一次使用 Handler 是为了在子线程中修改控件内容，因为 Android 不允许非 UI 线程修改控件，因此要用 Handler 通知 UI 线程去修改。今天抽空看了些文章和源码，理解一下背后的运行机制。","text":"博主第一次使用 Handler 是为了在子线程中修改控件内容，因为 Android 不允许非 UI 线程修改控件，因此要用 Handler 通知 UI 线程去修改。今天抽空看了些文章和源码，理解一下背后的运行机制。 Handler, Looper, MessageQueue的关系 可以说，Handler 背后的运作都是靠 Looper 支撑的，它们三者的关系可以这样表示 123456789class Handler &#123; Looper mLooper; MessageQueue queue; // 这个queue其实就是mLooper中的queue ...&#125;final class Looper &#123; MessageQueue queue; ...&#125; Handler 中持有 Looper 的引用，当 Handler 通过 sendMessage 等方法发送 Message 时，这些 Message 会被放入 queue 中，之后 Looper 会不断地轮询 queue，取出信息并执行。所以，如果没有 Looper，Handler 发送的消息只是简单地放入队列，而不会执行。下面从源码的角度慢慢剖析这些是怎么实现的。 Handler中的Looper是怎么来的 这个问题要从构造函数中入手，平时实例化 Handler 的代码一般是这样的 1Handler mHandler = new Handler(); 这里面没有传入任何参数，博主从这个最基本的构造函数进入源码后，发现只是一句代码的事 123public Handler() &#123; this(null, false);&#125; 它会进一步调用下面这个构造函数 123456789101112public Handler(Callback callback, boolean async) &#123; ...一些无关主题的代码 mLooper = Looper.myLooper(); if (mLooper == null) &#123; throw new RuntimeException( \"Can't create handler inside thread that has not called Looper.prepare()\"); &#125; mQueue = mLooper.mQueue; mCallback = callback; mAsynchronous = async;&#125; mLooper 引用的赋值会进一步调用 Looper.myLooper 函数 123456// sThreadLocal.get() will return null unless you've called prepare().static final ThreadLocal&lt;Looper&gt; sThreadLocal = new ThreadLocal&lt;Looper&gt;();public static @Nullable Looper myLooper() &#123; return sThreadLocal.get();&#125; 这个 ThreadLocal 保存的是线程的一些局部变量，具体来讲，当我们调用它的 get 方法时，它会返回当前线程的 Looper 引用，这样，Handler 就与所在线程的 Looper 绑定在一起了。那线程的 Looper 又是怎么来的呢？在揭示谜底前，博主先用一个例子铺垫。 新建一个 Android 工程，在一个子线程中实例化 Handler 123456789101112131415161718@Overrideprotected void onCreate(Bundle savedInstanceState) &#123; super.onCreate(savedInstanceState); setContentView(R.layout.activity_main); MyThread thread = new MyThread(); thread.start();&#125;class MyThread extends Thread &#123; Handler handler; @Override public void run() &#123; handler = new Handler(); Log.i(\"MyThread\", \"handler instantiation\"); &#125;&#125; 代码很简洁，先不要问为什么要在子线程中实例化，后面解释。跑起来后等待异常抛出： 123E/AndroidRuntime: FATAL EXCEPTION: Thread-440 java.lang.RuntimeException: Can't create handler inside thread that has not called Looper.prepare() at android.os.Handler.&lt;init&gt;(Handler.java:121) 好，真相就是，线程的 Looper 是通过 Looper.prepare() 获得的，直接找它的源码呀 12345678910111213141516 /** Initialize the current thread as a looper. * This gives you a chance to create handlers that then reference * this looper, before actually starting the loop. Be sure to call * &#123;@link #loop()&#125; after calling this method, and end it by calling * &#123;@link #quit()&#125;. */public static void prepare() &#123; prepare(true);&#125;private static void prepare(boolean quitAllowed) &#123; if (sThreadLocal.get() != null) &#123; throw new RuntimeException(\"Only one Looper may be created per thread\"); &#125; sThreadLocal.set(new Looper(quitAllowed));&#125; 好了，现在我们知道调用 prepare 函数的时候，系统 new 了一个 Looper 并 set 到 ThreadLocal 里面去了，这样，这个线程对应的 TheadLocal 便持有了 Looper，然后我们在实例化 Handler 的时候，Looper用 myLooper 方法从这个 ThreadLocal 中取出 Looper，并跟 Handler 关联起来。至此，Handler 实例化完成。读者可以试一下把上面例子中的 MyThread 类修改如下，看看能不能正常实例化 Handler 123456789101112class MyThread extends Thread &#123; Handler handler; @Override public void run() &#123; Looper.prepare(); handler = new Handler(); Log.i(\"MyThread\", \"handler instantiation\"); Log.i(\"MyThread\", \"\" + Thread.currentThread()); Log.i(\"MyThread\", \"\" + handler.getLooper().getThread()); &#125;&#125; 注意，一定要在 run 方法中实例化，只有 run 方法运行的时候，系统才将 cpu 切换给子线程，run 方法外面的作用域还都是 UI 线程的。可以看看 Log 中 handler 所在的线程是否是子线程。 UI线程的Looper 那么为什么我要在子线程中实例化 Handler 呢？只是想让你感受下差别。想想你平时在 UI 线程中实例化 Handler 的时候肯定没调用 Looper.prepare 方法吧。我们没调用并不代表系统不会帮我们调用呀，我们来看看 UI 线程的 Looper 是怎么实例化的。 我们要打开一个叫做 ActivityThread.java 的文件，博主在 AS 中没法进入这个文件的源码，所以就直接在 sdk 的 sources 目录下寻找了（现在知道下载源码的重要性了吧^_^）。这个类的包名是 android.app ，找到这个文件后打开它，在文件最后找到 main 函数。 123456789101112131415161718192021222324public static void main(String[] args) &#123; ......无关主题的代码 Looper.prepareMainLooper(); ActivityThread thread = new ActivityThread(); thread.attach(false); if (sMainThreadHandler == null) &#123; sMainThreadHandler = thread.getHandler(); &#125; if (false) &#123; Looper.myLooper().setMessageLogging(new LogPrinter(Log.DEBUG, \"ActivityThread\")); &#125; // End of event ActivityThreadMain. Trace.traceEnd(Trace.TRACE_TAG_ACTIVITY_MANAGER); Looper.loop(); throw new RuntimeException(\"Main thread loop unexpectedly exited\");&#125; 这个 main 是不是跟 Java 的 main 函数一模一样啊，Android 的第一层编译处理用的仍然是 Java compiler ，因此语法上肯定得符合 Java 的规范啦，没有 main 函数怎么玩。官网有讲到整个 build 的流程http://developer.android.com/intl/zh-cn/sdk/installing/studio-build.html 。 作为整个应用程序的入口，我们所有的 Activity 都是从这个 main 函数启动的呢。所以，这个 main 函数也就是我们平时说的 UI 线程。然后你看最开始那里，调用了 Looper 的 prepareMainLooper 方法，从名字推测也能知道就是初始化 UI 线程的 Looper 啦，不放心的话，我们再看看源码 123456789101112131415161718192021222324252627282930/** * Initialize the current thread as a looper, marking it as an * application's main looper. The main looper for your application * is created by the Android environment, so you should never need * to call this function yourself. See also: &#123;@link #prepare()&#125; */public static void prepareMainLooper() &#123; prepare(false); synchronized (Looper.class) &#123; if (sMainLooper != null) &#123; throw new IllegalStateException(\"The main Looper has already been prepared.\"); &#125; sMainLooper = myLooper(); &#125;&#125;private static void prepare(boolean quitAllowed) &#123; if (sThreadLocal.get() != null) &#123; throw new RuntimeException(\"Only one Looper may be created per thread\"); &#125; sThreadLocal.set(new Looper(quitAllowed));&#125;/** * Return the Looper object associated with the current thread. Returns * null if the calling thread is not associated with a Looper. */public static @Nullable Looper myLooper() &#123; return sThreadLocal.get();&#125; 懂了吧，虽然拐了几个弯，道理都是一样的。在UI主线程中，系统通过 ThreadLocal 获得 UI 线程的 Looper 对象。所以你平时直接通过 Handler handler = new Handler(); 是不会有问题的，因为这个 Handler 会跟主线程的 sMainLooper 绑定。 Handler绑定Looper 讲到这里，对于 Handler 的 Looper 是怎么来的这个问题应该没有疑问了吧。那如果主线程声明的 Handler 想跟子线程的 Looper 绑定要怎么做呢？除了前面提到的在 run 方法中实例化 Handler，Android 提供了其他灵活的接口，我们可以用 Handler 的另一个构造函数 12345678/** * Use the provided &#123;@link Looper&#125; instead of the default one. * * @param looper The looper, must not be null. */public Handler(Looper looper) &#123; this(looper, null, false);&#125; 先在子线程中实例化Looper，再把这个Looper赋值给Handler就行啦。具体可以看下面这个例子： 12345678910111213141516171819202122Handler handler;@Overrideprotected void onCreate(Bundle savedInstanceState) &#123; super.onCreate(savedInstanceState); setContentView(R.layout.activity_main); MyThread thread = new MyThread(); thread.start(); handler = new Handler(thread.looper); Log.i(\"main thread\", \"\" + handler.getLooper().getThread());&#125;class MyThread extends Thread &#123; Looper looper; @Override public void run() &#123; Looper.prepare(); looper = Looper.myLooper(); Log.i(\"sub thread\", \"\" + Thread.currentThread()); &#125;&#125; 代码很简单对吧，跑起来看挂不挂，反正我是挂了，什么异常呢 123Caused by: java.lang.NullPointerException at android.os.Handler.&lt;init&gt;(Handler.java:157) at com.xxx.myapplication.MainActivity.onCreate(MainActivity.java:19) 这个 Looper 居然是空的。其实也不足为奇，因为你在子线程中实例化的 Looper，又在主线程中实例化 Handler，稍微不同步就出差错了。那怎么让它们同步呢？就看你操作系统怎么学了。博主在原来的基础上加了判断 1234567891011121314151617181920212223242526272829303132333435363738Handler handler;@Overrideprotected void onCreate(Bundle savedInstanceState) &#123; super.onCreate(savedInstanceState); setContentView(R.layout.activity_main); MyThread thread = new MyThread(); thread.start(); handler = new Handler(thread.getLooper()); Log.i(\"main thread\", \"\" + handler.getLooper().getThread());&#125;class MyThread extends Thread &#123; Looper looper; @Override public void run() &#123; Looper.prepare(); synchronized (this) &#123; looper = Looper.myLooper(); notifyAll(); &#125; Log.i(\"sub thread\", \"\" + Thread.currentThread()); &#125; public Looper getLooper() &#123; synchronized (this) &#123; while (looper == null) &#123; try &#123; wait(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125; return looper; &#125;&#125; 这样，UI 线程在实例化 Handler 的时候，如果 Looper 是 null 的话，UI 线程就会等待，直到子线程成功实例化 looper 并唤醒。是不是觉得博主有点机智啊^_^，其实我是参考了 Android 的源码写的，Android 已经预料到这种问题，并提供了一个 HandlerThread 来帮我们处理同步的问题，这里就不细讲了，有兴趣的读者可以自行查看。（话说，博主在实际开发中并没有遇到在 Handler 中绑定其他 Looper 的例子，博主还比较小白😅） 扯了这么多，总算搞定了 Looper 来源的问题，一切才刚开始，下一篇文章我们再来慢慢剖析 Looper 是怎么工作的。","raw":null,"content":null,"categories":[{"name":"Android","slug":"Android","permalink":"https://jermmy.github.io/categories/Android/"}],"tags":[{"name":"Android","slug":"Android","permalink":"https://jermmy.github.io/tags/Android/"}]}]}