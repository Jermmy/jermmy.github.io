<!doctype html>



  


<html class="theme-next muse use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>



  
  
  <link rel="stylesheet" media="all" href="/lib/Han/dist/han.min.css?v=3.3">




<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.1" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="机器学习,决策树," />








  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.1" />






<meta name="description" content="关于决策树，想必大部分人都已经耳熟能详了，这是一种用来预测行为的树状分叉结构。本文主要想总结一下最常用的决策树生成算法：ID3，C4.5 以及 CART。 构造的原则 熟悉决策树的你一定记得，决策树每个非叶子结点对应的其实是一个属性。比方说，想判断一个男生是不是 gay，我们首先需要判断他的性别是不是男的，是的话继续判断他的性取向，之后继续判断他的其他行为……这里的「性别」，「性取向」就是属性，而">
<meta name="keywords" content="机器学习,决策树">
<meta property="og:type" content="article">
<meta property="og:title" content="决策树生成算法">
<meta property="og:url" content="https://jermmy.github.io/2017/04/13/2017-4-13-decision-tree/index.html">
<meta property="og:site_name" content="Jermmy&#39;s Lazy Blog">
<meta property="og:description" content="关于决策树，想必大部分人都已经耳熟能详了，这是一种用来预测行为的树状分叉结构。本文主要想总结一下最常用的决策树生成算法：ID3，C4.5 以及 CART。 构造的原则 熟悉决策树的你一定记得，决策树每个非叶子结点对应的其实是一个属性。比方说，想判断一个男生是不是 gay，我们首先需要判断他的性别是不是男的，是的话继续判断他的性取向，之后继续判断他的其他行为……这里的「性别」，「性取向」就是属性，而">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="https://jermmy.github.io/images/2017-4-13/eg.png">
<meta property="og:image" content="https://jermmy.github.io/images/2017-4-13/tree1.png">
<meta property="og:image" content="https://jermmy.github.io/images/2017-4-13/tree2.png">
<meta property="og:updated_time" content="2019-05-04T08:05:12.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="决策树生成算法">
<meta name="twitter:description" content="关于决策树，想必大部分人都已经耳熟能详了，这是一种用来预测行为的树状分叉结构。本文主要想总结一下最常用的决策树生成算法：ID3，C4.5 以及 CART。 构造的原则 熟悉决策树的你一定记得，决策树每个非叶子结点对应的其实是一个属性。比方说，想判断一个男生是不是 gay，我们首先需要判断他的性别是不是男的，是的话继续判断他的性取向，之后继续判断他的其他行为……这里的「性别」，「性取向」就是属性，而">
<meta name="twitter:image" content="https://jermmy.github.io/images/2017-4-13/eg.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    sidebar: {"position":"left","display":"hide","offset":12,"offset_float":0,"b2t":false,"scrollpercent":false},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://jermmy.github.io/2017/04/13/2017-4-13-decision-tree/"/>





  <title>决策树生成算法 | Jermmy's Lazy Blog</title>
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  




<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
            (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
          m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-84659849-1', 'auto');
  ga('send', 'pageview');
</script>


  <script type="text/javascript">
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?4d053879042f439aeb8fc5996a907b6b";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>










  
  
    
  

  <div class="container sidebar-position-left page-post-detail ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Jermmy's Lazy Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="https://jermmy.github.io/2017/04/13/2017-4-13-decision-tree/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Jermmy">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jermmy's Lazy Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                决策树生成算法
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-04-13T12:10:03+08:00">
                2017-04-13
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/机器学习/" itemprop="url" rel="index">
                    <span itemprop="name">机器学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2017/04/13/2017-4-13-decision-tree/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count"
                        data-disqus-identifier="2017/04/13/2017-4-13-decision-tree/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          
             <span id="/2017/04/13/2017-4-13-decision-tree/" class="leancloud_visitors" data-flag-title="决策树生成算法">
               <span class="post-meta-divider">|</span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               
                 <span class="post-meta-item-text">阅读次数 </span>
               
                 <span class="leancloud-visitors-count"></span>
             </span>
          

          

          

          

        </div>
      </header>
    

    <div class="post-body han-init-context" itemprop="articleBody">

      
      

      
        <p>关于决策树，想必大部分人都已经耳熟能详了，这是一种用来预测行为的树状分叉结构。本文主要想总结一下最常用的决策树生成算法：<strong><a href="https://en.wikipedia.org/wiki/ID3_algorithm" target="_blank" rel="noopener">ID3</a></strong>，<strong><a href="https://en.wikipedia.org/wiki/C4.5_algorithm" target="_blank" rel="noopener">C4.5</a></strong> 以及 <strong><a href="https://en.wikipedia.org/wiki/Predictive_analytics#Classification_and_regression_trees_.28CART.29" target="_blank" rel="noopener">CART</a></strong>。</p>
<h2 id="构造的原则">构造的原则</h2>
<p>熟悉决策树的你一定记得，决策树每个非叶子结点对应的其实是一个属性。比方说，想判断一个男生是不是 gay，我们首先需要判断他的性别是不是男的，是的话继续判断他的性取向，之后继续判断他的其他行为……这里的「性别」，「性取向」就是属性，而决策树的生成其实是依次挑选这些属性组成自己的节点，到最终可以明确得出结论的时候（也就是叶子结点），整棵树便生成了。所以，我们的目标就是按照某种方法依次挑选出这些属性。</p>
<p>我们挑选的原则是：每次选出这个属性后，可以最大限度地减小分类的可能性。回到 gay 这个问题，如果摆在我们眼前的属性有：「性取向」，「是否喜欢日漫」，「是否长发披肩」，那么，选择「性取向」这个属性，对我们之后的判断，帮助无疑是最大的。因为得知「性取向」之后，基本也就得到结论了。所以，对这个例子而言，「性取向」是我们优先挑选的属性。</p>
<p>那么，我们如何衡量这种帮助的大小呢？请往下看👇。</p>
<a id="more"></a>
<h2 id="id3-算法">ID3 算法</h2>
<p>ID3 算法归根到底就是提出一种合理的选择属性的方法。</p>
<p>（注意，决策树是一种知识学习算法，只有从众多样本中才能得出哪个属性最好，所以，构造决策树的前提是有大量的样本可供学习）</p>
<p>下面，为了方便讲解，我们需要引入信息学中<strong>「熵」</strong>的概念。</p>
<h3 id="熵entropy">熵（entropy）</h3>
<p>第一次接触熵的概念是在学高中化学的时候，课本告诉我们：一堆整齐有序的分子，最终都会演变成一个混乱复杂的群体，也就是，这个系统的熵值会逐渐变大。因此，简单整齐的系统，熵越小，越混乱的系统，熵越大。接下来，让我们回顾一下分子的布朗运动……</p>
<p>开个玩笑啦🤗。</p>
<p>同化学里的熵一样，信息学的熵也有类似的作用。在信息学中，如果熵越大，证明掌握的信息越少，事情越不确定。看到这里，你有没有觉得，熵的定义和我们前面提出的挑选属性的原则有点类似。是的，ID3 的精髓也就是在这，它通过计算属性的熵，来得出一个属性对事情的确定性能产生多大的影响，从而选出最好的属性。</p>
<p>那么熵该如何度量呢？</p>
<p>著名的信息论创始人「香农」提出一个度量熵的方法：假设有一堆样本 D，那么 D 的熵可以这样计算： <span class="math display">\[
H(D)=-\sum_{i=1}^{m}{p_ilog_2(p_i)}
\]</span> 其中，<span class="math inline">\(p_i\)</span> 表示第 i 个类别在整个样本中出现的概率。 举个例子吧。假设我们投掷 10 枚硬币，其中，5 枚正面朝上，5 枚正面朝下，那么我们总共得到 10 个样本，这堆样本的熵为：</p>
<p><span class="math inline">\(H(D)=-(\frac{5}{10}log_2{\frac{5}{10}} + \frac{5}{10}log_2{\frac{5}{10}})=1\)</span></p>
<p>反之，如果只有 1 枚硬币正面朝上，9 枚硬币正面朝下，那么熵为：</p>
<p><span class="math inline">\(H(D)=-(\frac{1}{10}log_2{\frac{1}{10}} + \frac{9}{10}log_2{\frac{9}{10}})=0.469\)</span></p>
<p>如果全部硬币正面朝上，你应该可以算出来，熵为 0。 举这个例子是想说明：当熵的值越大的时候，事情会更加难以确定，如果你知道 10 次实验中，正面朝上的为 5 次，朝下的也为 5 次，那么下一次哪一面朝上，你是不是很难确定。相反，如果熵的值越小，事情就越明朗。当熵为 0，也就是 10 次都正面朝上的时候，下一次你会不会觉得正面朝上的概率会大很多（请忘掉你的传统思维，我没说这是一枚正常的硬币）。</p>
<h3 id="选择属性">选择属性</h3>
<p>好了，有了熵的概念以及度量方法，下面我们可以正式地走一遍 ID3 的流程了。 同样的，假设我们有一堆数据 D，我们先计算出这堆样本的熵<span class="math inline">\(H(D)\)</span>，接下来，我们要对每个属性对信息的价值进行评估。假设我们挑选出 A 属性，那么，根据 A 属性的类别，我们可以把这堆样本分成几个子样本，每个子样本都对应 A 属性中的一类。我们继续按照熵的定义计算这几个子样本的熵，由于它们的熵是挑选出 A 后剩下的，因此我们定义为： <span class="math display">\[
Remainder(A)=\sum_{j=1}^{v}\frac{|D_j|}{|D|}H(D_j)
\]</span> 这个公式其实和之前的是一个道理，我们通过 A 将 D 分成几个子集 <span class="math inline">\(D_j\)</span>，这个时候，我们仍然需要计算这堆样本的熵，因此，先分别计算出每个 <span class="math inline">\(D_j\)</span> 的熵，然后乘以这个 <span class="math inline">\(D_j\)</span> 样本占所有数据集的比重，最后将全部子集的熵加起来即可。前面说了，这个熵是挑选 A 后剩下的，那么很自然的，我们想知道 A 到底帮助消减了多少熵，于是，我们定义最后一个公式，即<strong>信息增益</strong>： <span class="math display">\[
Gain(A)=H(D)-Remainder(A)
\]</span> 之前对熵的说明告诉我们，熵越大，信息越少，反之，信息越多。<span class="math inline">\(Gain(A)\)</span>其实就是 A 对信息的确定作用，它是我们选出 A 属性后，信息的混乱程度减少的量。 好了，到这里，ID3 的关键部分已经讲完了。其实，每次挑选属性的时候，我们都是计算出所有属性的<strong>信息增益</strong>，选择最大的作为分裂的属性，将样本分成几个子集，然后，对每个子集，继续选出最好的属性，继续分裂，直到所有样本都属于同一类为止（都是 gay 或者都是正面朝上）</p>
<h3 id="举个例子">举个例子</h3>
<p>下面用的这个例子摘自文末的参考博客<a href="http://www.cnblogs.com/leoo2sk/archive/2010/09/19/decision-tree.html" target="_blank" rel="noopener">算法杂货铺——分类算法之决策树(Decision tree)</a>。 假设我们有以下这堆 SNS 社区的资料，我们想确定一个账号是否是真实。</p>
<center>
<img src="/images/2017-4-13/eg.png" width="500px">
</center>
<p>其中，s 、m 和 l 分别表示小、中和大。 我们先计算出这堆样本的熵：</p>
<p><span class="math inline">\(H(D)=-(0.7*log_2{0.7}+0.3*log_2{0.3}) = 0.879\)</span></p>
<p>然后，我们计算每个属性的信息增益：</p>
<p><span class="math display">\[
\begin{align}
Remainder(L)=&amp;0.3*(-\frac{0}{3}log_2{0}{3}-\frac{3}{3}log_2{\frac{3}{3}})\\&amp;+0.4*(-\frac{1}{4}log_2\frac{1}{4}-\frac{3}{4}log_2\frac{3}{4})\\&amp;+0.3*(-\frac{1}{3}log_2\frac{1}{3})=0.603 
\end{align}
\]</span></p>
<p><span class="math display">\[
Gain(L)=0.879-0.603=0.276
\]</span></p>
<p>同样的道理：</p>
<p><span class="math display">\[Gain(F)=0.553\]</span></p>
<p><span class="math display">\[Gain(H)=0.033\]</span></p>
<p>经过比较，我们发现 F 的增益最高，于是选出 F 作为节点，构造出如下决策树：</p>
<center>
<img src="/images/2017-4-13/tree1.png" width="500px">
</center>
<p>注意，F 属性有三个类别，对应三个分支，其中，l 和 m 两个分支的数据都是同一类（账号真实性要么都是 no 要么都是 yes），因此这两个分支没法再分了，而 s 属性的分支，剩下一个四个样本的子集，我们之后的任务，是对这个子集继续分割，直到没法再分为止。 接下来要考虑 L 和 H 属性，同样的道理，我们继续计算增益，只不过这一次我们是在这个子集上计算。</p>
<p><span class="math display">\[H(D)=-(\frac{3}{4}*log_2{\frac{3}{4}}+\frac{1}{4}*log_2{\frac{1}{4}})=0.811\]</span></p>
<p><span class="math display">\[Remainder(L)=\frac{1}{2}*(0)+\frac{1}{2}(-\frac{1}{2}log_2{\frac{1}{2}}-\frac{1}{2}log_2{\frac{1}{2}})=0.5\]</span></p>
<p><span class="math display">\[Remainder(H)=\frac{3}{4}*[-\frac{2}{3}log_2(\frac{2}{3})-\frac{1}{3}log_2(\frac{1}{3})]+\frac{1}{4}*0=0.689\]</span></p>
<p><span class="math display">\[Gain(L)=0.811-0.5=0.311\]</span></p>
<p><span class="math display">\[Gain(H)=0.811-0.689=0.122\]</span></p>
<p>这一次，我们选择 L 属性进行分裂：</p>
<center>
<img src="/images/2017-4-13/tree2.png" width="400px">
</center>
<p>剩下的只有 H 属性，因此最后加上 H 节点。由于剩下的样本中只有 H=no 的数据，因此 yes 节点的数据没法判断（这种情况在数据量很大的时候一般不会遇到，因为数据量越大，涵盖的情况会更多），而剩下的两个样本存在 yes 和 no 两种情况，因此 no 节点往下也只能随机选择一种类别进行判断（这种情况一般是根据进行「多数表决」，即选择出现次数最多的类别作为最终类别，在数据量很大的情况下，出现次数一样多的情况几乎不会发生）。</p>
<h3 id="属性为连续值的情况">属性为连续值的情况</h3>
<p>上面给出的例子中，样本的特征都是离散值（e.g. s，m，l），而 ID3 算法确实也只对离散值起作用。如果遇到特征为连续值的情况，一般需要先将其离散化，例如：可以选定几个阈值<span class="math inline">\(a_1\)</span>，<span class="math inline">\(a_2\)</span>，<span class="math inline">\(a_3\)</span>（<span class="math inline">\(a_1&lt;a_2&lt;a_3\)</span>），根据这些阈值，将样本特征分为四类：<span class="math inline">\(f &lt; a_1\)</span>，<span class="math inline">\(a_1 &lt; f &lt; a_2\)</span>，<span class="math inline">\(a_2 &lt; f &lt; a_3\)</span>，<span class="math inline">\(f &gt; a_3\)</span>。然后，便可以按照一般的思路构建决策树了。</p>
<h2 id="c4.5算法">C4.5算法</h2>
<p>C4.5 算法主要对 ID3 进行了改进，用「增益率」来衡量属性的信息增益效率。算法中定义了「分裂信息」：</p>
<p><span class="math inline">\(SplitInfo(A)=-\sum_{j=1}^{v}{\frac{|D_j|}{|D|}log_2{\frac{|D_j|}{|D|}}}\)</span></p>
<p>然后，通过该信息，定义增益率公式为：</p>
<p><span class="math inline">\(GainRatio(A)=\frac{Gain(A)}{SplitInfo(A)}\)</span></p>
<p>C4.5选择具有最大「增益率」的属性作为分裂属性，而其余步骤，和 ID3 完全一致。</p>
<h2 id="cart">CART</h2>
<p>CART 指的是分类回归树（Classification And Regression Tree）。顾名思义，这是一棵既可以用于分类，也可以用于回归的树。不同于上面的两种树，CART 每一个非叶子节点只有有两个分支，所以 CART 是一棵<strong>二叉树</strong>。下面我们按照分类和回归两个用途分别介绍 CART 的构建。</p>
<h3 id="分类树的生成">分类树的生成</h3>
<p>CART 在选择分裂节点的时候，用「基尼指数（Gini）」来挑选最合适的特征进行分裂。所谓「基尼指数」，其实和 ID3 中熵的作用类似。假设我们有一个数据集 D，其中包含 N 个类别，那么「基尼指数」为： <span class="math display">\[
Gini(D) = 1 - \sum_{j=1}^{N}{P_j^2}
\]</span> 其中，<span class="math inline">\(P_j\)</span>表示每个类别出现的概率。 同熵一样，「基尼系数」的值越小，样本越纯，分类越容易。 我们根据 Gini 选择一个最合适的特征作为 CART 的分裂节点。注意，与 ID3 不同的是，如果特征的类别超过两类，CART 不会根据特征的所有类别分出子树，而是选择其中的一个类别，根据是否属于这个类别分成两棵子树。假设 A 特征中有三种类别（s、m、l），我们需要分别按照「是否属于 s 」、「是否属于 m 」、「是否属于 l 」将样本分为两类，根据 Gini 值最小的情况挑选出分裂的特征类别（比如：按照「是否属于 s 」将样本分为两类）。对于分裂后的样本的 Gini 值，我们按照如下公式计算： <span class="math display">\[
Gini(D, A) = \sum_{j}^{k}{\frac{|D_j|}{|D|}}
\]</span> 其中，k 表示分裂的子集数目。事实上，在 CART 中，k 的取值为 2。 然后，选择 <span class="math inline">\(Gini(D, A)\)</span> 最小的特征 A 作为分裂的特征即可。 这里还需要注意一点，在 ID3 中，已经选择过的特征是没法在之后的节点分裂中被选上的，即每个特征只能被挑选一次。但 CART 没有这种限制，每次都是将所有特征放在一起，通过「基尼系数」选出最好的，哪怕这个特征已经在之前被挑选过了。有学者认为，ID3 这种只挑选一次的做法容易导致 overfitting 问题，所以相信 CART 的这种做法能使模型的泛化能力更强。 CART 按照这样的方式，不断挑选特征分裂子数，直到剩下的子样本都属于同一类别，或者特征没法再分为止，这个停止条件可以参考 ID3 ，这里就不再举例说明了。</p>
<h3 id="回归树的生成">回归树的生成</h3>
<p>回归树相对来说比较难理解，我自己也花了较长时间咀嚼，其中还有一些不明白的地方，日后有了新的想法会继续补充修正。 为了更好地说明回归树的构建流程，我们假设有以下训练数据：</p>
<table>
<thead>
<tr class="header">
<th style="text-align: center;"><span class="math inline">\(X\)</span></th>
<th style="text-align: center;"><span class="math inline">\(Y\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">(<span class="math inline">\(x_{11}\)</span>，<span class="math inline">\(x_{12}\)</span>，<span class="math inline">\(x_{13}\)</span>)</td>
<td style="text-align: center;"><span class="math inline">\(y_1\)</span></td>
</tr>
<tr class="even">
<td style="text-align: center;">(<span class="math inline">\(x_{21}\)</span>，<span class="math inline">\(x_{22}\)</span>，<span class="math inline">\(x_{23}\)</span>)</td>
<td style="text-align: center;"><span class="math inline">\(y_2\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: center;">(<span class="math inline">\(x_{31}\)</span>，<span class="math inline">\(x_{32}\)</span>，<span class="math inline">\(x_{33}\)</span>)</td>
<td style="text-align: center;"><span class="math inline">\(y_3\)</span></td>
</tr>
</tbody>
</table>
<p>上面的表中一共有三个样本，每个样本有三个特征，为了解说方便，我们分别命名为特征 1、特征 2、特征 3（比如：<span class="math inline">\(x_{11}\)</span>，<span class="math inline">\(x_{21}\)</span>，<span class="math inline">\(x_{31}\)</span> 就属于特征 1）。 与分类树类似，回归树每次也需要从样本 <span class="math inline">\(X\)</span> 选取特征，并根据特征值将数据集切分为两份。在分类树中，我们是用「熵」或者「基尼系数」来确定分裂特征，但回归树中的 <span class="math inline">\(X\)</span> 和 <span class="math inline">\(Y\)</span> 都是连续值，因此需要采用新的特征挑选方式。 首先，我们先简单明确一下回归树的分裂原则：<strong>每次分裂后，每个样本集合内的数据要尽可能相似</strong>。这一点与分类树是不谋而合的，尽可能相似就是说，类别上要尽量统一。而为了做到这一点，最常用的方法便是「最小二乘法」。下面，我们定义「最小二乘法」的代价函数（可以简单认为就是回归树的信息熵）： <span class="math display">\[
\min_{j,s}[\min_{c_1}\sum_{x_i\ \in\ {R_1\ (j,s)}}{(y_i-c_1)}^2+\min_{c_2}\sum_{x_i\ \in\ {R_2\ (j,s)}}{(y_i-c_2)}^2]
\]</span></p>
<p>其中，</p>
<ul>
<li><span class="math inline">\(j\)</span> 表示样本的特征，上面例子中的 <span class="math inline">\(x_{11}\)</span>，<span class="math inline">\(x_{21}\)</span> 就属于同一个特征。</li>
<li><span class="math inline">\(s\)</span> 表示特征的分裂值，如果 <span class="math inline">\(s=x_{11}\)</span>，就表示所有样本以特征 1 为基准，按照 <span class="math inline">\(&gt;=x_{11}\)</span> 和 <span class="math inline">\(&lt;x_{11}\)</span> 分为两类。</li>
<li><span class="math inline">\(R_1\)</span> 表示分裂后的第一个样本集，<span class="math inline">\(R_2\)</span> 表示分裂后的第二个样本集。</li>
<li><span class="math inline">\(c_1\)</span>、<span class="math inline">\(c_2\)</span> 分别表示 <span class="math inline">\(R_1\)</span>、<span class="math inline">\(R_2\)</span> 的固定输出值。简单点说，它们是最能代表 <span class="math inline">\(R_1\)</span>，<span class="math inline">\(R_2\)</span> 内所有样本的值。</li>
</ul>
<p>如果我们进一步对 <span class="math inline">\(\sum_{x_i\ \in\ {R_1\ (j,s)}}{(y_i-c_1)}^2\)</span> 求导的话，就会发现，要使这个式子最小，<span class="math inline">\(c\)</span> 必须取 <span class="math inline">\(y_i\)</span> 的平均值（ <span class="math inline">\(y_i \in R\)</span> ）。因此，我们对原公式进行简化： <span class="math display">\[
\min_{j,s}[\sum_{x_i\ \in\ {R_1\ (j,s)}}{(y_i-c_1)}^2+\sum_{x_i\ \in\ {R_2\ (j,s)}}{(y_i-c_2)}^2]
\]</span> 其中，<span class="math inline">\(c_1\)</span>、<span class="math inline">\(c_2\)</span> 分别是 <span class="math inline">\(R_1\)</span>、<span class="math inline">\(R_2\)</span> 两个集合中 <span class="math inline">\(y\)</span> 的平均值。</p>
<p>（希望上面对符号的说明能减少读者对公式的畏惧🤒）</p>
<p>这个公式的做法其实很简单，就是枚举所有特征以及特征值，挑选出最好的特征以及特征值作为分裂点，将样本分为两部分，其中，每一部分内的样本值 <span class="math inline">\(y\)</span> 的平方差最小。平方差最小，意味着这个样本内的数据是最相近的，可以认为属于同一类。</p>
<p>至此，回归树的精髓部分就介绍完了。下面顺藤摸瓜讲一下回归树的构建过程。</p>
<p><strong>最小二乘回归树生成算法：</strong></p>
<ol type="1">
<li>依次遍历每个特征 j，根据所有样本中特征 j 的取值 s，我们按照上面的公式计算代价函数，这样便可以得到每对 ( <span class="math inline">\(j\)</span>，<span class="math inline">\(s\)</span> ) 的代价函数，我们选择函数值最小的作为切分点；</li>
<li>使用上一步的切分点将数据分为两份；</li>
<li>重复第 1、2 步，直到样本的平方差小于阈值或样本数目小于阈值为止。此时，叶子节点的数据就是该样本空间 <span class="math inline">\(R_m\)</span> 的平均值 <span class="math inline">\(c_m\)</span>；</li>
<li>根据第 3 步构造的各个样本空间 <span class="math inline">\(R_m\)</span>，生成回归树。</li>
</ol>
<p>在本文参考的博文 <a href="https://cethik.vip/2016/09/21/machineCAST/" target="_blank" rel="noopener">CART之回归树构建</a>内有一个构建回归树的简单例子，虽然计算方式略有不同，但和本文陈述的方法应该大同小异，我这里偷个懒，就不再举例了。</p>
<h2 id="参考">参考</h2>
<ul>
<li><a href="http://www.cnblogs.com/leoo2sk/archive/2010/09/19/decision-tree.html" target="_blank" rel="noopener">算法杂货铺——分类算法之决策树(Decision tree)</a></li>
<li><a href="https://en.wikipedia.org/wiki/ID3_algorithm" target="_blank" rel="noopener">ID3 algorithm</a></li>
<li><a href="https://en.wikipedia.org/wiki/C4.5_algorithm" target="_blank" rel="noopener">C4.5 algorithm</a></li>
<li><a href="http://www.cnblogs.com/happyblog/archive/2011/09/30/2196901.html" target="_blank" rel="noopener">CART算法学习及实现</a></li>
<li><a href="http://idatamining.net/blog/?p=698" target="_blank" rel="noopener">分类与回归树（classification and regression tree，CART）</a></li>
<li><a href="http://www.cnblogs.com/liuwu265/p/4688403.html" target="_blank" rel="noopener">CART分类与回归树 学习笔记</a></li>
<li><a href="https://cethik.vip/2016/09/21/machineCAST/" target="_blank" rel="noopener">CART之回归树构建</a></li>
</ul>

      
    </div>

    <div>
      
        

      
    </div>

    <div>
      
        

      
    </div>

    <div>
      
        
  <ul class="post-copyright">
    <li class="post-copyright-author">
      <strong>本文作者：</strong>
      Jermmy
    </li>
    <li class="post-copyright-link">
      <strong>本文链接：</strong>
      <a href="https://jermmy.github.io/2017/04/13/2017-4-13-decision-tree/" title="决策树生成算法">https://jermmy.github.io/2017/04/13/2017-4-13-decision-tree/</a>
    </li>
    <li class="post-copyright-license">
      <strong>版权声明： </strong>
      本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/3.0/" rel="external nofollow" target="_blank">CC BY-NC-SA 3.0</a> 许可协议。转载请注明出处！
    </li>
  </ul>


      
    </div>

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/机器学习/" rel="tag"># 机器学习</a>
          
            <a href="/tags/决策树/" rel="tag"># 决策树</a>
          
        </div>
      

      
        
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2017/04/12/2017-4-12-bagging-boosting-bootstrap/" rel="next" title="Bagging, Boosting, Bootstrap">
                <i class="fa fa-chevron-left"></i> Bagging, Boosting, Bootstrap
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2017/04/18/2017-4-18-python-numpy-randomize/" rel="prev" title="python numpy 三行代码打乱训练数据">
                python numpy 三行代码打乱训练数据 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          
  <div class="comments" id="comments">
    
      <div id="disqus_thread">
        <noscript>
          Please enable JavaScript to view the
          <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a>
        </noscript>
      </div>
    
  </div>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/images/avatar.png"
               alt="Jermmy" />
          <p class="site-author-name" itemprop="name">Jermmy</p>
           
              <p class="site-description motion-element" itemprop="description">In me the tiger sniffs the rose.</p>
          
        </div>
        <nav class="site-state motion-element">

          
            <div class="site-state-item site-state-posts">
              <a href="/archives">
                <span class="site-state-item-count">96</span>
                <span class="site-state-item-name">日志</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-categories">
              <a href="/categories/index.html">
                <span class="site-state-item-count">19</span>
                <span class="site-state-item-name">分类</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-tags">
              <a href="/tags/index.html">
                <span class="site-state-item-count">43</span>
                <span class="site-state-item-name">标签</span>
              </a>
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
                <a href="https://github.com/jermmy" target="_blank" title="GitHub">
                  
                    <i class="fa fa-fw fa-github"></i>
                  
                  GitHub
                </a>
              </span>
            
          
        </div>

        
        

        
        
          <div class="links-of-blogroll motion-element links-of-blogroll-inline">
            <div class="links-of-blogroll-title">
              <i class="fa  fa-fw fa-globe"></i>
              Links
            </div>
            <ul class="links-of-blogroll-list">
              
                <li class="links-of-blogroll-item">
                  <a href="http://www.ruanyifeng.com/blog/" title="阮一峰" target="_blank">阮一峰</a>
                </li>
              
                <li class="links-of-blogroll-item">
                  <a href="http://freemind.pluskid.org/" title="pluskid" target="_blank">pluskid</a>
                </li>
              
            </ul>
          </div>
        

        


      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#构造的原则"><span class="nav-number">1.</span> <span class="nav-text">构造的原则</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#id3-算法"><span class="nav-number">2.</span> <span class="nav-text">ID3 算法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#熵entropy"><span class="nav-number">2.1.</span> <span class="nav-text">熵（entropy）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#选择属性"><span class="nav-number">2.2.</span> <span class="nav-text">选择属性</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#举个例子"><span class="nav-number">2.3.</span> <span class="nav-text">举个例子</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#属性为连续值的情况"><span class="nav-number">2.4.</span> <span class="nav-text">属性为连续值的情况</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#c4.5算法"><span class="nav-number">3.</span> <span class="nav-text">C4.5算法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#cart"><span class="nav-number">4.</span> <span class="nav-text">CART</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#分类树的生成"><span class="nav-number">4.1.</span> <span class="nav-text">分类树的生成</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#回归树的生成"><span class="nav-number">4.2.</span> <span class="nav-text">回归树的生成</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#参考"><span class="nav-number">5.</span> <span class="nav-text">参考</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy;  2016 - 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Jermmy</span>
</div>


<div class="powered-by">
  由 <a class="theme-link" href="https://hexo.io">Hexo</a> 强力驱动
</div>

<div class="theme-info">
  主题 -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Muse
  </a>
</div>


        

        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.1"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.1"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.1"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.1"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.1"></script>



  


  

    
      <script id="dsq-count-scr" src="https://jermmy.disqus.com/count.js" async></script>
    

    
      <script type="text/javascript">
        var disqus_config = function () {
          this.page.url = 'https://jermmy.github.io/2017/04/13/2017-4-13-decision-tree/';
          this.page.identifier = '2017/04/13/2017-4-13-decision-tree/';
          this.page.title = '决策树生成算法';
        };
        var d = document, s = d.createElement('script');
        s.src = 'https://jermmy.disqus.com/embed.js';
        s.setAttribute('data-timestamp', '' + +new Date());
        (d.head || d.body).appendChild(s);
      </script>
    

  




	





  





  








  





  

  
  <script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.1.js"></script>
  <script>AV.initialize("FfkJQoNEh55mO6eDSUYJj0i1-gzGzoHsz", "sjxRrTn3UCxYsAQceJKYEfO7");</script>
  <script>
    function showTime(Counter) {
      var query = new AV.Query(Counter);
      var entries = [];
      var $visitors = $(".leancloud_visitors");

      $visitors.each(function () {
        entries.push( $(this).attr("id").trim() );
      });

      query.containedIn('url', entries);
      query.find()
        .done(function (results) {
          var COUNT_CONTAINER_REF = '.leancloud-visitors-count';

          if (results.length === 0) {
            $visitors.find(COUNT_CONTAINER_REF).text(0);
            return;
          }

          for (var i = 0; i < results.length; i++) {
            var item = results[i];
            var url = item.get('url');
            var time = item.get('time');
            var element = document.getElementById(url);

            $(element).find(COUNT_CONTAINER_REF).text(time);
          }
          for(var i = 0; i < entries.length; i++) {
            var url = entries[i];
            var element = document.getElementById(url);
            var countSpan = $(element).find(COUNT_CONTAINER_REF);
            if( countSpan.text() == '') {
              countSpan.text(0);
            }
          }
        })
        .fail(function (object, error) {
          console.log("Error: " + error.code + " " + error.message);
        });
    }

    function addCount(Counter) {
      var $visitors = $(".leancloud_visitors");
      var url = $visitors.attr('id').trim();
      var title = $visitors.attr('data-flag-title').trim();
      var query = new AV.Query(Counter);

      query.equalTo("url", url);
      query.find({
        success: function(results) {
          if (results.length > 0) {
            var counter = results[0];
            counter.fetchWhenSave(true);
            counter.increment("time");
            counter.save(null, {
              success: function(counter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(counter.get('time'));
              },
              error: function(counter, error) {
                console.log('Failed to save Visitor num, with error message: ' + error.message);
              }
            });
          } else {
            var newcounter = new Counter();
            /* Set ACL */
            var acl = new AV.ACL();
            acl.setPublicReadAccess(true);
            acl.setPublicWriteAccess(true);
            newcounter.setACL(acl);
            /* End Set ACL */
            newcounter.set("title", title);
            newcounter.set("url", url);
            newcounter.set("time", 1);
            newcounter.save(null, {
              success: function(newcounter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(newcounter.get('time'));
              },
              error: function(newcounter, error) {
                console.log('Failed to create');
              }
            });
          }
        },
        error: function(error) {
          console.log('Error:' + error.code + " " + error.message);
        }
      });
    }

    $(function() {
      var Counter = AV.Object.extend("Counter");
      if ($('.leancloud_visitors').length == 1) {
        addCount(Counter);
      } else if ($('.post-title-link').length > 1) {
        showTime(Counter);
      }
    });
  </script>



  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
